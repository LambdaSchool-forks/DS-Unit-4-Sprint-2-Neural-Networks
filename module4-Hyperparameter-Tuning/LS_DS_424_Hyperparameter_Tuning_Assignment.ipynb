{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Ryp-TVm4njD"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 4*\n",
    "\n",
    "## Your Mission, should you choose to accept it...\n",
    "\n",
    "To hyperparameter tune and extract every ounce of accuracy out of this telecom customer churn dataset: <https://drive.google.com/file/d/1dfbAsM9DwA7tYhInyflIpZnYs7VT-0AQ/view> \n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Load the data\n",
    "- Clean the data if necessary (it will be)\n",
    "- Create and fit a baseline Keras MLP model to the data.\n",
    "- Hyperparameter tune (at least) the following parameters:\n",
    " - batch_size\n",
    " - training epochs\n",
    " - optimizer\n",
    " - learning rate (if applicable to optimizer)\n",
    " - momentum (if applicable to optimizer)\n",
    " - activation functions\n",
    " - network weight initialization\n",
    " - dropout regularization\n",
    " - number of neurons in the hidden layer\n",
    " \n",
    " You must use Grid Search and Cross Validation for your initial pass of the above hyperparameters\n",
    " \n",
    " Try and get the maximum accuracy possible out of this data! You'll save big telecoms millions! Doesn't that sound great?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import category_encoders as ce\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNJ-tOBs4jM1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5634, 44) (5634,) (1409, 44) (1409,)\n"
     ]
    }
   ],
   "source": [
    "# load data and create df\n",
    "path = 'WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# numerical features\n",
    "numerical_features = df.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "# Get a series with the cardinality of the nonnumeric features\n",
    "cardinality = df.select_dtypes(exclude='number').nunique()\n",
    "\n",
    "# Get a list of all categorical features with cardinality <= 50\n",
    "categorical_features = cardinality[cardinality <= 150].index.tolist()\n",
    "\n",
    "# Combine Categorical and numerical features\n",
    "features = numerical_features + categorical_features \n",
    "\n",
    "# encode categoricals\n",
    "churn_dict = {'Yes':1, 'No':0}\n",
    "df['Churn'] = df['Churn'].map(churn_dict)\n",
    "encoder = ce.OneHotEncoder()\n",
    "df = encoder.fit_transform(df[features])\n",
    "\n",
    "features = df.drop(columns='Churn').columns.tolist()\n",
    "target = 'Churn'\n",
    "\n",
    "# # train / test split\n",
    "train, test = train_test_split(df, test_size = 0.2, random_state=22)\n",
    "\n",
    "# # X, y matrix\n",
    "X_train = train[features]\n",
    "y_train = train[target]\n",
    "X_test = test[features]\n",
    "y_test = test[target]\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.4357324  -0.29024187 -1.49304973 ... -0.54548944 -0.52710604\n",
      "   1.92836536]\n",
      " [-0.4357324   0.80586073 -1.51141491 ...  1.83321606 -0.52710604\n",
      "  -0.51857393]\n",
      " [ 2.29498656 -0.73680219  0.34513864 ... -0.54548944 -0.52710604\n",
      "  -0.51857393]\n",
      " ...\n",
      " [ 2.29498656 -0.16845269  1.38694568 ... -0.54548944 -0.52710604\n",
      "  -0.51857393]\n",
      " [-0.4357324  -0.69620579 -1.47802366 ... -0.54548944 -0.52710604\n",
      "   1.92836536]\n",
      " [-0.4357324   1.45540301 -1.3561456  ... -0.54548944  1.89715146\n",
      "  -0.51857393]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3774 samples, validate on 1860 samples\n",
      "Epoch 1/10\n",
      "3774/3774 [==============================] - 3s 772us/sample - loss: 1.0477 - acc: 0.7536 - val_loss: 1.0584 - val_acc: 0.7640\n",
      "Epoch 2/10\n",
      "3774/3774 [==============================] - 2s 400us/sample - loss: 0.8067 - acc: 0.7787 - val_loss: 0.7323 - val_acc: 0.7909\n",
      "Epoch 3/10\n",
      "3774/3774 [==============================] - 2s 444us/sample - loss: 0.6395 - acc: 0.7944 - val_loss: 0.7581 - val_acc: 0.7774\n",
      "Epoch 4/10\n",
      "3774/3774 [==============================] - 2s 593us/sample - loss: 0.6395 - acc: 0.7787 - val_loss: 0.8186 - val_acc: 0.7360\n",
      "Epoch 5/10\n",
      "3774/3774 [==============================] - 2s 485us/sample - loss: 0.5929 - acc: 0.7970 - val_loss: 0.7047 - val_acc: 0.7763\n",
      "Epoch 6/10\n",
      "3774/3774 [==============================] - 2s 448us/sample - loss: 0.7163 - acc: 0.7907 - val_loss: 0.6992 - val_acc: 0.7667\n",
      "Epoch 7/10\n",
      "3774/3774 [==============================] - 2s 438us/sample - loss: 0.9940 - acc: 0.7613 - val_loss: 0.9876 - val_acc: 0.7651\n",
      "Epoch 8/10\n",
      "3774/3774 [==============================] - 2s 428us/sample - loss: 0.7749 - acc: 0.7970 - val_loss: 0.8926 - val_acc: 0.7876\n",
      "Epoch 9/10\n",
      "3774/3774 [==============================] - 2s 456us/sample - loss: 0.7207 - acc: 0.8100 - val_loss: 0.8033 - val_acc: 0.7882\n",
      "Epoch 10/10\n",
      "3774/3774 [==============================] - 2s 469us/sample - loss: 0.7108 - acc: 0.8145 - val_loss: 0.8130 - val_acc: 0.7978\n",
      "1409/1409 [==============================] - 0s 69us/sample - loss: 0.9543 - acc: 0.7686\n",
      "acc: 0.7686302065849304\n"
     ]
    }
   ],
   "source": [
    "inputs = X_train.shape[1]\n",
    "\n",
    "# Create Model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(inputs,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit Model\n",
    "model.fit(X_train, y_train, \n",
    "          validation_split=0.33, \n",
    "          epochs=10, \n",
    "          batch_size=10)\n",
    "\n",
    "# Test Accuracy\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(f'{model.metrics_names[1]}: {scores[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make New Predictions and Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.7686302342086586\n"
     ]
    }
   ],
   "source": [
    "# Make predictions \n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = [1 if i>0.5 else 0 for i in y_pred]\n",
    "\n",
    "# Test Accuracy\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print('accuracy score:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Hyperparameters using GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.7887823979059855 using {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.7882499098777771, Stdev: 0.0017571021144191722 with: {'batch_size': 10, 'epochs': 20}\n",
      "Means: 0.7676606376965841, Stdev: 0.02384633344547162 with: {'batch_size': 20, 'epochs': 20}\n",
      "Means: 0.7831025918324789, Stdev: 0.014617108227276744 with: {'batch_size': 40, 'epochs': 20}\n",
      "Means: 0.7887823979059855, Stdev: 0.012826287850372778 with: {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.7877174218495687, Stdev: 0.007889993006359076 with: {'batch_size': 80, 'epochs': 20}\n",
      "Means: 0.7886048952738444, Stdev: 0.010279314433816437 with: {'batch_size': 100, 'epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_shape=(inputs,), activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='relu'))  \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [10, 20, 40, 60, 80, 100],\n",
    "              'epochs': [20]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.7672107877927609\n"
     ]
    }
   ],
   "source": [
    "# Make predictions \n",
    "y_pred = grid.predict(X_test)\n",
    "y_pred = [1 if i>0.5 else 0 for i in y_pred]\n",
    "\n",
    "# Test Accuracy\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print('accuracy score:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer (best = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate: 0.001\n",
      "Best: 0.7754703760147095 using {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.7754703760147095, Stdev: 0.013146538942099429 with: {'batch_size': 60, 'epochs': 20}\n",
      "\n",
      "learning rate: 0.01\n",
      "Best: 0.7447639505068461 using {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.7447639505068461, Stdev: 0.017554842781877994 with: {'batch_size': 60, 'epochs': 20}\n",
      "\n",
      "learning rate: 0.1\n",
      "Best: 0.738551656405131 using {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.738551656405131, Stdev: 0.008052025851505478 with: {'batch_size': 60, 'epochs': 20}\n",
      "\n",
      "learning rate: 0.2\n",
      "Best: 0.5722399751345316 using {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.5722399751345316, Stdev: 0.2276735378305616 with: {'batch_size': 60, 'epochs': 20}\n",
      "\n",
      "learning rate: 0.3\n",
      "Best: 0.5821796158949534 using {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.5821796158949534, Stdev: 0.2242774172794499 with: {'batch_size': 60, 'epochs': 20}\n",
      "\n",
      "learning rate: 0.5\n",
      "Best: 0.5821796158949534 using {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.5821796158949534, Stdev: 0.2242774172794499 with: {'batch_size': 60, 'epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "lr_list = [.001, .01, .1, .2, .3, .5]\n",
    "for i in lr_list:\n",
    "    def create_model():\n",
    "        # create model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, input_shape=(inputs,), activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(1, activation='relu'))  \n",
    "        # Compile model\n",
    "        model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.RMSprop(lr=i), metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    # create model\n",
    "    model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "    # define the grid search parameters\n",
    "    param_grid = {'batch_size': [60],\n",
    "                  'epochs': [20]}\n",
    "\n",
    "    # Create Grid Search\n",
    "    grid = GridSearchCV(estimator=model, cv=3, param_grid=param_grid, n_jobs=1)\n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "    # Report Results\n",
    "    print('\\nlearning rate: {}'.format(i))\n",
    "    print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate: 0.001\n",
      "Best: 0.7877174417177836 using {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.7877174417177836, Stdev: 0.0029594042149341682 with: {'batch_size': 60, 'epochs': 20}\n",
      "\n",
      "learning rate: 0.01\n",
      "Best: 0.7428115010261536 using {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.7428115010261536, Stdev: 0.006133160987832967 with: {'batch_size': 60, 'epochs': 20}\n",
      "\n",
      "learning rate: 0.1\n",
      "Best: 0.7387291391690572 using {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.7387291391690572, Stdev: 0.007889990324635276 with: {'batch_size': 60, 'epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "lr_list = [.001, .01, .1]\n",
    "for i in lr_list:\n",
    "    def create_model():\n",
    "        # create model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, input_shape=(inputs,), activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(1, activation='relu'))  \n",
    "        # Compile model\n",
    "        model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Nadam(lr=i), metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    # create model\n",
    "    model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "    # define the grid search parameters\n",
    "    param_grid = {'batch_size': [60],\n",
    "                  'epochs': [20]}\n",
    "\n",
    "    # Create Grid Search\n",
    "    grid = GridSearchCV(estimator=model, cv=3, param_grid=param_grid, n_jobs=1)\n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "    # Report Results\n",
    "    print('\\nlearning rate: {}'.format(i))\n",
    "    print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.7182398864442867\n"
     ]
    }
   ],
   "source": [
    "# Make predictions \n",
    "y_pred = grid.predict(X_test)\n",
    "y_pred = [1 if i>0.5 else 0 for i in y_pred]\n",
    "\n",
    "# Test Accuracy\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print('accuracy score:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.7694355845451355 using {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.7694355845451355, Stdev: 0.009789552249731895 with: {'batch_size': 60, 'epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_shape=(inputs,), activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1, activation='relu'))  \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [60],\n",
    "              'epochs': [20]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")\n",
    "\n",
    "# Dropout=0.2 makes the score worse\n",
    "# Dropout=0.1 makes the score worse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.8107916238272406 using {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.8107916238272406, Stdev: 0.011391109243850112 with: {'batch_size': 60, 'epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_shape=(inputs,), activation='sigmoid'))\n",
    "    model.add(Dense(64, activation='sigmoid'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [60],\n",
    "              'epochs': [20]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, cv=5, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")\n",
    "\n",
    "# sigmoid is better than relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4507/4507 [==============================] - 2s 342us/sample - loss: 0.5431 - acc: 0.7189\n",
      "Epoch 2/100\n",
      "4507/4507 [==============================] - 0s 94us/sample - loss: 0.4477 - acc: 0.7790\n",
      "Epoch 3/100\n",
      "4507/4507 [==============================] - 0s 94us/sample - loss: 0.4311 - acc: 0.7968\n",
      "Epoch 4/100\n",
      "4507/4507 [==============================] - 0s 94us/sample - loss: 0.4258 - acc: 0.7972\n",
      "Epoch 5/100\n",
      "4507/4507 [==============================] - 0s 93us/sample - loss: 0.4222 - acc: 0.7996\n",
      "Epoch 6/100\n",
      "4507/4507 [==============================] - 0s 94us/sample - loss: 0.4195 - acc: 0.8052\n",
      "Epoch 7/100\n",
      "4507/4507 [==============================] - 0s 95us/sample - loss: 0.4194 - acc: 0.8034\n",
      "Epoch 8/100\n",
      "4507/4507 [==============================] - 0s 95us/sample - loss: 0.4172 - acc: 0.8010\n",
      "Epoch 9/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.4173 - acc: 0.8063\n",
      "Epoch 10/100\n",
      "4507/4507 [==============================] - 0s 97us/sample - loss: 0.4156 - acc: 0.8061\n",
      "Epoch 11/100\n",
      "4507/4507 [==============================] - 0s 95us/sample - loss: 0.4147 - acc: 0.8090\n",
      "Epoch 12/100\n",
      "4507/4507 [==============================] - 0s 95us/sample - loss: 0.4168 - acc: 0.8067\n",
      "Epoch 13/100\n",
      "4507/4507 [==============================] - 0s 95us/sample - loss: 0.4149 - acc: 0.8063\n",
      "Epoch 14/100\n",
      "4507/4507 [==============================] - 0s 95us/sample - loss: 0.4144 - acc: 0.8076\n",
      "Epoch 15/100\n",
      "4507/4507 [==============================] - 0s 97us/sample - loss: 0.4169 - acc: 0.8047\n",
      "Epoch 16/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.4150 - acc: 0.8043\n",
      "Epoch 17/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.4151 - acc: 0.8076\n",
      "Epoch 18/100\n",
      "4507/4507 [==============================] - 0s 97us/sample - loss: 0.4131 - acc: 0.8092\n",
      "Epoch 19/100\n",
      "4507/4507 [==============================] - 0s 97us/sample - loss: 0.4131 - acc: 0.8118\n",
      "Epoch 20/100\n",
      "4507/4507 [==============================] - 0s 101us/sample - loss: 0.4121 - acc: 0.8090\n",
      "Epoch 21/100\n",
      "4507/4507 [==============================] - 0s 107us/sample - loss: 0.4128 - acc: 0.8094\n",
      "Epoch 22/100\n",
      "4507/4507 [==============================] - 0s 98us/sample - loss: 0.4116 - acc: 0.8083\n",
      "Epoch 23/100\n",
      "4507/4507 [==============================] - 0s 97us/sample - loss: 0.4120 - acc: 0.8092\n",
      "Epoch 24/100\n",
      "4507/4507 [==============================] - 0s 102us/sample - loss: 0.4133 - acc: 0.8090\n",
      "Epoch 25/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.4110 - acc: 0.8103\n",
      "Epoch 26/100\n",
      "4507/4507 [==============================] - 0s 97us/sample - loss: 0.4102 - acc: 0.8074\n",
      "Epoch 27/100\n",
      "4507/4507 [==============================] - 0s 97us/sample - loss: 0.4103 - acc: 0.8079\n",
      "Epoch 28/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.4118 - acc: 0.8092\n",
      "Epoch 29/100\n",
      "4507/4507 [==============================] - 0s 97us/sample - loss: 0.4100 - acc: 0.8096\n",
      "Epoch 30/100\n",
      "4507/4507 [==============================] - 0s 94us/sample - loss: 0.4099 - acc: 0.8112\n",
      "Epoch 31/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.4097 - acc: 0.8094\n",
      "Epoch 32/100\n",
      "4507/4507 [==============================] - 0s 95us/sample - loss: 0.4081 - acc: 0.8087\n",
      "Epoch 33/100\n",
      "4507/4507 [==============================] - 0s 100us/sample - loss: 0.4094 - acc: 0.8081\n",
      "Epoch 34/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.4087 - acc: 0.8099\n",
      "Epoch 35/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.4081 - acc: 0.8083\n",
      "Epoch 36/100\n",
      "4507/4507 [==============================] - 0s 94us/sample - loss: 0.4089 - acc: 0.8118\n",
      "Epoch 37/100\n",
      "4507/4507 [==============================] - 0s 95us/sample - loss: 0.4077 - acc: 0.8130\n",
      "Epoch 38/100\n",
      "4507/4507 [==============================] - 0s 95us/sample - loss: 0.4064 - acc: 0.8125\n",
      "Epoch 39/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.4059 - acc: 0.8107\n",
      "Epoch 40/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.4053 - acc: 0.8105\n",
      "Epoch 41/100\n",
      "4507/4507 [==============================] - 0s 95us/sample - loss: 0.4056 - acc: 0.8125\n",
      "Epoch 42/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.4058 - acc: 0.8121\n",
      "Epoch 43/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.4047 - acc: 0.8114\n",
      "Epoch 44/100\n",
      "4507/4507 [==============================] - 0s 97us/sample - loss: 0.4052 - acc: 0.8134\n",
      "Epoch 45/100\n",
      "4507/4507 [==============================] - 0s 97us/sample - loss: 0.4033 - acc: 0.8123\n",
      "Epoch 46/100\n",
      "4507/4507 [==============================] - 0s 102us/sample - loss: 0.4031 - acc: 0.8103\n",
      "Epoch 47/100\n",
      "4507/4507 [==============================] - 0s 94us/sample - loss: 0.4026 - acc: 0.8101\n",
      "Epoch 48/100\n",
      "4507/4507 [==============================] - 0s 94us/sample - loss: 0.4027 - acc: 0.8147\n",
      "Epoch 49/100\n",
      "4507/4507 [==============================] - 0s 95us/sample - loss: 0.4019 - acc: 0.8145\n",
      "Epoch 50/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.4008 - acc: 0.8156\n",
      "Epoch 51/100\n",
      "4507/4507 [==============================] - 0s 95us/sample - loss: 0.4012 - acc: 0.81250s - loss: 0.3905 - acc: 0.\n",
      "Epoch 52/100\n",
      "4507/4507 [==============================] - 0s 98us/sample - loss: 0.4007 - acc: 0.8150\n",
      "Epoch 53/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.3994 - acc: 0.8152\n",
      "Epoch 54/100\n",
      "4507/4507 [==============================] - 0s 99us/sample - loss: 0.3996 - acc: 0.8156\n",
      "Epoch 55/100\n",
      "4507/4507 [==============================] - 0s 108us/sample - loss: 0.3997 - acc: 0.8150\n",
      "Epoch 56/100\n",
      "4507/4507 [==============================] - 0s 102us/sample - loss: 0.3987 - acc: 0.8158\n",
      "Epoch 57/100\n",
      "4507/4507 [==============================] - 0s 97us/sample - loss: 0.3982 - acc: 0.8134\n",
      "Epoch 58/100\n",
      "4507/4507 [==============================] - 0s 98us/sample - loss: 0.3972 - acc: 0.8138\n",
      "Epoch 59/100\n",
      "4507/4507 [==============================] - 0s 94us/sample - loss: 0.3966 - acc: 0.8165\n",
      "Epoch 60/100\n",
      "4507/4507 [==============================] - 0s 95us/sample - loss: 0.3962 - acc: 0.8136\n",
      "Epoch 61/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.3958 - acc: 0.8163\n",
      "Epoch 62/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.3974 - acc: 0.8156\n",
      "Epoch 63/100\n",
      "4507/4507 [==============================] - 0s 97us/sample - loss: 0.3939 - acc: 0.8183\n",
      "Epoch 64/100\n",
      "4507/4507 [==============================] - 0s 94us/sample - loss: 0.3931 - acc: 0.8176\n",
      "Epoch 65/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.3934 - acc: 0.8161\n",
      "Epoch 66/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.3921 - acc: 0.8176\n",
      "Epoch 67/100\n",
      "4507/4507 [==============================] - 0s 98us/sample - loss: 0.3912 - acc: 0.8192\n",
      "Epoch 68/100\n",
      "4507/4507 [==============================] - 0s 95us/sample - loss: 0.3911 - acc: 0.8145\n",
      "Epoch 69/100\n",
      "4507/4507 [==============================] - 0s 99us/sample - loss: 0.3924 - acc: 0.8181\n",
      "Epoch 70/100\n",
      "4507/4507 [==============================] - 0s 95us/sample - loss: 0.3897 - acc: 0.8201\n",
      "Epoch 71/100\n",
      "4507/4507 [==============================] - 0s 95us/sample - loss: 0.3892 - acc: 0.8218\n",
      "Epoch 72/100\n",
      "4507/4507 [==============================] - 0s 97us/sample - loss: 0.3888 - acc: 0.8214\n",
      "Epoch 73/100\n",
      "4507/4507 [==============================] - 0s 97us/sample - loss: 0.3874 - acc: 0.8196\n",
      "Epoch 74/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.3885 - acc: 0.8194\n",
      "Epoch 75/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.3855 - acc: 0.8203\n",
      "Epoch 76/100\n",
      "4507/4507 [==============================] - 0s 97us/sample - loss: 0.3853 - acc: 0.8207\n",
      "Epoch 77/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.3849 - acc: 0.8203\n",
      "Epoch 78/100\n",
      "4507/4507 [==============================] - 0s 97us/sample - loss: 0.3853 - acc: 0.8183\n",
      "Epoch 79/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.3846 - acc: 0.8198\n",
      "Epoch 80/100\n",
      "4507/4507 [==============================] - 0s 95us/sample - loss: 0.3802 - acc: 0.8241\n",
      "Epoch 81/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.3811 - acc: 0.8229\n",
      "Epoch 82/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.3808 - acc: 0.8238\n",
      "Epoch 83/100\n",
      "4507/4507 [==============================] - 0s 97us/sample - loss: 0.3790 - acc: 0.8234\n",
      "Epoch 84/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.3780 - acc: 0.8254\n",
      "Epoch 85/100\n",
      "4507/4507 [==============================] - 0s 95us/sample - loss: 0.3786 - acc: 0.8258\n",
      "Epoch 86/100\n",
      "4507/4507 [==============================] - 0s 95us/sample - loss: 0.3786 - acc: 0.8238\n",
      "Epoch 87/100\n",
      "4507/4507 [==============================] - 0s 95us/sample - loss: 0.3747 - acc: 0.8263\n",
      "Epoch 88/100\n",
      "4507/4507 [==============================] - 0s 106us/sample - loss: 0.3735 - acc: 0.8263\n",
      "Epoch 89/100\n",
      "4507/4507 [==============================] - 1s 116us/sample - loss: 0.3736 - acc: 0.8283\n",
      "Epoch 90/100\n",
      "4507/4507 [==============================] - 1s 152us/sample - loss: 0.3718 - acc: 0.8269\n",
      "Epoch 91/100\n",
      "4507/4507 [==============================] - 0s 106us/sample - loss: 0.3702 - acc: 0.8265\n",
      "Epoch 92/100\n",
      "4507/4507 [==============================] - 1s 115us/sample - loss: 0.3698 - acc: 0.8276\n",
      "Epoch 93/100\n",
      "4507/4507 [==============================] - 1s 118us/sample - loss: 0.3690 - acc: 0.8312\n",
      "Epoch 94/100\n",
      "4507/4507 [==============================] - 1s 138us/sample - loss: 0.3676 - acc: 0.8305\n",
      "Epoch 95/100\n",
      "4507/4507 [==============================] - 0s 101us/sample - loss: 0.3680 - acc: 0.8305\n",
      "Epoch 96/100\n",
      "4507/4507 [==============================] - 0s 103us/sample - loss: 0.3701 - acc: 0.8305\n",
      "Epoch 97/100\n",
      "4507/4507 [==============================] - 0s 108us/sample - loss: 0.3638 - acc: 0.8351\n",
      "Epoch 98/100\n",
      "4507/4507 [==============================] - 1s 124us/sample - loss: 0.3625 - acc: 0.8325\n",
      "Epoch 99/100\n",
      "4507/4507 [==============================] - 0s 110us/sample - loss: 0.3632 - acc: 0.8343\n",
      "Epoch 100/100\n",
      "4507/4507 [==============================] - 1s 115us/sample - loss: 0.3599 - acc: 0.8385\n",
      "1127/1127 [==============================] - 1s 1ms/sample - loss: 0.4082 - acc: 0.8110\n",
      "Epoch 1/100\n",
      "4507/4507 [==============================] - 2s 360us/sample - loss: 0.5807 - acc: 0.6743\n",
      "Epoch 2/100\n",
      "4507/4507 [==============================] - 0s 100us/sample - loss: 0.4569 - acc: 0.7730\n",
      "Epoch 3/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.4341 - acc: 0.7985\n",
      "Epoch 4/100\n",
      "4507/4507 [==============================] - 0s 98us/sample - loss: 0.4269 - acc: 0.8001\n",
      "Epoch 5/100\n",
      "4507/4507 [==============================] - 0s 98us/sample - loss: 0.4239 - acc: 0.8063\n",
      "Epoch 6/100\n",
      "4507/4507 [==============================] - 0s 98us/sample - loss: 0.4214 - acc: 0.8039\n",
      "Epoch 7/100\n",
      "4507/4507 [==============================] - 0s 100us/sample - loss: 0.4203 - acc: 0.8056\n",
      "Epoch 8/100\n",
      "4507/4507 [==============================] - 0s 97us/sample - loss: 0.4192 - acc: 0.8065\n",
      "Epoch 9/100\n",
      "4507/4507 [==============================] - 0s 99us/sample - loss: 0.4171 - acc: 0.8074\n",
      "Epoch 10/100\n",
      "4507/4507 [==============================] - 0s 102us/sample - loss: 0.4166 - acc: 0.8054\n",
      "Epoch 11/100\n",
      "4507/4507 [==============================] - 0s 100us/sample - loss: 0.4167 - acc: 0.8063\n",
      "Epoch 12/100\n",
      "4507/4507 [==============================] - 0s 98us/sample - loss: 0.4162 - acc: 0.8061\n",
      "Epoch 13/100\n",
      "4507/4507 [==============================] - 0s 99us/sample - loss: 0.4154 - acc: 0.8056\n",
      "Epoch 14/100\n",
      "4507/4507 [==============================] - 0s 99us/sample - loss: 0.4150 - acc: 0.8067\n",
      "Epoch 15/100\n",
      "4507/4507 [==============================] - 0s 99us/sample - loss: 0.4150 - acc: 0.8072\n",
      "Epoch 16/100\n",
      "4507/4507 [==============================] - 0s 98us/sample - loss: 0.4143 - acc: 0.8087\n",
      "Epoch 17/100\n",
      "4507/4507 [==============================] - 0s 97us/sample - loss: 0.4150 - acc: 0.8072\n",
      "Epoch 18/100\n",
      "4507/4507 [==============================] - 0s 99us/sample - loss: 0.4144 - acc: 0.8094\n",
      "Epoch 19/100\n",
      "4507/4507 [==============================] - 0s 100us/sample - loss: 0.4128 - acc: 0.8087\n",
      "Epoch 20/100\n",
      "4507/4507 [==============================] - 0s 99us/sample - loss: 0.4125 - acc: 0.8099\n",
      "Epoch 21/100\n",
      "4507/4507 [==============================] - 0s 100us/sample - loss: 0.4125 - acc: 0.8136\n",
      "Epoch 22/100\n",
      "4507/4507 [==============================] - 0s 100us/sample - loss: 0.4121 - acc: 0.8090\n",
      "Epoch 23/100\n",
      "4507/4507 [==============================] - 0s 99us/sample - loss: 0.4121 - acc: 0.8065\n",
      "Epoch 24/100\n",
      "4507/4507 [==============================] - 0s 99us/sample - loss: 0.4122 - acc: 0.8103\n",
      "Epoch 25/100\n",
      "4507/4507 [==============================] - 0s 99us/sample - loss: 0.4107 - acc: 0.8076\n",
      "Epoch 26/100\n",
      "4507/4507 [==============================] - 0s 99us/sample - loss: 0.4120 - acc: 0.8105\n",
      "Epoch 27/100\n",
      "4507/4507 [==============================] - 0s 97us/sample - loss: 0.4099 - acc: 0.8094\n",
      "Epoch 28/100\n",
      "4507/4507 [==============================] - 0s 105us/sample - loss: 0.4110 - acc: 0.8107\n",
      "Epoch 29/100\n",
      "4507/4507 [==============================] - 1s 113us/sample - loss: 0.4097 - acc: 0.8116\n",
      "Epoch 30/100\n",
      "4507/4507 [==============================] - 0s 108us/sample - loss: 0.4098 - acc: 0.8083\n",
      "Epoch 31/100\n",
      "4507/4507 [==============================] - 0s 101us/sample - loss: 0.4093 - acc: 0.8094\n",
      "Epoch 32/100\n",
      "4507/4507 [==============================] - 0s 97us/sample - loss: 0.4099 - acc: 0.8085\n",
      "Epoch 33/100\n",
      "4507/4507 [==============================] - 0s 102us/sample - loss: 0.4090 - acc: 0.8114\n",
      "Epoch 34/100\n",
      "4507/4507 [==============================] - 0s 100us/sample - loss: 0.4085 - acc: 0.8090\n",
      "Epoch 35/100\n",
      "4507/4507 [==============================] - 1s 121us/sample - loss: 0.4114 - acc: 0.8096\n",
      "Epoch 36/100\n",
      "4507/4507 [==============================] - 0s 102us/sample - loss: 0.4084 - acc: 0.8101\n",
      "Epoch 37/100\n",
      "4507/4507 [==============================] - 0s 100us/sample - loss: 0.4069 - acc: 0.8107\n",
      "Epoch 38/100\n",
      "4507/4507 [==============================] - 1s 113us/sample - loss: 0.4071 - acc: 0.8132\n",
      "Epoch 39/100\n",
      "4507/4507 [==============================] - 1s 119us/sample - loss: 0.4077 - acc: 0.8107\n",
      "Epoch 40/100\n",
      "4507/4507 [==============================] - 0s 101us/sample - loss: 0.4057 - acc: 0.8114\n",
      "Epoch 41/100\n",
      "4507/4507 [==============================] - 0s 110us/sample - loss: 0.4056 - acc: 0.8141\n",
      "Epoch 42/100\n",
      "4507/4507 [==============================] - 0s 102us/sample - loss: 0.4050 - acc: 0.8118\n",
      "Epoch 43/100\n",
      "4507/4507 [==============================] - 0s 98us/sample - loss: 0.4053 - acc: 0.8136\n",
      "Epoch 44/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.4064 - acc: 0.8138\n",
      "Epoch 45/100\n",
      "4507/4507 [==============================] - 1s 114us/sample - loss: 0.4045 - acc: 0.8147\n",
      "Epoch 46/100\n",
      "4507/4507 [==============================] - 0s 101us/sample - loss: 0.4042 - acc: 0.8114\n",
      "Epoch 47/100\n",
      "4507/4507 [==============================] - 1s 112us/sample - loss: 0.4030 - acc: 0.8134\n",
      "Epoch 48/100\n",
      "4507/4507 [==============================] - 0s 100us/sample - loss: 0.4022 - acc: 0.8136\n",
      "Epoch 49/100\n",
      "4507/4507 [==============================] - 0s 95us/sample - loss: 0.4022 - acc: 0.8121\n",
      "Epoch 50/100\n",
      "4507/4507 [==============================] - 0s 100us/sample - loss: 0.4019 - acc: 0.8150\n",
      "Epoch 51/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.4012 - acc: 0.8150\n",
      "Epoch 52/100\n",
      "4507/4507 [==============================] - 0s 106us/sample - loss: 0.4022 - acc: 0.8161\n",
      "Epoch 53/100\n",
      "4507/4507 [==============================] - 0s 97us/sample - loss: 0.4024 - acc: 0.8105\n",
      "Epoch 54/100\n",
      "4507/4507 [==============================] - 0s 100us/sample - loss: 0.4013 - acc: 0.8092\n",
      "Epoch 55/100\n",
      "4507/4507 [==============================] - 0s 105us/sample - loss: 0.3993 - acc: 0.8145\n",
      "Epoch 56/100\n",
      "4507/4507 [==============================] - 0s 104us/sample - loss: 0.4005 - acc: 0.8138\n",
      "Epoch 57/100\n",
      "4507/4507 [==============================] - 0s 104us/sample - loss: 0.3975 - acc: 0.8158\n",
      "Epoch 58/100\n",
      "4507/4507 [==============================] - 0s 102us/sample - loss: 0.3976 - acc: 0.8136\n",
      "Epoch 59/100\n",
      "4507/4507 [==============================] - 0s 105us/sample - loss: 0.3971 - acc: 0.8134\n",
      "Epoch 60/100\n",
      "4507/4507 [==============================] - 1s 120us/sample - loss: 0.3965 - acc: 0.8187\n",
      "Epoch 61/100\n",
      "4507/4507 [==============================] - 1s 113us/sample - loss: 0.3959 - acc: 0.8158\n",
      "Epoch 62/100\n",
      "4507/4507 [==============================] - 0s 108us/sample - loss: 0.3946 - acc: 0.8174\n",
      "Epoch 63/100\n",
      "4507/4507 [==============================] - 0s 107us/sample - loss: 0.3961 - acc: 0.8154\n",
      "Epoch 64/100\n",
      "4507/4507 [==============================] - 0s 107us/sample - loss: 0.3939 - acc: 0.8203\n",
      "Epoch 65/100\n",
      "4507/4507 [==============================] - 0s 103us/sample - loss: 0.3937 - acc: 0.8181\n",
      "Epoch 66/100\n",
      "4507/4507 [==============================] - 0s 105us/sample - loss: 0.3927 - acc: 0.8183\n",
      "Epoch 67/100\n",
      "4507/4507 [==============================] - 0s 103us/sample - loss: 0.3938 - acc: 0.8167\n",
      "Epoch 68/100\n",
      "4507/4507 [==============================] - 0s 104us/sample - loss: 0.3910 - acc: 0.8185\n",
      "Epoch 69/100\n",
      "4507/4507 [==============================] - 0s 104us/sample - loss: 0.3927 - acc: 0.8198\n",
      "Epoch 70/100\n",
      "4507/4507 [==============================] - 0s 103us/sample - loss: 0.3903 - acc: 0.8192\n",
      "Epoch 71/100\n",
      "4507/4507 [==============================] - 0s 108us/sample - loss: 0.3900 - acc: 0.8181\n",
      "Epoch 72/100\n",
      "4507/4507 [==============================] - 0s 104us/sample - loss: 0.3888 - acc: 0.8176\n",
      "Epoch 73/100\n",
      "4507/4507 [==============================] - 0s 106us/sample - loss: 0.3873 - acc: 0.8185\n",
      "Epoch 74/100\n",
      "4507/4507 [==============================] - 0s 105us/sample - loss: 0.3876 - acc: 0.8194\n",
      "Epoch 75/100\n",
      "4507/4507 [==============================] - 0s 103us/sample - loss: 0.3861 - acc: 0.8187\n",
      "Epoch 76/100\n",
      "4507/4507 [==============================] - 0s 97us/sample - loss: 0.3855 - acc: 0.8185\n",
      "Epoch 77/100\n",
      "4507/4507 [==============================] - 0s 98us/sample - loss: 0.3883 - acc: 0.8196\n",
      "Epoch 78/100\n",
      "4507/4507 [==============================] - 0s 98us/sample - loss: 0.3839 - acc: 0.8254\n",
      "Epoch 79/100\n",
      "4507/4507 [==============================] - 0s 99us/sample - loss: 0.3825 - acc: 0.8209\n",
      "Epoch 80/100\n",
      "4507/4507 [==============================] - 1s 122us/sample - loss: 0.3812 - acc: 0.8225\n",
      "Epoch 81/100\n",
      "4507/4507 [==============================] - 1s 141us/sample - loss: 0.3813 - acc: 0.8227\n",
      "Epoch 82/100\n",
      "4507/4507 [==============================] - 1s 120us/sample - loss: 0.3793 - acc: 0.8241\n",
      "Epoch 83/100\n",
      "4507/4507 [==============================] - 0s 106us/sample - loss: 0.3783 - acc: 0.8225\n",
      "Epoch 84/100\n",
      "4507/4507 [==============================] - 0s 99us/sample - loss: 0.3795 - acc: 0.8236\n",
      "Epoch 85/100\n",
      "4507/4507 [==============================] - 0s 98us/sample - loss: 0.3774 - acc: 0.8243\n",
      "Epoch 86/100\n",
      "4507/4507 [==============================] - 0s 99us/sample - loss: 0.3763 - acc: 0.8245\n",
      "Epoch 87/100\n",
      "4507/4507 [==============================] - 0s 101us/sample - loss: 0.3748 - acc: 0.8278\n",
      "Epoch 88/100\n",
      "4507/4507 [==============================] - 0s 100us/sample - loss: 0.3744 - acc: 0.8296\n",
      "Epoch 89/100\n",
      "4507/4507 [==============================] - 0s 98us/sample - loss: 0.3740 - acc: 0.8280\n",
      "Epoch 90/100\n",
      "4507/4507 [==============================] - 0s 102us/sample - loss: 0.3721 - acc: 0.8294\n",
      "Epoch 91/100\n",
      "4507/4507 [==============================] - 1s 114us/sample - loss: 0.3728 - acc: 0.8287\n",
      "Epoch 92/100\n",
      "4507/4507 [==============================] - 1s 124us/sample - loss: 0.3708 - acc: 0.8247\n",
      "Epoch 93/100\n",
      "4507/4507 [==============================] - 1s 116us/sample - loss: 0.3697 - acc: 0.8256\n",
      "Epoch 94/100\n",
      "4507/4507 [==============================] - 0s 102us/sample - loss: 0.3690 - acc: 0.8267\n",
      "Epoch 95/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.3702 - acc: 0.8278\n",
      "Epoch 96/100\n",
      "4507/4507 [==============================] - 0s 107us/sample - loss: 0.3682 - acc: 0.8303\n",
      "Epoch 97/100\n",
      "4507/4507 [==============================] - 1s 115us/sample - loss: 0.3653 - acc: 0.8329\n",
      "Epoch 98/100\n",
      "4507/4507 [==============================] - 0s 107us/sample - loss: 0.3636 - acc: 0.8305\n",
      "Epoch 99/100\n",
      "4507/4507 [==============================] - 0s 99us/sample - loss: 0.3629 - acc: 0.8343\n",
      "Epoch 100/100\n",
      "4507/4507 [==============================] - 0s 100us/sample - loss: 0.3627 - acc: 0.8347\n",
      "1127/1127 [==============================] - 1s 1ms/sample - loss: 0.4245 - acc: 0.7986\n",
      "Epoch 1/100\n",
      "4507/4507 [==============================] - 2s 393us/sample - loss: 0.5349 - acc: 0.7213\n",
      "Epoch 2/100\n",
      "4507/4507 [==============================] - 1s 118us/sample - loss: 0.4372 - acc: 0.7877\n",
      "Epoch 3/100\n",
      "4507/4507 [==============================] - 0s 101us/sample - loss: 0.4180 - acc: 0.8083\n",
      "Epoch 4/100\n",
      "4507/4507 [==============================] - 1s 115us/sample - loss: 0.4121 - acc: 0.8092\n",
      "Epoch 5/100\n",
      "4507/4507 [==============================] - 0s 104us/sample - loss: 0.4086 - acc: 0.8130\n",
      "Epoch 6/100\n",
      "4507/4507 [==============================] - 0s 102us/sample - loss: 0.4064 - acc: 0.8107\n",
      "Epoch 7/100\n",
      "4507/4507 [==============================] - 0s 100us/sample - loss: 0.4057 - acc: 0.8105\n",
      "Epoch 8/100\n",
      "4507/4507 [==============================] - 0s 109us/sample - loss: 0.4041 - acc: 0.8107\n",
      "Epoch 9/100\n",
      "4507/4507 [==============================] - 1s 112us/sample - loss: 0.4034 - acc: 0.8107\n",
      "Epoch 10/100\n",
      "4507/4507 [==============================] - 0s 109us/sample - loss: 0.4031 - acc: 0.8114\n",
      "Epoch 11/100\n",
      "4507/4507 [==============================] - 1s 113us/sample - loss: 0.4008 - acc: 0.8123\n",
      "Epoch 12/100\n",
      "4507/4507 [==============================] - 0s 103us/sample - loss: 0.4017 - acc: 0.8101\n",
      "Epoch 13/100\n",
      "4507/4507 [==============================] - 0s 107us/sample - loss: 0.3998 - acc: 0.8147\n",
      "Epoch 14/100\n",
      "4507/4507 [==============================] - 0s 108us/sample - loss: 0.3994 - acc: 0.8167\n",
      "Epoch 15/100\n",
      "4507/4507 [==============================] - 0s 105us/sample - loss: 0.4012 - acc: 0.8114\n",
      "Epoch 16/100\n",
      "4507/4507 [==============================] - 0s 100us/sample - loss: 0.3993 - acc: 0.8147\n",
      "Epoch 17/100\n",
      "4507/4507 [==============================] - 0s 100us/sample - loss: 0.3982 - acc: 0.8138\n",
      "Epoch 18/100\n",
      "4507/4507 [==============================] - 0s 106us/sample - loss: 0.3978 - acc: 0.8145\n",
      "Epoch 19/100\n",
      "4507/4507 [==============================] - 0s 107us/sample - loss: 0.3985 - acc: 0.8114\n",
      "Epoch 20/100\n",
      "4507/4507 [==============================] - 0s 104us/sample - loss: 0.3973 - acc: 0.8121\n",
      "Epoch 21/100\n",
      "4507/4507 [==============================] - 1s 112us/sample - loss: 0.3976 - acc: 0.8123\n",
      "Epoch 22/100\n",
      "4507/4507 [==============================] - 0s 101us/sample - loss: 0.3967 - acc: 0.8143\n",
      "Epoch 23/100\n",
      "4507/4507 [==============================] - 0s 107us/sample - loss: 0.3975 - acc: 0.8161\n",
      "Epoch 24/100\n",
      "4507/4507 [==============================] - 0s 108us/sample - loss: 0.3969 - acc: 0.8150\n",
      "Epoch 25/100\n",
      "4507/4507 [==============================] - 0s 101us/sample - loss: 0.3957 - acc: 0.8147\n",
      "Epoch 26/100\n",
      "4507/4507 [==============================] - 0s 102us/sample - loss: 0.3956 - acc: 0.8165\n",
      "Epoch 27/100\n",
      "4507/4507 [==============================] - 0s 101us/sample - loss: 0.3943 - acc: 0.8152\n",
      "Epoch 28/100\n",
      "4507/4507 [==============================] - 0s 104us/sample - loss: 0.3945 - acc: 0.8152\n",
      "Epoch 29/100\n",
      "4507/4507 [==============================] - 0s 100us/sample - loss: 0.3938 - acc: 0.8158\n",
      "Epoch 30/100\n",
      "4507/4507 [==============================] - 1s 113us/sample - loss: 0.3945 - acc: 0.8181\n",
      "Epoch 31/100\n",
      "4507/4507 [==============================] - 1s 129us/sample - loss: 0.3930 - acc: 0.8125\n",
      "Epoch 32/100\n",
      "4507/4507 [==============================] - 1s 129us/sample - loss: 0.3924 - acc: 0.8163\n",
      "Epoch 33/100\n",
      "4507/4507 [==============================] - 1s 123us/sample - loss: 0.3926 - acc: 0.8167\n",
      "Epoch 34/100\n",
      "4507/4507 [==============================] - 0s 111us/sample - loss: 0.3932 - acc: 0.8176\n",
      "Epoch 35/100\n",
      "4507/4507 [==============================] - 0s 101us/sample - loss: 0.3913 - acc: 0.8161\n",
      "Epoch 36/100\n",
      "4507/4507 [==============================] - 0s 101us/sample - loss: 0.3912 - acc: 0.8167\n",
      "Epoch 37/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.3904 - acc: 0.8198\n",
      "Epoch 38/100\n",
      "4507/4507 [==============================] - 0s 101us/sample - loss: 0.3896 - acc: 0.8183\n",
      "Epoch 39/100\n",
      "4507/4507 [==============================] - 0s 102us/sample - loss: 0.3903 - acc: 0.8194\n",
      "Epoch 40/100\n",
      "4507/4507 [==============================] - 0s 106us/sample - loss: 0.3889 - acc: 0.8187\n",
      "Epoch 41/100\n",
      "4507/4507 [==============================] - 0s 104us/sample - loss: 0.3887 - acc: 0.8187\n",
      "Epoch 42/100\n",
      "4507/4507 [==============================] - 0s 101us/sample - loss: 0.3879 - acc: 0.8207\n",
      "Epoch 43/100\n",
      "4507/4507 [==============================] - 0s 101us/sample - loss: 0.3877 - acc: 0.8225\n",
      "Epoch 44/100\n",
      "4507/4507 [==============================] - 0s 100us/sample - loss: 0.3884 - acc: 0.8192\n",
      "Epoch 45/100\n",
      "4507/4507 [==============================] - 0s 103us/sample - loss: 0.3864 - acc: 0.8174\n",
      "Epoch 46/100\n",
      "4507/4507 [==============================] - 0s 103us/sample - loss: 0.3859 - acc: 0.8183\n",
      "Epoch 47/100\n",
      "4507/4507 [==============================] - 0s 100us/sample - loss: 0.3867 - acc: 0.8198\n",
      "Epoch 48/100\n",
      "4507/4507 [==============================] - 0s 101us/sample - loss: 0.3837 - acc: 0.8209\n",
      "Epoch 49/100\n",
      "4507/4507 [==============================] - 0s 108us/sample - loss: 0.3847 - acc: 0.8214\n",
      "Epoch 50/100\n",
      "4507/4507 [==============================] - 0s 103us/sample - loss: 0.3849 - acc: 0.8216\n",
      "Epoch 51/100\n",
      "4507/4507 [==============================] - 0s 102us/sample - loss: 0.3830 - acc: 0.8234\n",
      "Epoch 52/100\n",
      "4507/4507 [==============================] - 0s 105us/sample - loss: 0.3830 - acc: 0.8243\n",
      "Epoch 53/100\n",
      "4507/4507 [==============================] - 0s 108us/sample - loss: 0.3818 - acc: 0.8241\n",
      "Epoch 54/100\n",
      "4507/4507 [==============================] - 1s 114us/sample - loss: 0.3814 - acc: 0.8243\n",
      "Epoch 55/100\n",
      "4507/4507 [==============================] - 0s 106us/sample - loss: 0.3819 - acc: 0.8247\n",
      "Epoch 56/100\n",
      "4507/4507 [==============================] - 0s 107us/sample - loss: 0.3799 - acc: 0.8247\n",
      "Epoch 57/100\n",
      "4507/4507 [==============================] - 0s 107us/sample - loss: 0.3808 - acc: 0.8238\n",
      "Epoch 58/100\n",
      "4507/4507 [==============================] - 0s 99us/sample - loss: 0.3803 - acc: 0.8258\n",
      "Epoch 59/100\n",
      "4507/4507 [==============================] - 0s 100us/sample - loss: 0.3789 - acc: 0.8249\n",
      "Epoch 60/100\n",
      "4507/4507 [==============================] - 0s 101us/sample - loss: 0.3780 - acc: 0.8245\n",
      "Epoch 61/100\n",
      "4507/4507 [==============================] - 1s 128us/sample - loss: 0.3767 - acc: 0.8263\n",
      "Epoch 62/100\n",
      "4507/4507 [==============================] - 1s 132us/sample - loss: 0.3775 - acc: 0.8274\n",
      "Epoch 63/100\n",
      "4507/4507 [==============================] - 1s 130us/sample - loss: 0.3781 - acc: 0.8234\n",
      "Epoch 64/100\n",
      "4507/4507 [==============================] - 0s 106us/sample - loss: 0.3755 - acc: 0.8300\n",
      "Epoch 65/100\n",
      "4507/4507 [==============================] - 0s 98us/sample - loss: 0.3743 - acc: 0.8241\n",
      "Epoch 66/100\n",
      "4507/4507 [==============================] - 0s 103us/sample - loss: 0.3741 - acc: 0.8283\n",
      "Epoch 67/100\n",
      "4507/4507 [==============================] - 0s 104us/sample - loss: 0.3738 - acc: 0.8296\n",
      "Epoch 68/100\n",
      "4507/4507 [==============================] - 0s 103us/sample - loss: 0.3723 - acc: 0.8289\n",
      "Epoch 69/100\n",
      "4507/4507 [==============================] - 0s 106us/sample - loss: 0.3712 - acc: 0.8285\n",
      "Epoch 70/100\n",
      "4507/4507 [==============================] - 0s 101us/sample - loss: 0.3716 - acc: 0.8307\n",
      "Epoch 71/100\n",
      "4507/4507 [==============================] - 1s 116us/sample - loss: 0.3704 - acc: 0.8303\n",
      "Epoch 72/100\n",
      "4507/4507 [==============================] - 0s 104us/sample - loss: 0.3700 - acc: 0.8314\n",
      "Epoch 73/100\n",
      "4507/4507 [==============================] - 0s 101us/sample - loss: 0.3704 - acc: 0.8298\n",
      "Epoch 74/100\n",
      "4507/4507 [==============================] - 0s 105us/sample - loss: 0.3701 - acc: 0.8309\n",
      "Epoch 75/100\n",
      "4507/4507 [==============================] - 0s 98us/sample - loss: 0.3675 - acc: 0.8334\n",
      "Epoch 76/100\n",
      "4507/4507 [==============================] - 0s 98us/sample - loss: 0.3653 - acc: 0.8320\n",
      "Epoch 77/100\n",
      "4507/4507 [==============================] - 1s 128us/sample - loss: 0.3658 - acc: 0.8316\n",
      "Epoch 78/100\n",
      "4507/4507 [==============================] - 1s 118us/sample - loss: 0.3660 - acc: 0.8307\n",
      "Epoch 79/100\n",
      "4507/4507 [==============================] - 0s 106us/sample - loss: 0.3647 - acc: 0.8336\n",
      "Epoch 80/100\n",
      "4507/4507 [==============================] - 0s 107us/sample - loss: 0.3619 - acc: 0.8371\n",
      "Epoch 81/100\n",
      "4507/4507 [==============================] - 1s 127us/sample - loss: 0.3617 - acc: 0.8369\n",
      "Epoch 82/100\n",
      "4507/4507 [==============================] - 1s 197us/sample - loss: 0.3620 - acc: 0.8331\n",
      "Epoch 83/100\n",
      "4507/4507 [==============================] - 1s 160us/sample - loss: 0.3595 - acc: 0.8371\n",
      "Epoch 84/100\n",
      "4507/4507 [==============================] - 1s 180us/sample - loss: 0.3595 - acc: 0.8360\n",
      "Epoch 85/100\n",
      "4507/4507 [==============================] - 1s 155us/sample - loss: 0.3604 - acc: 0.8380\n",
      "Epoch 86/100\n",
      "4507/4507 [==============================] - 1s 111us/sample - loss: 0.3586 - acc: 0.8331\n",
      "Epoch 87/100\n",
      "4507/4507 [==============================] - 1s 126us/sample - loss: 0.3582 - acc: 0.8347\n",
      "Epoch 88/100\n",
      "4507/4507 [==============================] - 1s 121us/sample - loss: 0.3564 - acc: 0.8340\n",
      "Epoch 89/100\n",
      "4507/4507 [==============================] - 1s 128us/sample - loss: 0.3543 - acc: 0.8380\n",
      "Epoch 90/100\n",
      "4507/4507 [==============================] - 1s 121us/sample - loss: 0.3543 - acc: 0.8378\n",
      "Epoch 91/100\n",
      "4507/4507 [==============================] - 1s 124us/sample - loss: 0.3524 - acc: 0.8380\n",
      "Epoch 92/100\n",
      "4507/4507 [==============================] - 0s 104us/sample - loss: 0.3498 - acc: 0.8409\n",
      "Epoch 93/100\n",
      "4507/4507 [==============================] - 1s 112us/sample - loss: 0.3516 - acc: 0.8420\n",
      "Epoch 94/100\n",
      "4507/4507 [==============================] - 1s 116us/sample - loss: 0.3503 - acc: 0.8391\n",
      "Epoch 95/100\n",
      "4507/4507 [==============================] - 0s 110us/sample - loss: 0.3491 - acc: 0.8420\n",
      "Epoch 96/100\n",
      "4507/4507 [==============================] - 1s 112us/sample - loss: 0.3461 - acc: 0.8449\n",
      "Epoch 97/100\n",
      "4507/4507 [==============================] - 0s 102us/sample - loss: 0.3462 - acc: 0.8447\n",
      "Epoch 98/100\n",
      "4507/4507 [==============================] - 0s 96us/sample - loss: 0.3432 - acc: 0.8447\n",
      "Epoch 99/100\n",
      "4507/4507 [==============================] - 0s 103us/sample - loss: 0.3444 - acc: 0.8391\n",
      "Epoch 100/100\n",
      "4507/4507 [==============================] - 0s 102us/sample - loss: 0.3424 - acc: 0.8476\n",
      "1127/1127 [==============================] - 1s 1ms/sample - loss: 0.4899 - acc: 0.7755\n",
      "Epoch 1/100\n",
      "4507/4507 [==============================] - 2s 362us/sample - loss: 0.5299 - acc: 0.7238\n",
      "Epoch 2/100\n",
      "4507/4507 [==============================] - 0s 99us/sample - loss: 0.4409 - acc: 0.7846\n",
      "Epoch 3/100\n",
      "4507/4507 [==============================] - 0s 102us/sample - loss: 0.4258 - acc: 0.7972\n",
      "Epoch 4/100\n",
      "4507/4507 [==============================] - 0s 102us/sample - loss: 0.4206 - acc: 0.8019\n",
      "Epoch 5/100\n",
      "4507/4507 [==============================] - 0s 100us/sample - loss: 0.4193 - acc: 0.8012\n",
      "Epoch 6/100\n",
      "4507/4507 [==============================] - 0s 107us/sample - loss: 0.4156 - acc: 0.8021\n",
      "Epoch 7/100\n",
      "4507/4507 [==============================] - 0s 106us/sample - loss: 0.4159 - acc: 0.8043\n",
      "Epoch 8/100\n",
      "4507/4507 [==============================] - 0s 103us/sample - loss: 0.4168 - acc: 0.8050\n",
      "Epoch 9/100\n",
      "4507/4507 [==============================] - 0s 106us/sample - loss: 0.4154 - acc: 0.8059\n",
      "Epoch 10/100\n",
      "4507/4507 [==============================] - 1s 118us/sample - loss: 0.4127 - acc: 0.8056\n",
      "Epoch 11/100\n",
      "4507/4507 [==============================] - 1s 112us/sample - loss: 0.4122 - acc: 0.8070\n",
      "Epoch 12/100\n",
      "4507/4507 [==============================] - 0s 102us/sample - loss: 0.4126 - acc: 0.8047\n",
      "Epoch 13/100\n",
      "4507/4507 [==============================] - 0s 109us/sample - loss: 0.4125 - acc: 0.8063\n",
      "Epoch 14/100\n",
      "4507/4507 [==============================] - 0s 106us/sample - loss: 0.4127 - acc: 0.8056\n",
      "Epoch 15/100\n",
      "4507/4507 [==============================] - 0s 102us/sample - loss: 0.4121 - acc: 0.8061\n",
      "Epoch 16/100\n",
      "4507/4507 [==============================] - 0s 106us/sample - loss: 0.4109 - acc: 0.8094\n",
      "Epoch 17/100\n",
      "4507/4507 [==============================] - 0s 101us/sample - loss: 0.4112 - acc: 0.8047\n",
      "Epoch 18/100\n",
      "4507/4507 [==============================] - 0s 99us/sample - loss: 0.4105 - acc: 0.8061\n",
      "Epoch 19/100\n",
      "4507/4507 [==============================] - 0s 98us/sample - loss: 0.4127 - acc: 0.8041\n",
      "Epoch 20/100\n",
      "4507/4507 [==============================] - 0s 97us/sample - loss: 0.4119 - acc: 0.8067\n",
      "Epoch 21/100\n",
      "4507/4507 [==============================] - 0s 102us/sample - loss: 0.4093 - acc: 0.8110\n",
      "Epoch 22/100\n",
      "4507/4507 [==============================] - 0s 98us/sample - loss: 0.4098 - acc: 0.8070\n",
      "Epoch 23/100\n",
      "4507/4507 [==============================] - 0s 100us/sample - loss: 0.4094 - acc: 0.8087\n",
      "Epoch 24/100\n",
      "4507/4507 [==============================] - 0s 100us/sample - loss: 0.4109 - acc: 0.8072\n",
      "Epoch 25/100\n",
      "4507/4507 [==============================] - 0s 103us/sample - loss: 0.4095 - acc: 0.8112\n",
      "Epoch 26/100\n",
      "4507/4507 [==============================] - 0s 106us/sample - loss: 0.4106 - acc: 0.8056\n",
      "Epoch 27/100\n",
      "4507/4507 [==============================] - 1s 118us/sample - loss: 0.4096 - acc: 0.8103\n",
      "Epoch 28/100\n",
      "4507/4507 [==============================] - 1s 123us/sample - loss: 0.4084 - acc: 0.8065\n",
      "Epoch 29/100\n",
      "4507/4507 [==============================] - 0s 111us/sample - loss: 0.4077 - acc: 0.8094\n",
      "Epoch 30/100\n",
      "4507/4507 [==============================] - 0s 104us/sample - loss: 0.4076 - acc: 0.8081\n",
      "Epoch 31/100\n",
      "4507/4507 [==============================] - 0s 102us/sample - loss: 0.4078 - acc: 0.8105\n",
      "Epoch 32/100\n",
      "4507/4507 [==============================] - 0s 100us/sample - loss: 0.4067 - acc: 0.8114\n",
      "Epoch 33/100\n",
      "4507/4507 [==============================] - 0s 109us/sample - loss: 0.4060 - acc: 0.8110\n",
      "Epoch 34/100\n",
      "4507/4507 [==============================] - 0s 103us/sample - loss: 0.4067 - acc: 0.8081\n",
      "Epoch 35/100\n",
      "4507/4507 [==============================] - 0s 104us/sample - loss: 0.4079 - acc: 0.8099\n",
      "Epoch 36/100\n",
      "4507/4507 [==============================] - 0s 106us/sample - loss: 0.4071 - acc: 0.8103\n",
      "Epoch 37/100\n",
      "4507/4507 [==============================] - 0s 103us/sample - loss: 0.4044 - acc: 0.8090\n",
      "Epoch 38/100\n",
      "4507/4507 [==============================] - 0s 110us/sample - loss: 0.4055 - acc: 0.8134\n",
      "Epoch 39/100\n",
      "4507/4507 [==============================] - 0s 103us/sample - loss: 0.4042 - acc: 0.8132\n",
      "Epoch 40/100\n",
      "3540/4507 [======================>.......] - ETA: 0s - loss: 0.4027 - acc: 0.8130"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_shape=(inputs,), activation='sigmoid'))\n",
    "    model.add(Dense(64, activation='sigmoid'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [60],\n",
    "              'epochs': [100]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, cv=5, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FfZRtJ7MCN3x"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Try to implement Random Search Hyperparameter Tuning on this dataset\n",
    "- Try to implement Bayesian Optimiation tuning on this dataset using hyperas or hyperopt (if you're brave)\n",
    "- Practice hyperparameter tuning other datasets that we have looked at. How high can you get MNIST? Above 99%?\n",
    "- Study for the Sprint Challenge\n",
    " - Can you implement both perceptron and MLP models from scratch with forward and backpropagation?\n",
    " - Can you implement both perceptron and MLP models in keras and tune their hyperparameters with cross validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_434_Hyperparameter_Tuning_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
