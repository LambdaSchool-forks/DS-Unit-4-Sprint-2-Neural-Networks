diff --git a/module2-backpropagation/LS_DS_422_Gradient_Descent_Backprop_Lecture.ipynb b/module2-backpropagation/LS_DS_422_Gradient_Descent_Backprop_Lecture.ipynb
index 4a1b583..5ecbaf0 100644
--- a/module2-backpropagation/LS_DS_422_Gradient_Descent_Backprop_Lecture.ipynb
+++ b/module2-backpropagation/LS_DS_422_Gradient_Descent_Backprop_Lecture.ipynb
@@ -128,7 +128,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 1,
+   "execution_count": 2,
    "metadata": {
     "colab": {},
     "colab_type": "code",
@@ -142,8 +142,9 @@
     "\n",
     "# hours studying, hours sleep\n",
     "X = np.array(([2,9],\n",
-    "          [1,5],\n",
-    "          [3,6]), dtype=float)\n",
+    "              [1,5],\n",
+    "              [3,6]), dtype=float)\n",
+    "\n",
     "# Exam Scores\n",
     "y = np.array(([92],\n",
     "              [86],\n",
@@ -152,14 +153,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": []
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 3,
+   "execution_count": 8,
    "metadata": {
     "colab": {},
     "colab_type": "code",
@@ -175,9 +169,9 @@
       " [0.33333333 0.55555556]\n",
       " [1.         0.66666667]]\n",
       "Test Score \n",
-      " [[0.92]\n",
-      " [0.86]\n",
-      " [0.89]]\n"
+      " [[0.0092]\n",
+      " [0.0086]\n",
+      " [0.0089]]\n"
      ]
     }
    ],
@@ -185,7 +179,8 @@
     "# Normalizing Data on feature \n",
     "# Neural Network would probably do this on its own, but it will help us converge on a solution faster\n",
     "\n",
-    " = X / np.amax(axis=0)\n",
+    "X = X / np.amax(X, axis=0)\n",
+    "y = y / 100\n",
     "\n",
     "print(\"Studying, Sleeping \\n\", X)\n",
     "print(\"Test Score \\n\", y)"
@@ -203,7 +198,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 12,
+   "execution_count": 9,
    "metadata": {
     "colab": {},
     "colab_type": "code",
@@ -213,12 +208,13 @@
    "source": [
     "class NeuralNetwork:\n",
     "    def __init__(self):\n",
+    "        \n",
     "        # Set up Architecture of Neural Network\n",
     "        self.inputs = 2\n",
     "        self.hiddenNodes = 3\n",
     "        self.outputNodes = 1\n",
     "\n",
-    "        # Initial Weights\n",
+    "        # Initialize Weights\n",
     "        # 2x3 Matrix Array for the First Layer\n",
     "        self.weights1 = np.random.rand(self.inputs, self.hiddenNodes)\n",
     "        \n",
@@ -247,8 +243,7 @@
     "        # Final activation of output\n",
     "        self.activated_output = self.sigmoid(self.output_sum)\n",
     "        \n",
-    "        return self.activated_output\n",
-    "        "
+    "        return self.activated_output"
    ]
   },
   {
@@ -273,7 +268,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 13,
+   "execution_count": 10,
    "metadata": {
     "colab": {},
     "colab_type": "code",
@@ -284,8 +279,8 @@
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "input [2. 9.]\n",
-      "output [0.83906433]\n"
+      "input [0.66666667 1.        ]\n",
+      "output [0.79105842]\n"
      ]
     }
    ],
@@ -300,25 +295,16 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 14,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "##"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 15,
+   "execution_count": 11,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
-       "array([91.16093567])"
+       "array([-0.78185842])"
       ]
      },
-     "execution_count": 15,
+     "execution_count": 11,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -330,7 +316,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 16,
+   "execution_count": 12,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -340,7 +326,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 17,
+   "execution_count": 13,
    "metadata": {},
    "outputs": [
     {
@@ -348,12 +334,12 @@
      "output_type": "stream",
      "text": [
       "Layer 1 wieghts: \n",
-      " [[0.74906093 0.8517804  0.41523945]\n",
-      " [0.39456394 0.69989532 0.90594155]]\n",
+      " [[0.73665387 0.81926386 0.88654768]\n",
+      " [0.92386319 0.28768809 0.29744598]]\n",
       "Layer 2 wieghts: \n",
-      " [[0.09945959]\n",
-      " [0.75522925]\n",
-      " [0.79758034]]\n"
+      " [[0.43328179]\n",
+      " [0.56557272]\n",
+      " [0.83042626]]\n"
      ]
     }
    ],
diff --git a/module3-Intro-to-Keras/LS_DS_423_Keras_Lecture.ipynb b/module3-Intro-to-Keras/LS_DS_423_Keras_Lecture.ipynb
index 9233c24..1338c0a 100644
--- a/module3-Intro-to-Keras/LS_DS_423_Keras_Lecture.ipynb
+++ b/module3-Intro-to-Keras/LS_DS_423_Keras_Lecture.ipynb
@@ -2075,9 +2075,18 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 17,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "10000/10000 [==============================] - 1s 67us/sample - loss: 0.2544 - acc: 0.9351\n",
+      "acc: 93.51000189781189\n"
+     ]
+    }
+   ],
    "source": [
     "history = mnist_model.fit(X_train, y_train, batch_size=32, epochs=epochs, validation_split=.1, verbose=0)\n",
     "scores = mnist_model.evaluate(X_test, y_test)\n",
diff --git a/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Lecture.ipynb b/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Lecture.ipynb
index f4baa72..e6ea315 100644
--- a/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Lecture.ipynb
+++ b/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Lecture.ipynb
@@ -19,7 +19,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 5,
+   "execution_count": 1,
    "metadata": {
     "colab": {},
     "colab_type": "code",
@@ -36,6 +36,43 @@
     "numpy.random.seed(42)"
    ]
   },
+  {
+   "cell_type": "code",
+   "execution_count": 2,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/html": [
+       "\n",
+       "        Notebook configured with <a href=\"https://wandb.com\" target=\"_blank\">W&B</a>. You can <a href=\"https://app.wandb.ai/ds5/ds5-hyperparameter-tuning/runs/m3yffucc\" target=\"_blank\">open</a> the run page, or call <code>%%wandb</code>\n",
+       "        in a cell containing your training loop to display live results.  Learn more in our <a href=\"https://docs.wandb.com/docs/integrations/jupyter.html\" target=\"_blank\">docs</a>.\n",
+       "    "
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "data": {
+      "text/plain": [
+       "W&B Run: https://app.wandb.ai/ds5/ds5-hyperparameter-tuning/runs/m3yffucc"
+      ]
+     },
+     "execution_count": 2,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "import wandb\n",
+    "from wandb.keras import WandbCallback\n",
+    "wandb.init(project=\"ds5-hyperparameter-tuning\", entity=\"ds5\")"
+   ]
+  },
   {
    "cell_type": "markdown",
    "metadata": {
@@ -50,7 +87,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 7,
+   "execution_count": 3,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -211,7 +248,7 @@
        "4     18.7  396.90   5.33  36.2  "
       ]
      },
-     "execution_count": 7,
+     "execution_count": 3,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -242,7 +279,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 8,
+   "execution_count": 5,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -300,7 +337,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 9,
+   "execution_count": 17,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -315,10 +352,7 @@
      "name": "stderr",
      "output_type": "stream",
      "text": [
-      "WARNING: Logging before flag parsing goes to stderr.\n",
-      "W0815 10:08:20.955155 4430419392 deprecation.py:506] From /Users/jonathansokoll/anaconda3/envs/U4-S2-NNF/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
-      "Instructions for updating:\n",
-      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
+      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n"
      ]
     },
     {
@@ -326,115 +360,1015 @@
      "output_type": "stream",
      "text": [
       "Train on 339 samples, validate on 167 samples\n",
-      "Epoch 1/50\n",
-      "339/339 [==============================] - 0s 366us/sample - loss: 625.2513 - mean_squared_error: 625.2513 - val_loss: 279.9214 - val_mean_squared_error: 279.9214\n",
-      "Epoch 2/50\n",
-      "339/339 [==============================] - 0s 95us/sample - loss: 373.3546 - mean_squared_error: 373.3546 - val_loss: 180.1021 - val_mean_squared_error: 180.1021\n",
-      "Epoch 3/50\n",
-      "339/339 [==============================] - 0s 93us/sample - loss: 101.2103 - mean_squared_error: 101.2103 - val_loss: 137.3386 - val_mean_squared_error: 137.3387\n",
-      "Epoch 4/50\n",
-      "339/339 [==============================] - 0s 84us/sample - loss: 33.7659 - mean_squared_error: 33.7659 - val_loss: 114.6741 - val_mean_squared_error: 114.6741\n",
-      "Epoch 5/50\n",
-      "339/339 [==============================] - 0s 82us/sample - loss: 18.5104 - mean_squared_error: 18.5104 - val_loss: 113.3629 - val_mean_squared_error: 113.3628\n",
-      "Epoch 6/50\n",
-      "339/339 [==============================] - 0s 91us/sample - loss: 14.5301 - mean_squared_error: 14.5301 - val_loss: 117.4675 - val_mean_squared_error: 117.4675\n",
-      "Epoch 7/50\n",
-      "339/339 [==============================] - 0s 92us/sample - loss: 13.1114 - mean_squared_error: 13.1114 - val_loss: 115.7412 - val_mean_squared_error: 115.7412\n",
-      "Epoch 8/50\n",
-      "339/339 [==============================] - 0s 95us/sample - loss: 12.1406 - mean_squared_error: 12.1406 - val_loss: 112.9944 - val_mean_squared_error: 112.9944\n",
-      "Epoch 9/50\n",
-      "339/339 [==============================] - 0s 89us/sample - loss: 11.5139 - mean_squared_error: 11.5139 - val_loss: 110.0447 - val_mean_squared_error: 110.0447\n",
-      "Epoch 10/50\n",
-      "339/339 [==============================] - 0s 88us/sample - loss: 10.9077 - mean_squared_error: 10.9077 - val_loss: 106.9348 - val_mean_squared_error: 106.9348\n",
-      "Epoch 11/50\n",
-      "339/339 [==============================] - 0s 85us/sample - loss: 10.5648 - mean_squared_error: 10.5648 - val_loss: 103.4860 - val_mean_squared_error: 103.4860\n",
-      "Epoch 12/50\n",
-      "339/339 [==============================] - 0s 87us/sample - loss: 9.9297 - mean_squared_error: 9.9297 - val_loss: 100.5036 - val_mean_squared_error: 100.5036\n",
-      "Epoch 13/50\n",
-      "339/339 [==============================] - 0s 87us/sample - loss: 9.4464 - mean_squared_error: 9.4464 - val_loss: 98.4739 - val_mean_squared_error: 98.4739\n",
-      "Epoch 14/50\n",
-      "339/339 [==============================] - 0s 84us/sample - loss: 9.1621 - mean_squared_error: 9.1621 - val_loss: 95.9992 - val_mean_squared_error: 95.9992\n",
-      "Epoch 15/50\n",
-      "339/339 [==============================] - 0s 85us/sample - loss: 8.9122 - mean_squared_error: 8.9122 - val_loss: 91.8091 - val_mean_squared_error: 91.8091\n",
-      "Epoch 16/50\n",
-      "339/339 [==============================] - 0s 86us/sample - loss: 8.5564 - mean_squared_error: 8.5564 - val_loss: 91.1116 - val_mean_squared_error: 91.1116\n",
-      "Epoch 17/50\n",
-      "339/339 [==============================] - 0s 87us/sample - loss: 8.0328 - mean_squared_error: 8.0328 - val_loss: 87.0650 - val_mean_squared_error: 87.0650\n",
-      "Epoch 18/50\n",
-      "339/339 [==============================] - 0s 83us/sample - loss: 7.7535 - mean_squared_error: 7.7535 - val_loss: 86.3742 - val_mean_squared_error: 86.3742\n",
-      "Epoch 19/50\n",
-      "339/339 [==============================] - 0s 86us/sample - loss: 7.5547 - mean_squared_error: 7.5547 - val_loss: 82.6835 - val_mean_squared_error: 82.6835\n",
-      "Epoch 20/50\n",
-      "339/339 [==============================] - 0s 85us/sample - loss: 7.3827 - mean_squared_error: 7.3827 - val_loss: 81.1169 - val_mean_squared_error: 81.1169\n",
-      "Epoch 21/50\n",
-      "339/339 [==============================] - 0s 83us/sample - loss: 7.1717 - mean_squared_error: 7.1717 - val_loss: 78.6616 - val_mean_squared_error: 78.6616\n",
-      "Epoch 22/50\n",
-      "339/339 [==============================] - 0s 84us/sample - loss: 6.9017 - mean_squared_error: 6.9017 - val_loss: 77.7369 - val_mean_squared_error: 77.7369\n",
-      "Epoch 23/50\n",
-      "339/339 [==============================] - 0s 83us/sample - loss: 6.7186 - mean_squared_error: 6.7186 - val_loss: 74.8546 - val_mean_squared_error: 74.8546\n",
-      "Epoch 24/50\n",
-      "339/339 [==============================] - 0s 83us/sample - loss: 6.4259 - mean_squared_error: 6.4259 - val_loss: 72.0877 - val_mean_squared_error: 72.0877\n",
-      "Epoch 25/50\n",
-      "339/339 [==============================] - 0s 86us/sample - loss: 6.5469 - mean_squared_error: 6.5469 - val_loss: 70.2001 - val_mean_squared_error: 70.2001\n",
-      "Epoch 26/50\n",
-      "339/339 [==============================] - 0s 86us/sample - loss: 6.1998 - mean_squared_error: 6.1998 - val_loss: 71.6276 - val_mean_squared_error: 71.6276\n",
-      "Epoch 27/50\n",
-      "339/339 [==============================] - 0s 86us/sample - loss: 6.1886 - mean_squared_error: 6.1886 - val_loss: 66.5151 - val_mean_squared_error: 66.5151\n",
-      "Epoch 28/50\n",
-      "339/339 [==============================] - 0s 86us/sample - loss: 5.9647 - mean_squared_error: 5.9647 - val_loss: 64.1536 - val_mean_squared_error: 64.1536\n",
-      "Epoch 29/50\n",
-      "339/339 [==============================] - 0s 82us/sample - loss: 5.8475 - mean_squared_error: 5.8475 - val_loss: 62.9218 - val_mean_squared_error: 62.9218\n",
-      "Epoch 30/50\n",
-      "339/339 [==============================] - 0s 88us/sample - loss: 5.7472 - mean_squared_error: 5.7472 - val_loss: 61.4670 - val_mean_squared_error: 61.4670\n",
-      "Epoch 31/50\n",
-      "339/339 [==============================] - 0s 88us/sample - loss: 5.6262 - mean_squared_error: 5.6262 - val_loss: 60.4396 - val_mean_squared_error: 60.4396\n",
-      "Epoch 32/50\n",
-      "339/339 [==============================] - 0s 84us/sample - loss: 5.5064 - mean_squared_error: 5.5064 - val_loss: 59.4765 - val_mean_squared_error: 59.4765\n",
-      "Epoch 33/50\n",
-      "339/339 [==============================] - 0s 84us/sample - loss: 5.4368 - mean_squared_error: 5.4368 - val_loss: 57.5645 - val_mean_squared_error: 57.5645\n",
-      "Epoch 34/50\n",
-      "339/339 [==============================] - 0s 84us/sample - loss: 5.3329 - mean_squared_error: 5.3329 - val_loss: 54.6676 - val_mean_squared_error: 54.6676\n",
-      "Epoch 35/50\n",
-      "339/339 [==============================] - 0s 85us/sample - loss: 5.2066 - mean_squared_error: 5.2066 - val_loss: 56.0873 - val_mean_squared_error: 56.0873\n",
-      "Epoch 36/50\n",
-      "339/339 [==============================] - 0s 91us/sample - loss: 5.2098 - mean_squared_error: 5.2098 - val_loss: 53.7955 - val_mean_squared_error: 53.7955\n",
-      "Epoch 37/50\n",
-      "339/339 [==============================] - 0s 87us/sample - loss: 5.4417 - mean_squared_error: 5.4417 - val_loss: 52.4190 - val_mean_squared_error: 52.4189\n",
-      "Epoch 38/50\n",
-      "339/339 [==============================] - 0s 85us/sample - loss: 4.9801 - mean_squared_error: 4.9801 - val_loss: 51.0138 - val_mean_squared_error: 51.0138\n",
-      "Epoch 39/50\n",
-      "339/339 [==============================] - 0s 94us/sample - loss: 5.0640 - mean_squared_error: 5.0640 - val_loss: 50.9198 - val_mean_squared_error: 50.9198\n",
-      "Epoch 40/50\n",
-      "339/339 [==============================] - 0s 97us/sample - loss: 5.0598 - mean_squared_error: 5.0598 - val_loss: 51.4312 - val_mean_squared_error: 51.4312\n",
-      "Epoch 41/50\n",
-      "339/339 [==============================] - 0s 94us/sample - loss: 4.9155 - mean_squared_error: 4.9155 - val_loss: 49.5652 - val_mean_squared_error: 49.5652\n",
-      "Epoch 42/50\n",
-      "339/339 [==============================] - 0s 95us/sample - loss: 4.6731 - mean_squared_error: 4.6731 - val_loss: 47.9462 - val_mean_squared_error: 47.9462\n",
-      "Epoch 43/50\n",
-      "339/339 [==============================] - 0s 93us/sample - loss: 4.7018 - mean_squared_error: 4.7018 - val_loss: 48.1317 - val_mean_squared_error: 48.1317\n",
-      "Epoch 44/50\n",
-      "339/339 [==============================] - 0s 89us/sample - loss: 4.8628 - mean_squared_error: 4.8628 - val_loss: 47.4068 - val_mean_squared_error: 47.4068\n",
-      "Epoch 45/50\n",
-      "339/339 [==============================] - 0s 90us/sample - loss: 4.5611 - mean_squared_error: 4.5611 - val_loss: 47.3415 - val_mean_squared_error: 47.3415\n",
-      "Epoch 46/50\n",
-      "339/339 [==============================] - 0s 98us/sample - loss: 4.6660 - mean_squared_error: 4.6660 - val_loss: 45.0834 - val_mean_squared_error: 45.0834\n",
-      "Epoch 47/50\n",
-      "339/339 [==============================] - 0s 93us/sample - loss: 4.5393 - mean_squared_error: 4.5393 - val_loss: 44.8492 - val_mean_squared_error: 44.8492\n",
-      "Epoch 48/50\n",
-      "339/339 [==============================] - 0s 95us/sample - loss: 4.4971 - mean_squared_error: 4.4971 - val_loss: 44.5613 - val_mean_squared_error: 44.5613\n",
-      "Epoch 49/50\n",
-      "339/339 [==============================] - 0s 94us/sample - loss: 4.5186 - mean_squared_error: 4.5186 - val_loss: 42.3793 - val_mean_squared_error: 42.3793\n",
-      "Epoch 50/50\n",
-      "339/339 [==============================] - 0s 95us/sample - loss: 4.4599 - mean_squared_error: 4.4599 - val_loss: 44.0912 - val_mean_squared_error: 44.0912\n"
+      "Epoch 1/500\n",
+      "339/339 [==============================] - 1s 4ms/sample - loss: 707.1253 - mean_squared_error: 707.1253 - val_loss: 367.0487 - val_mean_squared_error: 367.0487\n",
+      "Epoch 2/500\n",
+      "339/339 [==============================] - 0s 120us/sample - loss: 703.2657 - mean_squared_error: 703.2657 - val_loss: 364.4439 - val_mean_squared_error: 364.4439\n",
+      "Epoch 3/500\n",
+      "339/339 [==============================] - 0s 126us/sample - loss: 699.3994 - mean_squared_error: 699.3994 - val_loss: 361.8493 - val_mean_squared_error: 361.8493\n",
+      "Epoch 4/500\n",
+      "339/339 [==============================] - 0s 119us/sample - loss: 695.5380 - mean_squared_error: 695.5380 - val_loss: 359.2580 - val_mean_squared_error: 359.2580\n",
+      "Epoch 5/500\n",
+      "339/339 [==============================] - 0s 117us/sample - loss: 691.6714 - mean_squared_error: 691.6714 - val_loss: 356.6697 - val_mean_squared_error: 356.6697\n",
+      "Epoch 6/500\n",
+      "339/339 [==============================] - 0s 131us/sample - loss: 687.7928 - mean_squared_error: 687.7928 - val_loss: 354.0831 - val_mean_squared_error: 354.0831\n",
+      "Epoch 7/500\n",
+      "339/339 [==============================] - 0s 129us/sample - loss: 683.8965 - mean_squared_error: 683.8965 - val_loss: 351.4958 - val_mean_squared_error: 351.4958\n",
+      "Epoch 8/500\n",
+      "339/339 [==============================] - 0s 127us/sample - loss: 679.9855 - mean_squared_error: 679.9855 - val_loss: 348.9025 - val_mean_squared_error: 348.9025\n",
+      "Epoch 9/500\n",
+      "339/339 [==============================] - 0s 130us/sample - loss: 676.0427 - mean_squared_error: 676.0427 - val_loss: 346.2984 - val_mean_squared_error: 346.2984\n",
+      "Epoch 10/500\n",
+      "339/339 [==============================] - 0s 129us/sample - loss: 672.0599 - mean_squared_error: 672.0599 - val_loss: 343.6833 - val_mean_squared_error: 343.6833\n",
+      "Epoch 11/500\n",
+      "339/339 [==============================] - 0s 127us/sample - loss: 668.0358 - mean_squared_error: 668.0358 - val_loss: 341.0604 - val_mean_squared_error: 341.0604\n",
+      "Epoch 12/500\n",
+      "339/339 [==============================] - 0s 125us/sample - loss: 663.9683 - mean_squared_error: 663.9683 - val_loss: 338.4159 - val_mean_squared_error: 338.4159\n",
+      "Epoch 13/500\n",
+      "339/339 [==============================] - 0s 127us/sample - loss: 659.8496 - mean_squared_error: 659.8496 - val_loss: 335.7444 - val_mean_squared_error: 335.7444\n",
+      "Epoch 14/500\n",
+      "339/339 [==============================] - 0s 122us/sample - loss: 655.6631 - mean_squared_error: 655.6631 - val_loss: 333.0386 - val_mean_squared_error: 333.0386\n",
+      "Epoch 15/500\n",
+      "339/339 [==============================] - 0s 128us/sample - loss: 651.3896 - mean_squared_error: 651.3896 - val_loss: 330.2949 - val_mean_squared_error: 330.2949\n",
+      "Epoch 16/500\n",
+      "339/339 [==============================] - 0s 133us/sample - loss: 647.0135 - mean_squared_error: 647.0135 - val_loss: 327.5188 - val_mean_squared_error: 327.5188\n",
+      "Epoch 17/500\n",
+      "339/339 [==============================] - 0s 137us/sample - loss: 642.5370 - mean_squared_error: 642.5370 - val_loss: 324.7115 - val_mean_squared_error: 324.7115\n",
+      "Epoch 18/500\n",
+      "339/339 [==============================] - 0s 124us/sample - loss: 637.9396 - mean_squared_error: 637.9396 - val_loss: 321.8572 - val_mean_squared_error: 321.8572\n",
+      "Epoch 19/500\n",
+      "339/339 [==============================] - 0s 124us/sample - loss: 633.2122 - mean_squared_error: 633.2122 - val_loss: 318.9758 - val_mean_squared_error: 318.9758\n",
+      "Epoch 20/500\n",
+      "339/339 [==============================] - 0s 129us/sample - loss: 628.3228 - mean_squared_error: 628.3228 - val_loss: 316.0584 - val_mean_squared_error: 316.0584\n",
+      "Epoch 21/500\n",
+      "339/339 [==============================] - 0s 128us/sample - loss: 623.2594 - mean_squared_error: 623.2594 - val_loss: 313.1057 - val_mean_squared_error: 313.1057\n",
+      "Epoch 22/500\n",
+      "339/339 [==============================] - 0s 126us/sample - loss: 618.0065 - mean_squared_error: 618.0065 - val_loss: 310.1033 - val_mean_squared_error: 310.1033\n",
+      "Epoch 23/500\n",
+      "339/339 [==============================] - 0s 129us/sample - loss: 612.5513 - mean_squared_error: 612.5513 - val_loss: 307.0578 - val_mean_squared_error: 307.0578\n",
+      "Epoch 24/500\n",
+      "339/339 [==============================] - 0s 133us/sample - loss: 606.8658 - mean_squared_error: 606.8658 - val_loss: 303.9559 - val_mean_squared_error: 303.9559\n",
+      "Epoch 25/500\n",
+      "339/339 [==============================] - 0s 283us/sample - loss: 600.9410 - mean_squared_error: 600.9410 - val_loss: 300.8027 - val_mean_squared_error: 300.8027\n",
+      "Epoch 26/500\n",
+      "339/339 [==============================] - 0s 202us/sample - loss: 594.7741 - mean_squared_error: 594.7741 - val_loss: 297.6038 - val_mean_squared_error: 297.6038\n",
+      "Epoch 27/500\n",
+      "339/339 [==============================] - 0s 152us/sample - loss: 588.3511 - mean_squared_error: 588.3511 - val_loss: 294.3595 - val_mean_squared_error: 294.3595\n",
+      "Epoch 28/500\n",
+      "339/339 [==============================] - 0s 114us/sample - loss: 581.6686 - mean_squared_error: 581.6686 - val_loss: 291.0607 - val_mean_squared_error: 291.0607\n",
+      "Epoch 29/500\n",
+      "339/339 [==============================] - 0s 127us/sample - loss: 574.7027 - mean_squared_error: 574.7027 - val_loss: 287.7221 - val_mean_squared_error: 287.7221\n",
+      "Epoch 30/500\n",
+      "339/339 [==============================] - 0s 117us/sample - loss: 567.4366 - mean_squared_error: 567.4366 - val_loss: 284.3358 - val_mean_squared_error: 284.3358\n",
+      "Epoch 31/500\n",
+      "339/339 [==============================] - 0s 133us/sample - loss: 559.8574 - mean_squared_error: 559.8574 - val_loss: 280.9123 - val_mean_squared_error: 280.9123\n",
+      "Epoch 32/500\n",
+      "339/339 [==============================] - 0s 127us/sample - loss: 551.9579 - mean_squared_error: 551.9579 - val_loss: 277.4510 - val_mean_squared_error: 277.4510\n",
+      "Epoch 33/500\n",
+      "339/339 [==============================] - 0s 133us/sample - loss: 543.7377 - mean_squared_error: 543.7377 - val_loss: 273.9405 - val_mean_squared_error: 273.9405\n",
+      "Epoch 34/500\n",
+      "339/339 [==============================] - 0s 131us/sample - loss: 535.1998 - mean_squared_error: 535.1998 - val_loss: 270.3871 - val_mean_squared_error: 270.3871\n",
+      "Epoch 35/500\n",
+      "339/339 [==============================] - 0s 130us/sample - loss: 526.3381 - mean_squared_error: 526.3381 - val_loss: 266.7899 - val_mean_squared_error: 266.7899\n",
+      "Epoch 36/500\n",
+      "339/339 [==============================] - 0s 128us/sample - loss: 517.1467 - mean_squared_error: 517.1467 - val_loss: 263.1567 - val_mean_squared_error: 263.1567\n",
+      "Epoch 37/500\n",
+      "339/339 [==============================] - 0s 128us/sample - loss: 507.6341 - mean_squared_error: 507.6341 - val_loss: 259.4998 - val_mean_squared_error: 259.4998\n",
+      "Epoch 38/500\n",
+      "339/339 [==============================] - 0s 133us/sample - loss: 497.7962 - mean_squared_error: 497.7962 - val_loss: 255.8110 - val_mean_squared_error: 255.8110\n",
+      "Epoch 39/500\n",
+      "339/339 [==============================] - 0s 140us/sample - loss: 487.6231 - mean_squared_error: 487.6231 - val_loss: 252.0955 - val_mean_squared_error: 252.0955\n",
+      "Epoch 40/500\n",
+      "339/339 [==============================] - 0s 205us/sample - loss: 477.1282 - mean_squared_error: 477.1282 - val_loss: 248.3496 - val_mean_squared_error: 248.3496\n",
+      "Epoch 41/500\n",
+      "339/339 [==============================] - 0s 252us/sample - loss: 466.3199 - mean_squared_error: 466.3199 - val_loss: 244.5787 - val_mean_squared_error: 244.5787\n",
+      "Epoch 42/500\n",
+      "339/339 [==============================] - 0s 178us/sample - loss: 455.2078 - mean_squared_error: 455.2078 - val_loss: 240.7859 - val_mean_squared_error: 240.7859\n",
+      "Epoch 43/500\n",
+      "339/339 [==============================] - 0s 121us/sample - loss: 443.7898 - mean_squared_error: 443.7898 - val_loss: 236.9764 - val_mean_squared_error: 236.9764\n",
+      "Epoch 44/500\n",
+      "339/339 [==============================] - 0s 134us/sample - loss: 432.0805 - mean_squared_error: 432.0805 - val_loss: 233.1506 - val_mean_squared_error: 233.1506\n",
+      "Epoch 45/500\n",
+      "339/339 [==============================] - 0s 116us/sample - loss: 420.0817 - mean_squared_error: 420.0817 - val_loss: 229.3112 - val_mean_squared_error: 229.3112\n",
+      "Epoch 46/500\n",
+      "339/339 [==============================] - 0s 127us/sample - loss: 407.8080 - mean_squared_error: 407.8080 - val_loss: 225.4674 - val_mean_squared_error: 225.4674\n",
+      "Epoch 47/500\n",
+      "339/339 [==============================] - 0s 137us/sample - loss: 395.2823 - mean_squared_error: 395.2823 - val_loss: 221.6264 - val_mean_squared_error: 221.6264\n",
+      "Epoch 48/500\n",
+      "339/339 [==============================] - 0s 140us/sample - loss: 382.5212 - mean_squared_error: 382.5212 - val_loss: 217.7952 - val_mean_squared_error: 217.7952\n",
+      "Epoch 49/500\n",
+      "339/339 [==============================] - 0s 150us/sample - loss: 369.5437 - mean_squared_error: 369.5437 - val_loss: 213.9741 - val_mean_squared_error: 213.9741\n",
+      "Epoch 50/500\n",
+      "339/339 [==============================] - 0s 142us/sample - loss: 356.3770 - mean_squared_error: 356.3770 - val_loss: 210.1681 - val_mean_squared_error: 210.1681\n",
+      "Epoch 51/500\n",
+      "339/339 [==============================] - 0s 154us/sample - loss: 343.0487 - mean_squared_error: 343.0487 - val_loss: 206.3766 - val_mean_squared_error: 206.3766\n",
+      "Epoch 52/500\n",
+      "339/339 [==============================] - 0s 140us/sample - loss: 329.5905 - mean_squared_error: 329.5905 - val_loss: 202.6082 - val_mean_squared_error: 202.6082\n",
+      "Epoch 53/500\n",
+      "339/339 [==============================] - 0s 139us/sample - loss: 316.0431 - mean_squared_error: 316.0431 - val_loss: 198.8748 - val_mean_squared_error: 198.8748\n",
+      "Epoch 54/500\n",
+      "339/339 [==============================] - 0s 146us/sample - loss: 302.4322 - mean_squared_error: 302.4322 - val_loss: 195.1857 - val_mean_squared_error: 195.1857\n",
+      "Epoch 55/500\n",
+      "339/339 [==============================] - 0s 152us/sample - loss: 288.7947 - mean_squared_error: 288.7947 - val_loss: 191.5491 - val_mean_squared_error: 191.5491\n",
+      "Epoch 56/500\n",
+      "339/339 [==============================] - 0s 199us/sample - loss: 275.1744 - mean_squared_error: 275.1744 - val_loss: 187.9731 - val_mean_squared_error: 187.9731\n",
+      "Epoch 57/500\n",
+      "339/339 [==============================] - 0s 189us/sample - loss: 261.6028 - mean_squared_error: 261.6028 - val_loss: 184.4651 - val_mean_squared_error: 184.4651\n",
+      "Epoch 58/500\n",
+      "339/339 [==============================] - 0s 148us/sample - loss: 248.1246 - mean_squared_error: 248.1246 - val_loss: 181.0161 - val_mean_squared_error: 181.0161\n",
+      "Epoch 59/500\n",
+      "339/339 [==============================] - 0s 136us/sample - loss: 234.7888 - mean_squared_error: 234.7888 - val_loss: 177.6445 - val_mean_squared_error: 177.6445\n",
+      "Epoch 60/500\n",
+      "339/339 [==============================] - 0s 221us/sample - loss: 221.6376 - mean_squared_error: 221.6376 - val_loss: 174.3722 - val_mean_squared_error: 174.3722\n",
+      "Epoch 61/500\n",
+      "339/339 [==============================] - 0s 150us/sample - loss: 208.7269 - mean_squared_error: 208.7269 - val_loss: 171.2090 - val_mean_squared_error: 171.2090\n",
+      "Epoch 62/500\n",
+      "339/339 [==============================] - 0s 472us/sample - loss: 196.1066 - mean_squared_error: 196.1066 - val_loss: 168.1622 - val_mean_squared_error: 168.1622\n",
+      "Epoch 63/500\n",
+      "339/339 [==============================] - 0s 193us/sample - loss: 183.8197 - mean_squared_error: 183.8197 - val_loss: 165.2400 - val_mean_squared_error: 165.2400\n",
+      "Epoch 64/500\n",
+      "339/339 [==============================] - 0s 122us/sample - loss: 171.9232 - mean_squared_error: 171.9232 - val_loss: 162.4459 - val_mean_squared_error: 162.4459\n",
+      "Epoch 65/500\n",
+      "339/339 [==============================] - 0s 126us/sample - loss: 160.4694 - mean_squared_error: 160.4694 - val_loss: 159.7809 - val_mean_squared_error: 159.7809\n",
+      "Epoch 66/500\n",
+      "339/339 [==============================] - 0s 190us/sample - loss: 149.5101 - mean_squared_error: 149.5101 - val_loss: 157.2634 - val_mean_squared_error: 157.2634\n",
+      "Epoch 67/500\n",
+      "339/339 [==============================] - 0s 312us/sample - loss: 139.0836 - mean_squared_error: 139.0836 - val_loss: 154.8961 - val_mean_squared_error: 154.8961\n",
+      "Epoch 68/500\n",
+      "339/339 [==============================] - 0s 203us/sample - loss: 129.2272 - mean_squared_error: 129.2272 - val_loss: 152.6814 - val_mean_squared_error: 152.6814\n",
+      "Epoch 69/500\n",
+      "339/339 [==============================] - 0s 168us/sample - loss: 119.9741 - mean_squared_error: 119.9741 - val_loss: 150.6147 - val_mean_squared_error: 150.6147\n",
+      "Epoch 70/500\n",
+      "339/339 [==============================] - 0s 179us/sample - loss: 111.3420 - mean_squared_error: 111.3420 - val_loss: 148.6955 - val_mean_squared_error: 148.6955\n",
+      "Epoch 71/500\n",
+      "339/339 [==============================] - 0s 146us/sample - loss: 103.3607 - mean_squared_error: 103.3607 - val_loss: 146.9184 - val_mean_squared_error: 146.9184\n",
+      "Epoch 72/500\n",
+      "339/339 [==============================] - 0s 227us/sample - loss: 96.0412 - mean_squared_error: 96.0412 - val_loss: 145.2744 - val_mean_squared_error: 145.2744\n",
+      "Epoch 73/500\n",
+      "339/339 [==============================] - 0s 205us/sample - loss: 89.3852 - mean_squared_error: 89.3852 - val_loss: 143.7540 - val_mean_squared_error: 143.7540\n",
+      "Epoch 74/500\n",
+      "339/339 [==============================] - 0s 149us/sample - loss: 83.3809 - mean_squared_error: 83.3809 - val_loss: 142.3436 - val_mean_squared_error: 142.3436\n",
+      "Epoch 75/500\n",
+      "339/339 [==============================] - 0s 138us/sample - loss: 78.0112 - mean_squared_error: 78.0112 - val_loss: 141.0155 - val_mean_squared_error: 141.0155\n",
+      "Epoch 76/500\n",
+      "339/339 [==============================] - 0s 163us/sample - loss: 73.2511 - mean_squared_error: 73.2511 - val_loss: 139.7559 - val_mean_squared_error: 139.7559\n",
+      "Epoch 77/500\n",
+      "339/339 [==============================] - 0s 126us/sample - loss: 69.0616 - mean_squared_error: 69.0616 - val_loss: 138.5465 - val_mean_squared_error: 138.5465\n",
+      "Epoch 78/500\n",
+      "339/339 [==============================] - 0s 132us/sample - loss: 65.3984 - mean_squared_error: 65.3984 - val_loss: 137.3688 - val_mean_squared_error: 137.3688\n",
+      "Epoch 79/500\n",
+      "339/339 [==============================] - 0s 132us/sample - loss: 62.2098 - mean_squared_error: 62.2098 - val_loss: 136.2114 - val_mean_squared_error: 136.2114\n",
+      "Epoch 80/500\n",
+      "339/339 [==============================] - 0s 144us/sample - loss: 59.4372 - mean_squared_error: 59.4372 - val_loss: 135.0585 - val_mean_squared_error: 135.0585\n",
+      "Epoch 81/500\n",
+      "339/339 [==============================] - 0s 126us/sample - loss: 57.0228 - mean_squared_error: 57.0228 - val_loss: 133.8944 - val_mean_squared_error: 133.8944\n",
+      "Epoch 82/500\n",
+      "339/339 [==============================] - 0s 126us/sample - loss: 54.9094 - mean_squared_error: 54.9094 - val_loss: 132.7035 - val_mean_squared_error: 132.7035\n",
+      "Epoch 83/500\n",
+      "339/339 [==============================] - 0s 159us/sample - loss: 53.0387 - mean_squared_error: 53.0387 - val_loss: 131.4763 - val_mean_squared_error: 131.4763\n",
+      "Epoch 84/500\n",
+      "339/339 [==============================] - 0s 161us/sample - loss: 51.3538 - mean_squared_error: 51.3538 - val_loss: 130.2090 - val_mean_squared_error: 130.2090\n",
+      "Epoch 85/500\n",
+      "339/339 [==============================] - 0s 139us/sample - loss: 49.8059 - mean_squared_error: 49.8059 - val_loss: 128.8986 - val_mean_squared_error: 128.8986\n",
+      "Epoch 86/500\n",
+      "339/339 [==============================] - 0s 131us/sample - loss: 48.3526 - mean_squared_error: 48.3526 - val_loss: 127.5456 - val_mean_squared_error: 127.5456\n",
+      "Epoch 87/500\n",
+      "339/339 [==============================] - 0s 119us/sample - loss: 46.9576 - mean_squared_error: 46.9576 - val_loss: 126.1530 - val_mean_squared_error: 126.1530\n",
+      "Epoch 88/500\n",
+      "339/339 [==============================] - 0s 123us/sample - loss: 45.5930 - mean_squared_error: 45.5930 - val_loss: 124.7261 - val_mean_squared_error: 124.7261\n",
+      "Epoch 89/500\n",
+      "339/339 [==============================] - 0s 174us/sample - loss: 44.2361 - mean_squared_error: 44.2361 - val_loss: 123.2735 - val_mean_squared_error: 123.2735\n",
+      "Epoch 90/500\n",
+      "339/339 [==============================] - 0s 127us/sample - loss: 42.8740 - mean_squared_error: 42.8740 - val_loss: 121.8046 - val_mean_squared_error: 121.8046\n",
+      "Epoch 91/500\n",
+      "339/339 [==============================] - 0s 124us/sample - loss: 41.5007 - mean_squared_error: 41.5007 - val_loss: 120.3249 - val_mean_squared_error: 120.3249\n",
+      "Epoch 92/500\n",
+      "339/339 [==============================] - 0s 130us/sample - loss: 40.1160 - mean_squared_error: 40.1160 - val_loss: 118.8442 - val_mean_squared_error: 118.8442\n",
+      "Epoch 93/500\n",
+      "339/339 [==============================] - 0s 129us/sample - loss: 38.7237 - mean_squared_error: 38.7237 - val_loss: 117.3716 - val_mean_squared_error: 117.3716\n",
+      "Epoch 94/500\n",
+      "339/339 [==============================] - 0s 130us/sample - loss: 37.3324 - mean_squared_error: 37.3324 - val_loss: 115.9180 - val_mean_squared_error: 115.9180\n",
+      "Epoch 95/500\n",
+      "339/339 [==============================] - 0s 128us/sample - loss: 35.9510 - mean_squared_error: 35.9510 - val_loss: 114.4956 - val_mean_squared_error: 114.4956\n",
+      "Epoch 96/500\n",
+      "339/339 [==============================] - 0s 121us/sample - loss: 34.5931 - mean_squared_error: 34.5931 - val_loss: 113.1129 - val_mean_squared_error: 113.1129\n",
+      "Epoch 97/500\n",
+      "339/339 [==============================] - 0s 399us/sample - loss: 33.2691 - mean_squared_error: 33.2691 - val_loss: 111.7755 - val_mean_squared_error: 111.7755\n",
+      "Epoch 98/500\n",
+      "339/339 [==============================] - 0s 126us/sample - loss: 31.9876 - mean_squared_error: 31.9876 - val_loss: 110.4905 - val_mean_squared_error: 110.4905\n",
+      "Epoch 99/500\n",
+      "339/339 [==============================] - 0s 114us/sample - loss: 30.7598 - mean_squared_error: 30.7598 - val_loss: 109.2631 - val_mean_squared_error: 109.2631\n",
+      "Epoch 100/500\n",
+      "339/339 [==============================] - 0s 123us/sample - loss: 29.5930 - mean_squared_error: 29.5930 - val_loss: 108.0965 - val_mean_squared_error: 108.0965\n",
+      "Epoch 101/500\n",
+      "339/339 [==============================] - 0s 122us/sample - loss: 28.4914 - mean_squared_error: 28.4914 - val_loss: 106.9939 - val_mean_squared_error: 106.9939\n",
+      "Epoch 102/500\n",
+      "339/339 [==============================] - 0s 132us/sample - loss: 27.4605 - mean_squared_error: 27.4605 - val_loss: 105.9601 - val_mean_squared_error: 105.9601\n",
+      "Epoch 103/500\n",
+      "339/339 [==============================] - 0s 125us/sample - loss: 26.5007 - mean_squared_error: 26.5007 - val_loss: 104.9946 - val_mean_squared_error: 104.9946\n",
+      "Epoch 104/500\n",
+      "339/339 [==============================] - 0s 132us/sample - loss: 25.6125 - mean_squared_error: 25.6125 - val_loss: 104.0985 - val_mean_squared_error: 104.0985\n",
+      "Epoch 105/500\n",
+      "339/339 [==============================] - 0s 129us/sample - loss: 24.7950 - mean_squared_error: 24.7950 - val_loss: 103.2701 - val_mean_squared_error: 103.2701\n",
+      "Epoch 106/500\n",
+      "339/339 [==============================] - 0s 129us/sample - loss: 24.0456 - mean_squared_error: 24.0456 - val_loss: 102.5072 - val_mean_squared_error: 102.5072\n",
+      "Epoch 107/500\n",
+      "339/339 [==============================] - 0s 141us/sample - loss: 23.3606 - mean_squared_error: 23.3606 - val_loss: 101.8106 - val_mean_squared_error: 101.8106\n",
+      "Epoch 108/500\n",
+      "339/339 [==============================] - 0s 122us/sample - loss: 22.7354 - mean_squared_error: 22.7354 - val_loss: 101.1771 - val_mean_squared_error: 101.1771\n",
+      "Epoch 109/500\n",
+      "339/339 [==============================] - 0s 132us/sample - loss: 22.1651 - mean_squared_error: 22.1651 - val_loss: 100.6043 - val_mean_squared_error: 100.6043\n",
+      "Epoch 110/500\n",
+      "339/339 [==============================] - 0s 130us/sample - loss: 21.6447 - mean_squared_error: 21.6447 - val_loss: 100.0891 - val_mean_squared_error: 100.0891\n",
+      "Epoch 111/500\n",
+      "339/339 [==============================] - 0s 126us/sample - loss: 21.1689 - mean_squared_error: 21.1689 - val_loss: 99.6285 - val_mean_squared_error: 99.6285\n",
+      "Epoch 112/500\n",
+      "339/339 [==============================] - 0s 139us/sample - loss: 20.7324 - mean_squared_error: 20.7324 - val_loss: 99.2185 - val_mean_squared_error: 99.2185\n",
+      "Epoch 113/500\n",
+      "339/339 [==============================] - 0s 166us/sample - loss: 20.3304 - mean_squared_error: 20.3304 - val_loss: 98.8570 - val_mean_squared_error: 98.8570\n",
+      "Epoch 114/500\n",
+      "339/339 [==============================] - 0s 198us/sample - loss: 19.9589 - mean_squared_error: 19.9589 - val_loss: 98.5415 - val_mean_squared_error: 98.5415\n",
+      "Epoch 115/500\n",
+      "339/339 [==============================] - 0s 153us/sample - loss: 19.6146 - mean_squared_error: 19.6146 - val_loss: 98.2679 - val_mean_squared_error: 98.2679\n",
+      "Epoch 116/500\n",
+      "339/339 [==============================] - 0s 156us/sample - loss: 19.2932 - mean_squared_error: 19.2932 - val_loss: 98.0337 - val_mean_squared_error: 98.0337\n",
+      "Epoch 117/500\n",
+      "339/339 [==============================] - 0s 174us/sample - loss: 18.9926 - mean_squared_error: 18.9926 - val_loss: 97.8362 - val_mean_squared_error: 97.8362\n",
+      "Epoch 118/500\n",
+      "339/339 [==============================] - 0s 197us/sample - loss: 18.7097 - mean_squared_error: 18.7097 - val_loss: 97.6710 - val_mean_squared_error: 97.6710\n",
+      "Epoch 119/500\n",
+      "339/339 [==============================] - 0s 184us/sample - loss: 18.4433 - mean_squared_error: 18.4433 - val_loss: 97.5356 - val_mean_squared_error: 97.5356\n",
+      "Epoch 120/500\n",
+      "339/339 [==============================] - 0s 145us/sample - loss: 18.1917 - mean_squared_error: 18.1917 - val_loss: 97.4277 - val_mean_squared_error: 97.4277\n",
+      "Epoch 121/500\n",
+      "339/339 [==============================] - 0s 158us/sample - loss: 17.9531 - mean_squared_error: 17.9531 - val_loss: 97.3440 - val_mean_squared_error: 97.3440\n",
+      "Epoch 122/500\n",
+      "339/339 [==============================] - 0s 179us/sample - loss: 17.7265 - mean_squared_error: 17.7265 - val_loss: 97.2816 - val_mean_squared_error: 97.2816\n",
+      "Epoch 123/500\n",
+      "339/339 [==============================] - 0s 158us/sample - loss: 17.5118 - mean_squared_error: 17.5118 - val_loss: 97.2353 - val_mean_squared_error: 97.2353\n",
+      "Epoch 124/500\n",
+      "339/339 [==============================] - 0s 183us/sample - loss: 17.3078 - mean_squared_error: 17.3078 - val_loss: 97.2028 - val_mean_squared_error: 97.2028\n",
+      "Epoch 125/500\n",
+      "339/339 [==============================] - 0s 135us/sample - loss: 17.1143 - mean_squared_error: 17.1143 - val_loss: 97.1846 - val_mean_squared_error: 97.1846\n",
+      "Epoch 126/500\n",
+      "339/339 [==============================] - 0s 132us/sample - loss: 16.9310 - mean_squared_error: 16.9310 - val_loss: 97.1787 - val_mean_squared_error: 97.1787\n",
+      "Epoch 127/500\n",
+      "339/339 [==============================] - 0s 34us/sample - loss: 16.7573 - mean_squared_error: 16.7573 - val_loss: 97.1828 - val_mean_squared_error: 97.1828\n",
+      "Epoch 128/500\n",
+      "339/339 [==============================] - 0s 49us/sample - loss: 16.5926 - mean_squared_error: 16.5926 - val_loss: 97.1945 - val_mean_squared_error: 97.1945\n",
+      "Epoch 129/500\n",
+      "339/339 [==============================] - 0s 38us/sample - loss: 16.4367 - mean_squared_error: 16.4367 - val_loss: 97.2095 - val_mean_squared_error: 97.2095\n",
+      "Epoch 130/500\n",
+      "339/339 [==============================] - 0s 32us/sample - loss: 16.2893 - mean_squared_error: 16.2893 - val_loss: 97.2276 - val_mean_squared_error: 97.2276\n",
+      "Epoch 131/500\n",
+      "339/339 [==============================] - 0s 34us/sample - loss: 16.1496 - mean_squared_error: 16.1496 - val_loss: 97.2466 - val_mean_squared_error: 97.2466\n",
+      "Epoch 132/500\n",
+      "339/339 [==============================] - 0s 32us/sample - loss: 16.0174 - mean_squared_error: 16.0174 - val_loss: 97.2637 - val_mean_squared_error: 97.2637\n",
+      "Epoch 133/500\n",
+      "339/339 [==============================] - 0s 45us/sample - loss: 15.8922 - mean_squared_error: 15.8922 - val_loss: 97.2788 - val_mean_squared_error: 97.2788\n",
+      "Epoch 134/500\n",
+      "339/339 [==============================] - 0s 34us/sample - loss: 15.7731 - mean_squared_error: 15.7731 - val_loss: 97.2907 - val_mean_squared_error: 97.2907\n",
+      "Epoch 135/500\n",
+      "339/339 [==============================] - 0s 32us/sample - loss: 15.6602 - mean_squared_error: 15.6602 - val_loss: 97.2974 - val_mean_squared_error: 97.2974\n",
+      "Epoch 136/500\n",
+      "339/339 [==============================] - 0s 58us/sample - loss: 15.5527 - mean_squared_error: 15.5527 - val_loss: 97.2974 - val_mean_squared_error: 97.2974\n",
+      "Epoch 137/500\n",
+      "339/339 [==============================] - 0s 48us/sample - loss: 15.4498 - mean_squared_error: 15.4498 - val_loss: 97.2889 - val_mean_squared_error: 97.2889\n",
+      "Epoch 138/500\n",
+      "339/339 [==============================] - 0s 35us/sample - loss: 15.3512 - mean_squared_error: 15.3512 - val_loss: 97.2706 - val_mean_squared_error: 97.2706\n",
+      "Epoch 139/500\n",
+      "339/339 [==============================] - 0s 50us/sample - loss: 15.2563 - mean_squared_error: 15.2563 - val_loss: 97.2414 - val_mean_squared_error: 97.2414\n",
+      "Epoch 140/500\n",
+      "339/339 [==============================] - 0s 43us/sample - loss: 15.1648 - mean_squared_error: 15.1648 - val_loss: 97.2011 - val_mean_squared_error: 97.2011\n",
+      "Epoch 141/500\n",
+      "339/339 [==============================] - 0s 137us/sample - loss: 15.0759 - mean_squared_error: 15.0759 - val_loss: 97.1479 - val_mean_squared_error: 97.1479\n",
+      "Epoch 142/500\n",
+      "339/339 [==============================] - 0s 143us/sample - loss: 14.9895 - mean_squared_error: 14.9895 - val_loss: 97.0821 - val_mean_squared_error: 97.0821\n",
+      "Epoch 143/500\n",
+      "339/339 [==============================] - 0s 134us/sample - loss: 14.9055 - mean_squared_error: 14.9055 - val_loss: 97.0041 - val_mean_squared_error: 97.0041\n",
+      "Epoch 144/500\n",
+      "339/339 [==============================] - 0s 404us/sample - loss: 14.8235 - mean_squared_error: 14.8235 - val_loss: 96.9141 - val_mean_squared_error: 96.9141\n",
+      "Epoch 145/500\n",
+      "339/339 [==============================] - 0s 198us/sample - loss: 14.7434 - mean_squared_error: 14.7434 - val_loss: 96.8123 - val_mean_squared_error: 96.8123\n",
+      "Epoch 146/500\n",
+      "339/339 [==============================] - 0s 128us/sample - loss: 14.6652 - mean_squared_error: 14.6652 - val_loss: 96.6991 - val_mean_squared_error: 96.6991\n",
+      "Epoch 147/500\n",
+      "339/339 [==============================] - 0s 122us/sample - loss: 14.5887 - mean_squared_error: 14.5887 - val_loss: 96.5741 - val_mean_squared_error: 96.5741\n",
+      "Epoch 148/500\n",
+      "339/339 [==============================] - 0s 130us/sample - loss: 14.5137 - mean_squared_error: 14.5137 - val_loss: 96.4376 - val_mean_squared_error: 96.4376\n",
+      "Epoch 149/500\n",
+      "339/339 [==============================] - 0s 131us/sample - loss: 14.4401 - mean_squared_error: 14.4401 - val_loss: 96.2903 - val_mean_squared_error: 96.2903\n",
+      "Epoch 150/500\n",
+      "339/339 [==============================] - 0s 134us/sample - loss: 14.3680 - mean_squared_error: 14.3680 - val_loss: 96.1323 - val_mean_squared_error: 96.1323\n",
+      "Epoch 151/500\n",
+      "339/339 [==============================] - 0s 169us/sample - loss: 14.2969 - mean_squared_error: 14.2969 - val_loss: 95.9651 - val_mean_squared_error: 95.9651\n",
+      "Epoch 152/500\n",
+      "339/339 [==============================] - 0s 134us/sample - loss: 14.2272 - mean_squared_error: 14.2272 - val_loss: 95.7892 - val_mean_squared_error: 95.7892\n",
+      "Epoch 153/500\n",
+      "339/339 [==============================] - 0s 129us/sample - loss: 14.1587 - mean_squared_error: 14.1587 - val_loss: 95.6056 - val_mean_squared_error: 95.6056\n",
+      "Epoch 154/500\n",
+      "339/339 [==============================] - 0s 127us/sample - loss: 14.0913 - mean_squared_error: 14.0913 - val_loss: 95.4151 - val_mean_squared_error: 95.4151\n",
+      "Epoch 155/500\n",
+      "339/339 [==============================] - 0s 128us/sample - loss: 14.0251 - mean_squared_error: 14.0251 - val_loss: 95.2183 - val_mean_squared_error: 95.2183\n",
+      "Epoch 156/500\n",
+      "339/339 [==============================] - 0s 134us/sample - loss: 13.9602 - mean_squared_error: 13.9602 - val_loss: 95.0158 - val_mean_squared_error: 95.0158\n",
+      "Epoch 157/500\n",
+      "339/339 [==============================] - 0s 137us/sample - loss: 13.8962 - mean_squared_error: 13.8962 - val_loss: 94.8084 - val_mean_squared_error: 94.8084\n",
+      "Epoch 158/500\n",
+      "339/339 [==============================] - 0s 134us/sample - loss: 13.8334 - mean_squared_error: 13.8334 - val_loss: 94.5967 - val_mean_squared_error: 94.5967\n",
+      "Epoch 159/500\n",
+      "339/339 [==============================] - 0s 134us/sample - loss: 13.7717 - mean_squared_error: 13.7717 - val_loss: 94.3806 - val_mean_squared_error: 94.3806\n",
+      "Epoch 160/500\n",
+      "339/339 [==============================] - 0s 136us/sample - loss: 13.7109 - mean_squared_error: 13.7109 - val_loss: 94.1609 - val_mean_squared_error: 94.1609\n",
+      "Epoch 161/500\n",
+      "339/339 [==============================] - 0s 135us/sample - loss: 13.6511 - mean_squared_error: 13.6511 - val_loss: 93.9391 - val_mean_squared_error: 93.9391\n",
+      "Epoch 162/500\n",
+      "339/339 [==============================] - 0s 137us/sample - loss: 13.5924 - mean_squared_error: 13.5924 - val_loss: 93.7156 - val_mean_squared_error: 93.7156\n",
+      "Epoch 163/500\n",
+      "339/339 [==============================] - 0s 137us/sample - loss: 13.5347 - mean_squared_error: 13.5347 - val_loss: 93.4893 - val_mean_squared_error: 93.4893\n",
+      "Epoch 164/500\n",
+      "339/339 [==============================] - 0s 138us/sample - loss: 13.4781 - mean_squared_error: 13.4781 - val_loss: 93.2612 - val_mean_squared_error: 93.2612\n",
+      "Epoch 165/500\n",
+      "339/339 [==============================] - 0s 135us/sample - loss: 13.4222 - mean_squared_error: 13.4222 - val_loss: 93.0304 - val_mean_squared_error: 93.0304\n",
+      "Epoch 166/500\n",
+      "339/339 [==============================] - 0s 136us/sample - loss: 13.3672 - mean_squared_error: 13.3672 - val_loss: 92.7978 - val_mean_squared_error: 92.7978\n",
+      "Epoch 167/500\n",
+      "339/339 [==============================] - 0s 144us/sample - loss: 13.3127 - mean_squared_error: 13.3127 - val_loss: 92.5644 - val_mean_squared_error: 92.5644\n",
+      "Epoch 168/500\n",
+      "339/339 [==============================] - 0s 126us/sample - loss: 13.2587 - mean_squared_error: 13.2587 - val_loss: 92.3309 - val_mean_squared_error: 92.3309\n",
+      "Epoch 169/500\n",
+      "339/339 [==============================] - 0s 136us/sample - loss: 13.2053 - mean_squared_error: 13.2053 - val_loss: 92.0961 - val_mean_squared_error: 92.0961\n",
+      "Epoch 170/500\n",
+      "339/339 [==============================] - 0s 135us/sample - loss: 13.1525 - mean_squared_error: 13.1525 - val_loss: 91.8606 - val_mean_squared_error: 91.8606\n",
+      "Epoch 171/500\n",
+      "339/339 [==============================] - 0s 164us/sample - loss: 13.1004 - mean_squared_error: 13.1004 - val_loss: 91.6246 - val_mean_squared_error: 91.6246\n",
+      "Epoch 172/500\n",
+      "339/339 [==============================] - 0s 129us/sample - loss: 13.0489 - mean_squared_error: 13.0489 - val_loss: 91.3887 - val_mean_squared_error: 91.3887\n",
+      "Epoch 173/500\n",
+      "339/339 [==============================] - 0s 132us/sample - loss: 12.9979 - mean_squared_error: 12.9979 - val_loss: 91.1538 - val_mean_squared_error: 91.1538\n",
+      "Epoch 174/500\n",
+      "339/339 [==============================] - 0s 135us/sample - loss: 12.9474 - mean_squared_error: 12.9474 - val_loss: 90.9199 - val_mean_squared_error: 90.9199\n",
+      "Epoch 175/500\n",
+      "339/339 [==============================] - 0s 133us/sample - loss: 12.8974 - mean_squared_error: 12.8974 - val_loss: 90.6868 - val_mean_squared_error: 90.6868\n",
+      "Epoch 176/500\n",
+      "339/339 [==============================] - 0s 136us/sample - loss: 12.8481 - mean_squared_error: 12.8481 - val_loss: 90.4541 - val_mean_squared_error: 90.4541\n",
+      "Epoch 177/500\n",
+      "339/339 [==============================] - 0s 140us/sample - loss: 12.7994 - mean_squared_error: 12.7994 - val_loss: 90.2217 - val_mean_squared_error: 90.2217\n",
+      "Epoch 178/500\n",
+      "339/339 [==============================] - 0s 138us/sample - loss: 12.7512 - mean_squared_error: 12.7512 - val_loss: 89.9885 - val_mean_squared_error: 89.9885\n",
+      "Epoch 179/500\n",
+      "339/339 [==============================] - 0s 137us/sample - loss: 12.7035 - mean_squared_error: 12.7035 - val_loss: 89.7545 - val_mean_squared_error: 89.7545\n",
+      "Epoch 180/500\n",
+      "339/339 [==============================] - 0s 130us/sample - loss: 12.6564 - mean_squared_error: 12.6564 - val_loss: 89.5201 - val_mean_squared_error: 89.5201\n",
+      "Epoch 181/500\n",
+      "339/339 [==============================] - 0s 140us/sample - loss: 12.6096 - mean_squared_error: 12.6096 - val_loss: 89.2852 - val_mean_squared_error: 89.2852\n",
+      "Epoch 182/500\n",
+      "339/339 [==============================] - 0s 135us/sample - loss: 12.5631 - mean_squared_error: 12.5631 - val_loss: 89.0505 - val_mean_squared_error: 89.0505\n",
+      "Epoch 183/500\n",
+      "339/339 [==============================] - 0s 137us/sample - loss: 12.5170 - mean_squared_error: 12.5170 - val_loss: 88.8172 - val_mean_squared_error: 88.8172\n",
+      "Epoch 184/500\n",
+      "339/339 [==============================] - 0s 136us/sample - loss: 12.4713 - mean_squared_error: 12.4713 - val_loss: 88.5853 - val_mean_squared_error: 88.5853\n",
+      "Epoch 185/500\n",
+      "339/339 [==============================] - 0s 376us/sample - loss: 12.4261 - mean_squared_error: 12.4261 - val_loss: 88.3546 - val_mean_squared_error: 88.3546\n",
+      "Epoch 186/500\n",
+      "339/339 [==============================] - 0s 176us/sample - loss: 12.3812 - mean_squared_error: 12.3812 - val_loss: 88.1250 - val_mean_squared_error: 88.1250\n",
+      "Epoch 187/500\n",
+      "339/339 [==============================] - 0s 137us/sample - loss: 12.3368 - mean_squared_error: 12.3368 - val_loss: 87.8964 - val_mean_squared_error: 87.8964\n",
+      "Epoch 188/500\n",
+      "339/339 [==============================] - 0s 144us/sample - loss: 12.2929 - mean_squared_error: 12.2929 - val_loss: 87.6688 - val_mean_squared_error: 87.6688\n",
+      "Epoch 189/500\n",
+      "339/339 [==============================] - 0s 249us/sample - loss: 12.2496 - mean_squared_error: 12.2496 - val_loss: 87.4416 - val_mean_squared_error: 87.4416\n",
+      "Epoch 190/500\n",
+      "339/339 [==============================] - 0s 296us/sample - loss: 12.2069 - mean_squared_error: 12.2069 - val_loss: 87.2142 - val_mean_squared_error: 87.2142\n",
+      "Epoch 191/500\n",
+      "339/339 [==============================] - 0s 135us/sample - loss: 12.1649 - mean_squared_error: 12.1649 - val_loss: 86.9855 - val_mean_squared_error: 86.9855\n",
+      "Epoch 192/500\n",
+      "339/339 [==============================] - 0s 125us/sample - loss: 12.1234 - mean_squared_error: 12.1234 - val_loss: 86.7568 - val_mean_squared_error: 86.7568\n",
+      "Epoch 193/500\n",
+      "339/339 [==============================] - 0s 148us/sample - loss: 12.0822 - mean_squared_error: 12.0822 - val_loss: 86.5288 - val_mean_squared_error: 86.5288\n",
+      "Epoch 194/500\n",
+      "339/339 [==============================] - 0s 226us/sample - loss: 12.0415 - mean_squared_error: 12.0415 - val_loss: 86.3018 - val_mean_squared_error: 86.3018\n",
+      "Epoch 195/500\n",
+      "339/339 [==============================] - 0s 219us/sample - loss: 12.0012 - mean_squared_error: 12.0012 - val_loss: 86.0756 - val_mean_squared_error: 86.0756\n",
+      "Epoch 196/500\n",
+      "339/339 [==============================] - 0s 130us/sample - loss: 11.9614 - mean_squared_error: 11.9614 - val_loss: 85.8504 - val_mean_squared_error: 85.8504\n",
+      "Epoch 197/500\n",
+      "339/339 [==============================] - 0s 134us/sample - loss: 11.9220 - mean_squared_error: 11.9220 - val_loss: 85.6262 - val_mean_squared_error: 85.6262\n",
+      "Epoch 198/500\n",
+      "339/339 [==============================] - 0s 153us/sample - loss: 11.8828 - mean_squared_error: 11.8828 - val_loss: 85.4045 - val_mean_squared_error: 85.4045\n",
+      "Epoch 199/500\n",
+      "339/339 [==============================] - 0s 127us/sample - loss: 11.8439 - mean_squared_error: 11.8439 - val_loss: 85.1853 - val_mean_squared_error: 85.1853\n",
+      "Epoch 200/500\n",
+      "339/339 [==============================] - 0s 143us/sample - loss: 11.8053 - mean_squared_error: 11.8053 - val_loss: 84.9681 - val_mean_squared_error: 84.9681\n",
+      "Epoch 201/500\n",
+      "339/339 [==============================] - 0s 140us/sample - loss: 11.7671 - mean_squared_error: 11.7671 - val_loss: 84.7528 - val_mean_squared_error: 84.7528\n",
+      "Epoch 202/500\n",
+      "339/339 [==============================] - 0s 285us/sample - loss: 11.7292 - mean_squared_error: 11.7292 - val_loss: 84.5391 - val_mean_squared_error: 84.5391\n",
+      "Epoch 203/500\n",
+      "339/339 [==============================] - 0s 280us/sample - loss: 11.6916 - mean_squared_error: 11.6916 - val_loss: 84.3269 - val_mean_squared_error: 84.3269\n",
+      "Epoch 204/500\n",
+      "339/339 [==============================] - 0s 353us/sample - loss: 11.6543 - mean_squared_error: 11.6543 - val_loss: 84.1167 - val_mean_squared_error: 84.1167\n",
+      "Epoch 205/500\n",
+      "339/339 [==============================] - 0s 165us/sample - loss: 11.6174 - mean_squared_error: 11.6174 - val_loss: 83.9076 - val_mean_squared_error: 83.9076\n",
+      "Epoch 206/500\n",
+      "339/339 [==============================] - 0s 125us/sample - loss: 11.5807 - mean_squared_error: 11.5807 - val_loss: 83.6996 - val_mean_squared_error: 83.6996\n",
+      "Epoch 207/500\n",
+      "339/339 [==============================] - 0s 138us/sample - loss: 11.5442 - mean_squared_error: 11.5442 - val_loss: 83.4933 - val_mean_squared_error: 83.4933\n",
+      "Epoch 208/500\n",
+      "339/339 [==============================] - 0s 134us/sample - loss: 11.5080 - mean_squared_error: 11.5080 - val_loss: 83.2885 - val_mean_squared_error: 83.2885\n",
+      "Epoch 209/500\n",
+      "339/339 [==============================] - 0s 140us/sample - loss: 11.4720 - mean_squared_error: 11.4720 - val_loss: 83.0852 - val_mean_squared_error: 83.0852\n",
+      "Epoch 210/500\n",
+      "339/339 [==============================] - 0s 191us/sample - loss: 11.4364 - mean_squared_error: 11.4364 - val_loss: 82.8833 - val_mean_squared_error: 82.8833\n",
+      "Epoch 211/500\n",
+      "339/339 [==============================] - 0s 135us/sample - loss: 11.4010 - mean_squared_error: 11.4010 - val_loss: 82.6825 - val_mean_squared_error: 82.6825\n",
+      "Epoch 212/500\n",
+      "339/339 [==============================] - 0s 210us/sample - loss: 11.3657 - mean_squared_error: 11.3657 - val_loss: 82.4840 - val_mean_squared_error: 82.4840\n",
+      "Epoch 213/500\n",
+      "339/339 [==============================] - 0s 136us/sample - loss: 11.3307 - mean_squared_error: 11.3307 - val_loss: 82.2872 - val_mean_squared_error: 82.2872\n",
+      "Epoch 214/500\n",
+      "339/339 [==============================] - 0s 128us/sample - loss: 11.2958 - mean_squared_error: 11.2958 - val_loss: 82.0916 - val_mean_squared_error: 82.0916\n",
+      "Epoch 215/500\n",
+      "339/339 [==============================] - 0s 137us/sample - loss: 11.2609 - mean_squared_error: 11.2609 - val_loss: 81.8975 - val_mean_squared_error: 81.8975\n",
+      "Epoch 216/500\n",
+      "339/339 [==============================] - 0s 127us/sample - loss: 11.2262 - mean_squared_error: 11.2262 - val_loss: 81.7040 - val_mean_squared_error: 81.7040\n",
+      "Epoch 217/500\n",
+      "339/339 [==============================] - 0s 398us/sample - loss: 11.1916 - mean_squared_error: 11.1916 - val_loss: 81.5112 - val_mean_squared_error: 81.5112\n",
+      "Epoch 218/500\n",
+      "339/339 [==============================] - 0s 251us/sample - loss: 11.1573 - mean_squared_error: 11.1573 - val_loss: 81.3188 - val_mean_squared_error: 81.3188\n",
+      "Epoch 219/500\n",
+      "339/339 [==============================] - 0s 166us/sample - loss: 11.1232 - mean_squared_error: 11.1232 - val_loss: 81.1269 - val_mean_squared_error: 81.1269\n",
+      "Epoch 220/500\n",
+      "339/339 [==============================] - 0s 160us/sample - loss: 11.0892 - mean_squared_error: 11.0892 - val_loss: 80.9354 - val_mean_squared_error: 80.9354\n",
+      "Epoch 221/500\n",
+      "339/339 [==============================] - 0s 164us/sample - loss: 11.0555 - mean_squared_error: 11.0555 - val_loss: 80.7443 - val_mean_squared_error: 80.7443\n",
+      "Epoch 222/500\n",
+      "339/339 [==============================] - 0s 148us/sample - loss: 11.0221 - mean_squared_error: 11.0221 - val_loss: 80.5534 - val_mean_squared_error: 80.5534\n",
+      "Epoch 223/500\n",
+      "339/339 [==============================] - 0s 134us/sample - loss: 10.9889 - mean_squared_error: 10.9889 - val_loss: 80.3630 - val_mean_squared_error: 80.3630\n",
+      "Epoch 224/500\n",
+      "339/339 [==============================] - 0s 135us/sample - loss: 10.9561 - mean_squared_error: 10.9561 - val_loss: 80.1744 - val_mean_squared_error: 80.1744\n",
+      "Epoch 225/500\n",
+      "339/339 [==============================] - 0s 133us/sample - loss: 10.9234 - mean_squared_error: 10.9234 - val_loss: 79.9877 - val_mean_squared_error: 79.9877\n",
+      "Epoch 226/500\n",
+      "339/339 [==============================] - 0s 136us/sample - loss: 10.8909 - mean_squared_error: 10.8909 - val_loss: 79.8023 - val_mean_squared_error: 79.8023\n",
+      "Epoch 227/500\n",
+      "339/339 [==============================] - 0s 135us/sample - loss: 10.8586 - mean_squared_error: 10.8586 - val_loss: 79.6175 - val_mean_squared_error: 79.6175\n",
+      "Epoch 228/500\n",
+      "339/339 [==============================] - 0s 133us/sample - loss: 10.8262 - mean_squared_error: 10.8262 - val_loss: 79.4332 - val_mean_squared_error: 79.4332\n",
+      "Epoch 229/500\n",
+      "339/339 [==============================] - 0s 148us/sample - loss: 10.7940 - mean_squared_error: 10.7940 - val_loss: 79.2495 - val_mean_squared_error: 79.2495\n",
+      "Epoch 230/500\n",
+      "339/339 [==============================] - 0s 143us/sample - loss: 10.7620 - mean_squared_error: 10.7620 - val_loss: 79.0665 - val_mean_squared_error: 79.0665\n",
+      "Epoch 231/500\n",
+      "339/339 [==============================] - 0s 140us/sample - loss: 10.7300 - mean_squared_error: 10.7300 - val_loss: 78.8844 - val_mean_squared_error: 78.8844\n",
+      "Epoch 232/500\n",
+      "339/339 [==============================] - 0s 139us/sample - loss: 10.6981 - mean_squared_error: 10.6981 - val_loss: 78.7030 - val_mean_squared_error: 78.7030\n",
+      "Epoch 233/500\n",
+      "339/339 [==============================] - 0s 131us/sample - loss: 10.6663 - mean_squared_error: 10.6663 - val_loss: 78.5223 - val_mean_squared_error: 78.5223\n",
+      "Epoch 234/500\n",
+      "339/339 [==============================] - 0s 134us/sample - loss: 10.6348 - mean_squared_error: 10.6348 - val_loss: 78.3431 - val_mean_squared_error: 78.3431\n",
+      "Epoch 235/500\n",
+      "339/339 [==============================] - 0s 141us/sample - loss: 10.6033 - mean_squared_error: 10.6033 - val_loss: 78.1655 - val_mean_squared_error: 78.1655\n",
+      "Epoch 236/500\n",
+      "339/339 [==============================] - 0s 133us/sample - loss: 10.5721 - mean_squared_error: 10.5721 - val_loss: 77.9890 - val_mean_squared_error: 77.9890\n",
+      "Epoch 237/500\n",
+      "339/339 [==============================] - 0s 205us/sample - loss: 10.5412 - mean_squared_error: 10.5412 - val_loss: 77.8120 - val_mean_squared_error: 77.8120\n",
+      "Epoch 238/500\n",
+      "339/339 [==============================] - 0s 148us/sample - loss: 10.5102 - mean_squared_error: 10.5102 - val_loss: 77.6354 - val_mean_squared_error: 77.6354\n",
+      "Epoch 239/500\n",
+      "339/339 [==============================] - 0s 142us/sample - loss: 10.4794 - mean_squared_error: 10.4794 - val_loss: 77.4592 - val_mean_squared_error: 77.4592\n",
+      "Epoch 240/500\n",
+      "339/339 [==============================] - 0s 150us/sample - loss: 10.4487 - mean_squared_error: 10.4487 - val_loss: 77.2835 - val_mean_squared_error: 77.2835\n",
+      "Epoch 241/500\n",
+      "339/339 [==============================] - 0s 130us/sample - loss: 10.4181 - mean_squared_error: 10.4181 - val_loss: 77.1087 - val_mean_squared_error: 77.1087\n",
+      "Epoch 242/500\n",
+      "339/339 [==============================] - 0s 141us/sample - loss: 10.3875 - mean_squared_error: 10.3875 - val_loss: 76.9368 - val_mean_squared_error: 76.9368\n",
+      "Epoch 243/500\n",
+      "339/339 [==============================] - 0s 149us/sample - loss: 10.3568 - mean_squared_error: 10.3568 - val_loss: 76.7663 - val_mean_squared_error: 76.7663\n",
+      "Epoch 244/500\n",
+      "339/339 [==============================] - 0s 143us/sample - loss: 10.3261 - mean_squared_error: 10.3261 - val_loss: 76.5972 - val_mean_squared_error: 76.5972\n",
+      "Epoch 245/500\n",
+      "339/339 [==============================] - 0s 141us/sample - loss: 10.2952 - mean_squared_error: 10.2952 - val_loss: 76.4298 - val_mean_squared_error: 76.4298\n",
+      "Epoch 246/500\n",
+      "339/339 [==============================] - 0s 134us/sample - loss: 10.2645 - mean_squared_error: 10.2645 - val_loss: 76.2640 - val_mean_squared_error: 76.2640\n",
+      "Epoch 247/500\n",
+      "339/339 [==============================] - 0s 135us/sample - loss: 10.2338 - mean_squared_error: 10.2338 - val_loss: 76.0996 - val_mean_squared_error: 76.0996\n",
+      "Epoch 248/500\n",
+      "339/339 [==============================] - 0s 134us/sample - loss: 10.2032 - mean_squared_error: 10.2032 - val_loss: 75.9375 - val_mean_squared_error: 75.9375\n",
+      "Epoch 249/500\n",
+      "339/339 [==============================] - 0s 138us/sample - loss: 10.1729 - mean_squared_error: 10.1729 - val_loss: 75.7769 - val_mean_squared_error: 75.7769\n",
+      "Epoch 250/500\n",
+      "339/339 [==============================] - 0s 137us/sample - loss: 10.1427 - mean_squared_error: 10.1427 - val_loss: 75.6170 - val_mean_squared_error: 75.6170\n",
+      "Epoch 251/500\n",
+      "339/339 [==============================] - 0s 141us/sample - loss: 10.1127 - mean_squared_error: 10.1127 - val_loss: 75.4573 - val_mean_squared_error: 75.4573\n",
+      "Epoch 252/500\n",
+      "339/339 [==============================] - 0s 138us/sample - loss: 10.0830 - mean_squared_error: 10.0830 - val_loss: 75.2995 - val_mean_squared_error: 75.2995\n",
+      "Epoch 253/500\n",
+      "339/339 [==============================] - 0s 172us/sample - loss: 10.0534 - mean_squared_error: 10.0534 - val_loss: 75.1433 - val_mean_squared_error: 75.1433\n",
+      "Epoch 254/500\n",
+      "339/339 [==============================] - 0s 682us/sample - loss: 10.0240 - mean_squared_error: 10.0240 - val_loss: 74.9884 - val_mean_squared_error: 74.9884\n",
+      "Epoch 255/500\n",
+      "339/339 [==============================] - 0s 147us/sample - loss: 9.9947 - mean_squared_error: 9.9947 - val_loss: 74.8338 - val_mean_squared_error: 74.8338\n",
+      "Epoch 256/500\n",
+      "339/339 [==============================] - 0s 156us/sample - loss: 9.9657 - mean_squared_error: 9.9657 - val_loss: 74.6795 - val_mean_squared_error: 74.6795\n",
+      "Epoch 257/500\n",
+      "339/339 [==============================] - 0s 180us/sample - loss: 9.9371 - mean_squared_error: 9.9371 - val_loss: 74.5242 - val_mean_squared_error: 74.5242\n",
+      "Epoch 258/500\n",
+      "339/339 [==============================] - 0s 216us/sample - loss: 9.9086 - mean_squared_error: 9.9086 - val_loss: 74.3682 - val_mean_squared_error: 74.3682\n",
+      "Epoch 259/500\n",
+      "339/339 [==============================] - 0s 172us/sample - loss: 9.8805 - mean_squared_error: 9.8805 - val_loss: 74.2127 - val_mean_squared_error: 74.2127\n",
+      "Epoch 260/500\n",
+      "339/339 [==============================] - 0s 192us/sample - loss: 9.8525 - mean_squared_error: 9.8525 - val_loss: 74.0576 - val_mean_squared_error: 74.0576\n",
+      "Epoch 261/500\n",
+      "339/339 [==============================] - 0s 156us/sample - loss: 9.8248 - mean_squared_error: 9.8248 - val_loss: 73.9026 - val_mean_squared_error: 73.9026\n",
+      "Epoch 262/500\n",
+      "339/339 [==============================] - 0s 171us/sample - loss: 9.7972 - mean_squared_error: 9.7972 - val_loss: 73.7465 - val_mean_squared_error: 73.7465\n",
+      "Epoch 263/500\n",
+      "339/339 [==============================] - 0s 304us/sample - loss: 9.7697 - mean_squared_error: 9.7697 - val_loss: 73.5900 - val_mean_squared_error: 73.5900\n",
+      "Epoch 264/500\n",
+      "339/339 [==============================] - 0s 231us/sample - loss: 9.7424 - mean_squared_error: 9.7424 - val_loss: 73.4333 - val_mean_squared_error: 73.4333\n",
+      "Epoch 265/500\n",
+      "339/339 [==============================] - 0s 221us/sample - loss: 9.7152 - mean_squared_error: 9.7152 - val_loss: 73.2775 - val_mean_squared_error: 73.2775\n",
+      "Epoch 266/500\n",
+      "339/339 [==============================] - 0s 221us/sample - loss: 9.6879 - mean_squared_error: 9.6879 - val_loss: 73.1238 - val_mean_squared_error: 73.1238\n",
+      "Epoch 267/500\n",
+      "339/339 [==============================] - 0s 204us/sample - loss: 9.6607 - mean_squared_error: 9.6607 - val_loss: 72.9714 - val_mean_squared_error: 72.9714\n",
+      "Epoch 268/500\n",
+      "339/339 [==============================] - 0s 227us/sample - loss: 9.6335 - mean_squared_error: 9.6335 - val_loss: 72.8209 - val_mean_squared_error: 72.8209\n",
+      "Epoch 269/500\n",
+      "339/339 [==============================] - 0s 195us/sample - loss: 9.6066 - mean_squared_error: 9.6066 - val_loss: 72.6721 - val_mean_squared_error: 72.6721\n",
+      "Epoch 270/500\n",
+      "339/339 [==============================] - 0s 244us/sample - loss: 9.5798 - mean_squared_error: 9.5798 - val_loss: 72.5248 - val_mean_squared_error: 72.5248\n",
+      "Epoch 271/500\n",
+      "339/339 [==============================] - 0s 184us/sample - loss: 9.5534 - mean_squared_error: 9.5534 - val_loss: 72.3779 - val_mean_squared_error: 72.3779\n",
+      "Epoch 272/500\n",
+      "339/339 [==============================] - 0s 201us/sample - loss: 9.5272 - mean_squared_error: 9.5272 - val_loss: 72.2324 - val_mean_squared_error: 72.2324\n",
+      "Epoch 273/500\n",
+      "339/339 [==============================] - 0s 193us/sample - loss: 9.5011 - mean_squared_error: 9.5011 - val_loss: 72.0883 - val_mean_squared_error: 72.0883\n",
+      "Epoch 274/500\n",
+      "339/339 [==============================] - 0s 257us/sample - loss: 9.4748 - mean_squared_error: 9.4748 - val_loss: 71.9449 - val_mean_squared_error: 71.9449\n",
+      "Epoch 275/500\n",
+      "339/339 [==============================] - 0s 236us/sample - loss: 9.4484 - mean_squared_error: 9.4484 - val_loss: 71.8039 - val_mean_squared_error: 71.8039\n",
+      "Epoch 276/500\n",
+      "339/339 [==============================] - 0s 191us/sample - loss: 9.4218 - mean_squared_error: 9.4218 - val_loss: 71.6658 - val_mean_squared_error: 71.6658\n",
+      "Epoch 277/500\n",
+      "339/339 [==============================] - 0s 230us/sample - loss: 9.3951 - mean_squared_error: 9.3951 - val_loss: 71.5310 - val_mean_squared_error: 71.5310\n",
+      "Epoch 278/500\n",
+      "339/339 [==============================] - 0s 600us/sample - loss: 9.3684 - mean_squared_error: 9.3684 - val_loss: 71.3981 - val_mean_squared_error: 71.3981\n",
+      "Epoch 279/500\n",
+      "339/339 [==============================] - 0s 131us/sample - loss: 9.3419 - mean_squared_error: 9.3419 - val_loss: 71.2663 - val_mean_squared_error: 71.2663\n",
+      "Epoch 280/500\n",
+      "339/339 [==============================] - 0s 148us/sample - loss: 9.3154 - mean_squared_error: 9.3154 - val_loss: 71.1366 - val_mean_squared_error: 71.1366\n",
+      "Epoch 281/500\n",
+      "339/339 [==============================] - 0s 146us/sample - loss: 9.2891 - mean_squared_error: 9.2891 - val_loss: 71.0087 - val_mean_squared_error: 71.0087\n",
+      "Epoch 282/500\n",
+      "339/339 [==============================] - 0s 265us/sample - loss: 9.2629 - mean_squared_error: 9.2629 - val_loss: 70.8828 - val_mean_squared_error: 70.8828\n",
+      "Epoch 283/500\n",
+      "339/339 [==============================] - 0s 146us/sample - loss: 9.2368 - mean_squared_error: 9.2368 - val_loss: 70.7574 - val_mean_squared_error: 70.7574\n",
+      "Epoch 284/500\n",
+      "339/339 [==============================] - 0s 192us/sample - loss: 9.2112 - mean_squared_error: 9.2112 - val_loss: 70.6308 - val_mean_squared_error: 70.6308\n",
+      "Epoch 285/500\n",
+      "339/339 [==============================] - 0s 190us/sample - loss: 9.1857 - mean_squared_error: 9.1857 - val_loss: 70.5033 - val_mean_squared_error: 70.5033\n",
+      "Epoch 286/500\n",
+      "339/339 [==============================] - 0s 190us/sample - loss: 9.1598 - mean_squared_error: 9.1598 - val_loss: 70.3764 - val_mean_squared_error: 70.3764\n",
+      "Epoch 287/500\n",
+      "339/339 [==============================] - 0s 223us/sample - loss: 9.1338 - mean_squared_error: 9.1338 - val_loss: 70.2513 - val_mean_squared_error: 70.2513\n",
+      "Epoch 288/500\n",
+      "339/339 [==============================] - 0s 253us/sample - loss: 9.1083 - mean_squared_error: 9.1083 - val_loss: 70.1258 - val_mean_squared_error: 70.1258\n",
+      "Epoch 289/500\n",
+      "339/339 [==============================] - 0s 213us/sample - loss: 9.0832 - mean_squared_error: 9.0832 - val_loss: 70.0005 - val_mean_squared_error: 70.0005\n",
+      "Epoch 290/500\n",
+      "339/339 [==============================] - 0s 192us/sample - loss: 9.0584 - mean_squared_error: 9.0584 - val_loss: 69.8755 - val_mean_squared_error: 69.8755\n",
+      "Epoch 291/500\n",
+      "339/339 [==============================] - 0s 202us/sample - loss: 9.0338 - mean_squared_error: 9.0338 - val_loss: 69.7504 - val_mean_squared_error: 69.7504\n",
+      "Epoch 292/500\n",
+      "339/339 [==============================] - 0s 186us/sample - loss: 9.0093 - mean_squared_error: 9.0093 - val_loss: 69.6246 - val_mean_squared_error: 69.6246\n",
+      "Epoch 293/500\n",
+      "339/339 [==============================] - 0s 189us/sample - loss: 8.9850 - mean_squared_error: 8.9850 - val_loss: 69.4983 - val_mean_squared_error: 69.4983\n",
+      "Epoch 294/500\n",
+      "339/339 [==============================] - 0s 208us/sample - loss: 8.9610 - mean_squared_error: 8.9610 - val_loss: 69.3700 - val_mean_squared_error: 69.3700\n",
+      "Epoch 295/500\n",
+      "339/339 [==============================] - 0s 195us/sample - loss: 8.9374 - mean_squared_error: 8.9374 - val_loss: 69.2398 - val_mean_squared_error: 69.2398\n",
+      "Epoch 296/500\n",
+      "339/339 [==============================] - 0s 240us/sample - loss: 8.9139 - mean_squared_error: 8.9139 - val_loss: 69.1085 - val_mean_squared_error: 69.1085\n",
+      "Epoch 297/500\n",
+      "339/339 [==============================] - 0s 244us/sample - loss: 8.8907 - mean_squared_error: 8.8907 - val_loss: 68.9769 - val_mean_squared_error: 68.9769\n",
+      "Epoch 298/500\n",
+      "339/339 [==============================] - 0s 195us/sample - loss: 8.8676 - mean_squared_error: 8.8676 - val_loss: 68.8453 - val_mean_squared_error: 68.8453\n",
+      "Epoch 299/500\n",
+      "339/339 [==============================] - 0s 217us/sample - loss: 8.8445 - mean_squared_error: 8.8445 - val_loss: 68.7137 - val_mean_squared_error: 68.7137\n",
+      "Epoch 300/500\n",
+      "339/339 [==============================] - 0s 140us/sample - loss: 8.8218 - mean_squared_error: 8.8218 - val_loss: 68.5809 - val_mean_squared_error: 68.5809\n",
+      "Epoch 301/500\n",
+      "339/339 [==============================] - 0s 190us/sample - loss: 8.7994 - mean_squared_error: 8.7994 - val_loss: 68.4462 - val_mean_squared_error: 68.4462\n",
+      "Epoch 302/500\n",
+      "339/339 [==============================] - 0s 146us/sample - loss: 8.7771 - mean_squared_error: 8.7771 - val_loss: 68.3098 - val_mean_squared_error: 68.3098\n",
+      "Epoch 303/500\n",
+      "339/339 [==============================] - 0s 430us/sample - loss: 8.7550 - mean_squared_error: 8.7550 - val_loss: 68.1715 - val_mean_squared_error: 68.1715\n",
+      "Epoch 304/500\n",
+      "339/339 [==============================] - 0s 197us/sample - loss: 8.7332 - mean_squared_error: 8.7332 - val_loss: 68.0319 - val_mean_squared_error: 68.0319\n",
+      "Epoch 305/500\n",
+      "339/339 [==============================] - 0s 152us/sample - loss: 8.7115 - mean_squared_error: 8.7115 - val_loss: 67.8913 - val_mean_squared_error: 67.8913\n",
+      "Epoch 306/500\n",
+      "339/339 [==============================] - 0s 193us/sample - loss: 8.6900 - mean_squared_error: 8.6900 - val_loss: 67.7484 - val_mean_squared_error: 67.7484\n",
+      "Epoch 307/500\n",
+      "339/339 [==============================] - 0s 171us/sample - loss: 8.6689 - mean_squared_error: 8.6689 - val_loss: 67.6031 - val_mean_squared_error: 67.6031\n",
+      "Epoch 308/500\n",
+      "339/339 [==============================] - 0s 317us/sample - loss: 8.6479 - mean_squared_error: 8.6479 - val_loss: 67.4569 - val_mean_squared_error: 67.4569\n",
+      "Epoch 309/500\n",
+      "339/339 [==============================] - 0s 312us/sample - loss: 8.6270 - mean_squared_error: 8.6270 - val_loss: 67.3098 - val_mean_squared_error: 67.3098\n",
+      "Epoch 310/500\n",
+      "339/339 [==============================] - 0s 252us/sample - loss: 8.6063 - mean_squared_error: 8.6063 - val_loss: 67.1610 - val_mean_squared_error: 67.1610\n",
+      "Epoch 311/500\n",
+      "339/339 [==============================] - 0s 188us/sample - loss: 8.5857 - mean_squared_error: 8.5857 - val_loss: 67.0110 - val_mean_squared_error: 67.0110\n",
+      "Epoch 312/500\n",
+      "339/339 [==============================] - 0s 208us/sample - loss: 8.5653 - mean_squared_error: 8.5653 - val_loss: 66.8598 - val_mean_squared_error: 66.8598\n",
+      "Epoch 313/500\n",
+      "339/339 [==============================] - 0s 225us/sample - loss: 8.5450 - mean_squared_error: 8.5450 - val_loss: 66.7077 - val_mean_squared_error: 66.7077\n",
+      "Epoch 314/500\n",
+      "339/339 [==============================] - 0s 204us/sample - loss: 8.5249 - mean_squared_error: 8.5249 - val_loss: 66.5548 - val_mean_squared_error: 66.5548\n",
+      "Epoch 315/500\n",
+      "339/339 [==============================] - 0s 221us/sample - loss: 8.5048 - mean_squared_error: 8.5048 - val_loss: 66.4019 - val_mean_squared_error: 66.4019\n",
+      "Epoch 316/500\n",
+      "339/339 [==============================] - 0s 188us/sample - loss: 8.4848 - mean_squared_error: 8.4848 - val_loss: 66.2492 - val_mean_squared_error: 66.2492\n",
+      "Epoch 317/500\n",
+      "339/339 [==============================] - 0s 177us/sample - loss: 8.4649 - mean_squared_error: 8.4649 - val_loss: 66.0968 - val_mean_squared_error: 66.0968\n",
+      "Epoch 318/500\n",
+      "339/339 [==============================] - 0s 151us/sample - loss: 8.4451 - mean_squared_error: 8.4451 - val_loss: 65.9443 - val_mean_squared_error: 65.9443\n",
+      "Epoch 319/500\n",
+      "339/339 [==============================] - 0s 156us/sample - loss: 8.4254 - mean_squared_error: 8.4254 - val_loss: 65.7918 - val_mean_squared_error: 65.7918\n",
+      "Epoch 320/500\n",
+      "339/339 [==============================] - 0s 153us/sample - loss: 8.4059 - mean_squared_error: 8.4059 - val_loss: 65.6397 - val_mean_squared_error: 65.6397\n",
+      "Epoch 321/500\n",
+      "339/339 [==============================] - 0s 141us/sample - loss: 8.3866 - mean_squared_error: 8.3866 - val_loss: 65.4877 - val_mean_squared_error: 65.4877\n",
+      "Epoch 322/500\n",
+      "339/339 [==============================] - 0s 251us/sample - loss: 8.3675 - mean_squared_error: 8.3675 - val_loss: 65.3356 - val_mean_squared_error: 65.3356\n",
+      "Epoch 323/500\n",
+      "339/339 [==============================] - 0s 153us/sample - loss: 8.3487 - mean_squared_error: 8.3487 - val_loss: 65.1839 - val_mean_squared_error: 65.1839\n",
+      "Epoch 324/500\n",
+      "339/339 [==============================] - 0s 153us/sample - loss: 8.3300 - mean_squared_error: 8.3300 - val_loss: 65.0324 - val_mean_squared_error: 65.0324\n",
+      "Epoch 325/500\n",
+      "339/339 [==============================] - 0s 155us/sample - loss: 8.3114 - mean_squared_error: 8.3114 - val_loss: 64.8809 - val_mean_squared_error: 64.8809\n",
+      "Epoch 326/500\n",
+      "339/339 [==============================] - 0s 144us/sample - loss: 8.2929 - mean_squared_error: 8.2929 - val_loss: 64.7294 - val_mean_squared_error: 64.7294\n",
+      "Epoch 327/500\n",
+      "339/339 [==============================] - 0s 146us/sample - loss: 8.2746 - mean_squared_error: 8.2746 - val_loss: 64.5780 - val_mean_squared_error: 64.5780\n",
+      "Epoch 328/500\n",
+      "339/339 [==============================] - 0s 152us/sample - loss: 8.2565 - mean_squared_error: 8.2565 - val_loss: 64.4272 - val_mean_squared_error: 64.4272\n",
+      "Epoch 329/500\n",
+      "339/339 [==============================] - 0s 249us/sample - loss: 8.2384 - mean_squared_error: 8.2384 - val_loss: 64.2767 - val_mean_squared_error: 64.2767\n",
+      "Epoch 330/500\n",
+      "339/339 [==============================] - 0s 148us/sample - loss: 8.2203 - mean_squared_error: 8.2203 - val_loss: 64.1267 - val_mean_squared_error: 64.1267\n",
+      "Epoch 331/500\n",
+      "339/339 [==============================] - 0s 153us/sample - loss: 8.2024 - mean_squared_error: 8.2024 - val_loss: 63.9770 - val_mean_squared_error: 63.9770\n",
+      "Epoch 332/500\n",
+      "339/339 [==============================] - 0s 161us/sample - loss: 8.1845 - mean_squared_error: 8.1845 - val_loss: 63.8272 - val_mean_squared_error: 63.8272\n",
+      "Epoch 333/500\n",
+      "339/339 [==============================] - 0s 371us/sample - loss: 8.1668 - mean_squared_error: 8.1668 - val_loss: 63.6779 - val_mean_squared_error: 63.6779\n",
+      "Epoch 334/500\n",
+      "339/339 [==============================] - 0s 179us/sample - loss: 8.1491 - mean_squared_error: 8.1491 - val_loss: 63.5294 - val_mean_squared_error: 63.5294\n",
+      "Epoch 335/500\n",
+      "339/339 [==============================] - 0s 144us/sample - loss: 8.1314 - mean_squared_error: 8.1314 - val_loss: 63.3822 - val_mean_squared_error: 63.3822\n",
+      "Epoch 336/500\n",
+      "339/339 [==============================] - 0s 142us/sample - loss: 8.1138 - mean_squared_error: 8.1138 - val_loss: 63.2363 - val_mean_squared_error: 63.2363\n",
+      "Epoch 337/500\n",
+      "339/339 [==============================] - 0s 153us/sample - loss: 8.0964 - mean_squared_error: 8.0964 - val_loss: 63.0914 - val_mean_squared_error: 63.0914\n",
+      "Epoch 338/500\n",
+      "339/339 [==============================] - 0s 174us/sample - loss: 8.0791 - mean_squared_error: 8.0791 - val_loss: 62.9466 - val_mean_squared_error: 62.9466\n",
+      "Epoch 339/500\n",
+      "339/339 [==============================] - 0s 140us/sample - loss: 8.0619 - mean_squared_error: 8.0619 - val_loss: 62.8029 - val_mean_squared_error: 62.8029\n",
+      "Epoch 340/500\n",
+      "339/339 [==============================] - 0s 224us/sample - loss: 8.0448 - mean_squared_error: 8.0448 - val_loss: 62.6600 - val_mean_squared_error: 62.6600\n",
+      "Epoch 341/500\n",
+      "339/339 [==============================] - 0s 153us/sample - loss: 8.0277 - mean_squared_error: 8.0277 - val_loss: 62.5178 - val_mean_squared_error: 62.5178\n",
+      "Epoch 342/500\n",
+      "339/339 [==============================] - 0s 150us/sample - loss: 8.0107 - mean_squared_error: 8.0107 - val_loss: 62.3775 - val_mean_squared_error: 62.3775\n",
+      "Epoch 343/500\n",
+      "339/339 [==============================] - 0s 154us/sample - loss: 7.9937 - mean_squared_error: 7.9937 - val_loss: 62.2389 - val_mean_squared_error: 62.2389\n",
+      "Epoch 344/500\n",
+      "339/339 [==============================] - 0s 169us/sample - loss: 7.9767 - mean_squared_error: 7.9767 - val_loss: 62.1010 - val_mean_squared_error: 62.1010\n",
+      "Epoch 345/500\n",
+      "339/339 [==============================] - 0s 159us/sample - loss: 7.9599 - mean_squared_error: 7.9599 - val_loss: 61.9653 - val_mean_squared_error: 61.9653\n",
+      "Epoch 346/500\n",
+      "339/339 [==============================] - 0s 154us/sample - loss: 7.9432 - mean_squared_error: 7.9432 - val_loss: 61.8315 - val_mean_squared_error: 61.8315\n",
+      "Epoch 347/500\n",
+      "339/339 [==============================] - 0s 150us/sample - loss: 7.9267 - mean_squared_error: 7.9267 - val_loss: 61.6990 - val_mean_squared_error: 61.6990\n",
+      "Epoch 348/500\n",
+      "339/339 [==============================] - 0s 144us/sample - loss: 7.9103 - mean_squared_error: 7.9103 - val_loss: 61.5669 - val_mean_squared_error: 61.5669\n",
+      "Epoch 349/500\n",
+      "339/339 [==============================] - 0s 155us/sample - loss: 7.8941 - mean_squared_error: 7.8941 - val_loss: 61.4350 - val_mean_squared_error: 61.4350\n",
+      "Epoch 350/500\n",
+      "339/339 [==============================] - 0s 147us/sample - loss: 7.8781 - mean_squared_error: 7.8781 - val_loss: 61.3034 - val_mean_squared_error: 61.3034\n",
+      "Epoch 351/500\n",
+      "339/339 [==============================] - 0s 156us/sample - loss: 7.8621 - mean_squared_error: 7.8621 - val_loss: 61.1726 - val_mean_squared_error: 61.1726\n",
+      "Epoch 352/500\n",
+      "339/339 [==============================] - 0s 146us/sample - loss: 7.8463 - mean_squared_error: 7.8463 - val_loss: 61.0412 - val_mean_squared_error: 61.0412\n",
+      "Epoch 353/500\n",
+      "339/339 [==============================] - 0s 142us/sample - loss: 7.8305 - mean_squared_error: 7.8305 - val_loss: 60.9088 - val_mean_squared_error: 60.9088\n",
+      "Epoch 354/500\n",
+      "339/339 [==============================] - 0s 147us/sample - loss: 7.8148 - mean_squared_error: 7.8148 - val_loss: 60.7765 - val_mean_squared_error: 60.7765\n",
+      "Epoch 355/500\n",
+      "339/339 [==============================] - 0s 152us/sample - loss: 7.7991 - mean_squared_error: 7.7991 - val_loss: 60.6445 - val_mean_squared_error: 60.6445\n",
+      "Epoch 356/500\n",
+      "339/339 [==============================] - 0s 146us/sample - loss: 7.7835 - mean_squared_error: 7.7835 - val_loss: 60.5128 - val_mean_squared_error: 60.5128\n",
+      "Epoch 357/500\n",
+      "339/339 [==============================] - 0s 143us/sample - loss: 7.7679 - mean_squared_error: 7.7679 - val_loss: 60.3815 - val_mean_squared_error: 60.3815\n",
+      "Epoch 358/500\n",
+      "339/339 [==============================] - 0s 152us/sample - loss: 7.7525 - mean_squared_error: 7.7525 - val_loss: 60.2511 - val_mean_squared_error: 60.2511\n",
+      "Epoch 359/500\n",
+      "339/339 [==============================] - 0s 146us/sample - loss: 7.7370 - mean_squared_error: 7.7370 - val_loss: 60.1225 - val_mean_squared_error: 60.1225\n",
+      "Epoch 360/500\n",
+      "339/339 [==============================] - 0s 155us/sample - loss: 7.7217 - mean_squared_error: 7.7217 - val_loss: 59.9958 - val_mean_squared_error: 59.9958\n",
+      "Epoch 361/500\n",
+      "339/339 [==============================] - 0s 143us/sample - loss: 7.7066 - mean_squared_error: 7.7066 - val_loss: 59.8704 - val_mean_squared_error: 59.8704\n",
+      "Epoch 362/500\n",
+      "339/339 [==============================] - 0s 236us/sample - loss: 7.6917 - mean_squared_error: 7.6917 - val_loss: 59.7458 - val_mean_squared_error: 59.7458\n",
+      "Epoch 363/500\n",
+      "339/339 [==============================] - 0s 173us/sample - loss: 7.6769 - mean_squared_error: 7.6769 - val_loss: 59.6220 - val_mean_squared_error: 59.6220\n",
+      "Epoch 364/500\n",
+      "339/339 [==============================] - 0s 161us/sample - loss: 7.6621 - mean_squared_error: 7.6621 - val_loss: 59.4982 - val_mean_squared_error: 59.4982\n",
+      "Epoch 365/500\n",
+      "339/339 [==============================] - 0s 142us/sample - loss: 7.6474 - mean_squared_error: 7.6474 - val_loss: 59.3744 - val_mean_squared_error: 59.3744\n",
+      "Epoch 366/500\n",
+      "339/339 [==============================] - 0s 158us/sample - loss: 7.6327 - mean_squared_error: 7.6327 - val_loss: 59.2512 - val_mean_squared_error: 59.2512\n",
+      "Epoch 367/500\n",
+      "339/339 [==============================] - 0s 234us/sample - loss: 7.6180 - mean_squared_error: 7.6180 - val_loss: 59.1303 - val_mean_squared_error: 59.1303\n",
+      "Epoch 368/500\n",
+      "339/339 [==============================] - 0s 595us/sample - loss: 7.6034 - mean_squared_error: 7.6034 - val_loss: 59.0112 - val_mean_squared_error: 59.0112\n",
+      "Epoch 369/500\n",
+      "339/339 [==============================] - 0s 159us/sample - loss: 7.5887 - mean_squared_error: 7.5887 - val_loss: 58.8929 - val_mean_squared_error: 58.8929\n",
+      "Epoch 370/500\n",
+      "339/339 [==============================] - 0s 165us/sample - loss: 7.5741 - mean_squared_error: 7.5741 - val_loss: 58.7766 - val_mean_squared_error: 58.7766\n",
+      "Epoch 371/500\n",
+      "339/339 [==============================] - 0s 163us/sample - loss: 7.5596 - mean_squared_error: 7.5596 - val_loss: 58.6614 - val_mean_squared_error: 58.6614\n",
+      "Epoch 372/500\n",
+      "339/339 [==============================] - 0s 168us/sample - loss: 7.5452 - mean_squared_error: 7.5452 - val_loss: 58.5458 - val_mean_squared_error: 58.5458\n",
+      "Epoch 373/500\n",
+      "339/339 [==============================] - 0s 177us/sample - loss: 7.5308 - mean_squared_error: 7.5308 - val_loss: 58.4301 - val_mean_squared_error: 58.4301\n",
+      "Epoch 374/500\n",
+      "339/339 [==============================] - 0s 167us/sample - loss: 7.5165 - mean_squared_error: 7.5165 - val_loss: 58.3155 - val_mean_squared_error: 58.3155\n",
+      "Epoch 375/500\n",
+      "339/339 [==============================] - 0s 185us/sample - loss: 7.5022 - mean_squared_error: 7.5022 - val_loss: 58.2021 - val_mean_squared_error: 58.2021\n",
+      "Epoch 376/500\n",
+      "339/339 [==============================] - 0s 336us/sample - loss: 7.4880 - mean_squared_error: 7.4880 - val_loss: 58.0901 - val_mean_squared_error: 58.0901\n",
+      "Epoch 377/500\n",
+      "339/339 [==============================] - 0s 248us/sample - loss: 7.4738 - mean_squared_error: 7.4738 - val_loss: 57.9792 - val_mean_squared_error: 57.9792\n",
+      "Epoch 378/500\n",
+      "339/339 [==============================] - 0s 154us/sample - loss: 7.4597 - mean_squared_error: 7.4597 - val_loss: 57.8680 - val_mean_squared_error: 57.8680\n",
+      "Epoch 379/500\n",
+      "339/339 [==============================] - 0s 163us/sample - loss: 7.4457 - mean_squared_error: 7.4457 - val_loss: 57.7561 - val_mean_squared_error: 57.7561\n",
+      "Epoch 380/500\n",
+      "339/339 [==============================] - 0s 319us/sample - loss: 7.4317 - mean_squared_error: 7.4317 - val_loss: 57.6450 - val_mean_squared_error: 57.6450\n",
+      "Epoch 381/500\n",
+      "339/339 [==============================] - 0s 166us/sample - loss: 7.4180 - mean_squared_error: 7.4180 - val_loss: 57.5344 - val_mean_squared_error: 57.5344\n",
+      "Epoch 382/500\n",
+      "339/339 [==============================] - 0s 320us/sample - loss: 7.4044 - mean_squared_error: 7.4044 - val_loss: 57.4241 - val_mean_squared_error: 57.4241\n",
+      "Epoch 383/500\n",
+      "339/339 [==============================] - 0s 366us/sample - loss: 7.3909 - mean_squared_error: 7.3909 - val_loss: 57.3140 - val_mean_squared_error: 57.3140\n",
+      "Epoch 384/500\n",
+      "339/339 [==============================] - 0s 159us/sample - loss: 7.3774 - mean_squared_error: 7.3774 - val_loss: 57.2030 - val_mean_squared_error: 57.2030\n",
+      "Epoch 385/500\n",
+      "339/339 [==============================] - 0s 182us/sample - loss: 7.3640 - mean_squared_error: 7.3640 - val_loss: 57.0915 - val_mean_squared_error: 57.0915\n",
+      "Epoch 386/500\n",
+      "339/339 [==============================] - 0s 167us/sample - loss: 7.3507 - mean_squared_error: 7.3507 - val_loss: 56.9810 - val_mean_squared_error: 56.9810\n",
+      "Epoch 387/500\n",
+      "339/339 [==============================] - 0s 182us/sample - loss: 7.3375 - mean_squared_error: 7.3375 - val_loss: 56.8718 - val_mean_squared_error: 56.8718\n",
+      "Epoch 388/500\n",
+      "339/339 [==============================] - 0s 277us/sample - loss: 7.3243 - mean_squared_error: 7.3243 - val_loss: 56.7638 - val_mean_squared_error: 56.7638\n",
+      "Epoch 389/500\n",
+      "339/339 [==============================] - 0s 165us/sample - loss: 7.3112 - mean_squared_error: 7.3112 - val_loss: 56.6568 - val_mean_squared_error: 56.6568\n",
+      "Epoch 390/500\n",
+      "339/339 [==============================] - 0s 153us/sample - loss: 7.2981 - mean_squared_error: 7.2981 - val_loss: 56.5505 - val_mean_squared_error: 56.5505\n",
+      "Epoch 391/500\n",
+      "339/339 [==============================] - 0s 156us/sample - loss: 7.2851 - mean_squared_error: 7.2851 - val_loss: 56.4434 - val_mean_squared_error: 56.4434\n",
+      "Epoch 392/500\n",
+      "339/339 [==============================] - 0s 158us/sample - loss: 7.2722 - mean_squared_error: 7.2722 - val_loss: 56.3356 - val_mean_squared_error: 56.3356\n",
+      "Epoch 393/500\n",
+      "339/339 [==============================] - 0s 158us/sample - loss: 7.2594 - mean_squared_error: 7.2594 - val_loss: 56.2265 - val_mean_squared_error: 56.2265\n",
+      "Epoch 394/500\n",
+      "339/339 [==============================] - 0s 492us/sample - loss: 7.2468 - mean_squared_error: 7.2468 - val_loss: 56.1176 - val_mean_squared_error: 56.1176\n",
+      "Epoch 395/500\n",
+      "339/339 [==============================] - 0s 261us/sample - loss: 7.2342 - mean_squared_error: 7.2342 - val_loss: 56.0090 - val_mean_squared_error: 56.0090\n",
+      "Epoch 396/500\n",
+      "339/339 [==============================] - 0s 180us/sample - loss: 7.2218 - mean_squared_error: 7.2218 - val_loss: 55.9009 - val_mean_squared_error: 55.9009\n",
+      "Epoch 397/500\n",
+      "339/339 [==============================] - 0s 151us/sample - loss: 7.2094 - mean_squared_error: 7.2094 - val_loss: 55.7926 - val_mean_squared_error: 55.7926\n",
+      "Epoch 398/500\n",
+      "339/339 [==============================] - 0s 146us/sample - loss: 7.1970 - mean_squared_error: 7.1970 - val_loss: 55.6849 - val_mean_squared_error: 55.6849\n",
+      "Epoch 399/500\n",
+      "339/339 [==============================] - 0s 152us/sample - loss: 7.1846 - mean_squared_error: 7.1846 - val_loss: 55.5777 - val_mean_squared_error: 55.5777\n",
+      "Epoch 400/500\n",
+      "339/339 [==============================] - 0s 254us/sample - loss: 7.1722 - mean_squared_error: 7.1722 - val_loss: 55.4720 - val_mean_squared_error: 55.4720\n",
+      "Epoch 401/500\n",
+      "339/339 [==============================] - 0s 175us/sample - loss: 7.1599 - mean_squared_error: 7.1599 - val_loss: 55.3676 - val_mean_squared_error: 55.3676\n",
+      "Epoch 402/500\n",
+      "339/339 [==============================] - 0s 156us/sample - loss: 7.1477 - mean_squared_error: 7.1477 - val_loss: 55.2643 - val_mean_squared_error: 55.2643\n",
+      "Epoch 403/500\n",
+      "339/339 [==============================] - 0s 156us/sample - loss: 7.1355 - mean_squared_error: 7.1355 - val_loss: 55.1619 - val_mean_squared_error: 55.1619\n",
+      "Epoch 404/500\n",
+      "339/339 [==============================] - 0s 160us/sample - loss: 7.1232 - mean_squared_error: 7.1232 - val_loss: 55.0617 - val_mean_squared_error: 55.0617\n",
+      "Epoch 405/500\n",
+      "339/339 [==============================] - 0s 160us/sample - loss: 7.1109 - mean_squared_error: 7.1109 - val_loss: 54.9622 - val_mean_squared_error: 54.9622\n",
+      "Epoch 406/500\n",
+      "339/339 [==============================] - 0s 168us/sample - loss: 7.0985 - mean_squared_error: 7.0985 - val_loss: 54.8636 - val_mean_squared_error: 54.8636\n",
+      "Epoch 407/500\n",
+      "339/339 [==============================] - 0s 149us/sample - loss: 7.0861 - mean_squared_error: 7.0861 - val_loss: 54.7668 - val_mean_squared_error: 54.7668\n",
+      "Epoch 408/500\n",
+      "339/339 [==============================] - 0s 169us/sample - loss: 7.0737 - mean_squared_error: 7.0737 - val_loss: 54.6715 - val_mean_squared_error: 54.6715\n",
+      "Epoch 409/500\n",
+      "339/339 [==============================] - 0s 383us/sample - loss: 7.0613 - mean_squared_error: 7.0613 - val_loss: 54.5779 - val_mean_squared_error: 54.5779\n",
+      "Epoch 410/500\n",
+      "339/339 [==============================] - 0s 338us/sample - loss: 7.0489 - mean_squared_error: 7.0489 - val_loss: 54.4855 - val_mean_squared_error: 54.4855\n",
+      "Epoch 411/500\n",
+      "339/339 [==============================] - 0s 352us/sample - loss: 7.0368 - mean_squared_error: 7.0368 - val_loss: 54.3933 - val_mean_squared_error: 54.3933\n",
+      "Epoch 412/500\n",
+      "339/339 [==============================] - 0s 276us/sample - loss: 7.0248 - mean_squared_error: 7.0248 - val_loss: 54.3006 - val_mean_squared_error: 54.3006\n",
+      "Epoch 413/500\n",
+      "339/339 [==============================] - 0s 275us/sample - loss: 7.0130 - mean_squared_error: 7.0130 - val_loss: 54.2074 - val_mean_squared_error: 54.2074\n",
+      "Epoch 414/500\n",
+      "339/339 [==============================] - 0s 328us/sample - loss: 7.0012 - mean_squared_error: 7.0012 - val_loss: 54.1135 - val_mean_squared_error: 54.1135\n",
+      "Epoch 415/500\n",
+      "339/339 [==============================] - 0s 239us/sample - loss: 6.9895 - mean_squared_error: 6.9895 - val_loss: 54.0192 - val_mean_squared_error: 54.0192\n",
+      "Epoch 416/500\n",
+      "339/339 [==============================] - 0s 245us/sample - loss: 6.9777 - mean_squared_error: 6.9777 - val_loss: 53.9246 - val_mean_squared_error: 53.9246\n",
+      "Epoch 417/500\n",
+      "339/339 [==============================] - 0s 242us/sample - loss: 6.9660 - mean_squared_error: 6.9660 - val_loss: 53.8302 - val_mean_squared_error: 53.8302\n",
+      "Epoch 418/500\n",
+      "339/339 [==============================] - 0s 691us/sample - loss: 6.9543 - mean_squared_error: 6.9543 - val_loss: 53.7360 - val_mean_squared_error: 53.7360\n",
+      "Epoch 419/500\n",
+      "339/339 [==============================] - 0s 183us/sample - loss: 6.9427 - mean_squared_error: 6.9427 - val_loss: 53.6425 - val_mean_squared_error: 53.6425\n",
+      "Epoch 420/500\n",
+      "339/339 [==============================] - 0s 174us/sample - loss: 6.9311 - mean_squared_error: 6.9311 - val_loss: 53.5477 - val_mean_squared_error: 53.5477\n",
+      "Epoch 421/500\n",
+      "339/339 [==============================] - 0s 295us/sample - loss: 6.9194 - mean_squared_error: 6.9194 - val_loss: 53.4536 - val_mean_squared_error: 53.4536\n",
+      "Epoch 422/500\n",
+      "339/339 [==============================] - 0s 258us/sample - loss: 6.9078 - mean_squared_error: 6.9078 - val_loss: 53.3606 - val_mean_squared_error: 53.3606\n",
+      "Epoch 423/500\n",
+      "339/339 [==============================] - 0s 240us/sample - loss: 6.8960 - mean_squared_error: 6.8960 - val_loss: 53.2676 - val_mean_squared_error: 53.2676\n",
+      "Epoch 424/500\n",
+      "339/339 [==============================] - 0s 232us/sample - loss: 6.8845 - mean_squared_error: 6.8845 - val_loss: 53.1747 - val_mean_squared_error: 53.1747\n",
+      "Epoch 425/500\n",
+      "339/339 [==============================] - 0s 175us/sample - loss: 6.8731 - mean_squared_error: 6.8731 - val_loss: 53.0814 - val_mean_squared_error: 53.0814\n",
+      "Epoch 426/500\n",
+      "339/339 [==============================] - 0s 164us/sample - loss: 6.8616 - mean_squared_error: 6.8616 - val_loss: 52.9895 - val_mean_squared_error: 52.9895\n",
+      "Epoch 427/500\n",
+      "339/339 [==============================] - 0s 155us/sample - loss: 6.8502 - mean_squared_error: 6.8502 - val_loss: 52.8990 - val_mean_squared_error: 52.8990\n",
+      "Epoch 428/500\n",
+      "339/339 [==============================] - 0s 322us/sample - loss: 6.8388 - mean_squared_error: 6.8388 - val_loss: 52.8099 - val_mean_squared_error: 52.8099\n",
+      "Epoch 429/500\n",
+      "339/339 [==============================] - 0s 236us/sample - loss: 6.8275 - mean_squared_error: 6.8275 - val_loss: 52.7209 - val_mean_squared_error: 52.7209\n",
+      "Epoch 430/500\n",
+      "339/339 [==============================] - 0s 229us/sample - loss: 6.8161 - mean_squared_error: 6.8161 - val_loss: 52.6327 - val_mean_squared_error: 52.6327\n",
+      "Epoch 431/500\n",
+      "339/339 [==============================] - 0s 290us/sample - loss: 6.8048 - mean_squared_error: 6.8048 - val_loss: 52.5463 - val_mean_squared_error: 52.5463\n",
+      "Epoch 432/500\n",
+      "339/339 [==============================] - 0s 290us/sample - loss: 6.7935 - mean_squared_error: 6.7935 - val_loss: 52.4612 - val_mean_squared_error: 52.4612\n",
+      "Epoch 433/500\n",
+      "339/339 [==============================] - 0s 228us/sample - loss: 6.7823 - mean_squared_error: 6.7823 - val_loss: 52.3768 - val_mean_squared_error: 52.3768\n",
+      "Epoch 434/500\n",
+      "339/339 [==============================] - 0s 167us/sample - loss: 6.7711 - mean_squared_error: 6.7711 - val_loss: 52.2932 - val_mean_squared_error: 52.2932\n",
+      "Epoch 435/500\n",
+      "339/339 [==============================] - 0s 173us/sample - loss: 6.7600 - mean_squared_error: 6.7600 - val_loss: 52.2100 - val_mean_squared_error: 52.2100\n",
+      "Epoch 436/500\n",
+      "339/339 [==============================] - 0s 241us/sample - loss: 6.7491 - mean_squared_error: 6.7491 - val_loss: 52.1271 - val_mean_squared_error: 52.1271\n",
+      "Epoch 437/500\n",
+      "339/339 [==============================] - 0s 341us/sample - loss: 6.7382 - mean_squared_error: 6.7382 - val_loss: 52.0450 - val_mean_squared_error: 52.0450\n",
+      "Epoch 438/500\n",
+      "339/339 [==============================] - 0s 611us/sample - loss: 6.7274 - mean_squared_error: 6.7274 - val_loss: 51.9635 - val_mean_squared_error: 51.9635\n",
+      "Epoch 439/500\n",
+      "339/339 [==============================] - 0s 227us/sample - loss: 6.7167 - mean_squared_error: 6.7167 - val_loss: 51.8825 - val_mean_squared_error: 51.8825\n",
+      "Epoch 440/500\n",
+      "339/339 [==============================] - 0s 336us/sample - loss: 6.7059 - mean_squared_error: 6.7059 - val_loss: 51.8013 - val_mean_squared_error: 51.8013\n",
+      "Epoch 441/500\n",
+      "339/339 [==============================] - 0s 273us/sample - loss: 6.6953 - mean_squared_error: 6.6953 - val_loss: 51.7196 - val_mean_squared_error: 51.7196\n",
+      "Epoch 442/500\n",
+      "339/339 [==============================] - 0s 323us/sample - loss: 6.6846 - mean_squared_error: 6.6846 - val_loss: 51.6385 - val_mean_squared_error: 51.6385\n",
+      "Epoch 443/500\n",
+      "339/339 [==============================] - 0s 283us/sample - loss: 6.6740 - mean_squared_error: 6.6740 - val_loss: 51.5581 - val_mean_squared_error: 51.5581\n",
+      "Epoch 444/500\n",
+      "339/339 [==============================] - 0s 170us/sample - loss: 6.6635 - mean_squared_error: 6.6635 - val_loss: 51.4779 - val_mean_squared_error: 51.4779\n",
+      "Epoch 445/500\n",
+      "339/339 [==============================] - 0s 191us/sample - loss: 6.6530 - mean_squared_error: 6.6530 - val_loss: 51.3984 - val_mean_squared_error: 51.3984\n",
+      "Epoch 446/500\n",
+      "339/339 [==============================] - 0s 248us/sample - loss: 6.6427 - mean_squared_error: 6.6427 - val_loss: 51.3197 - val_mean_squared_error: 51.3197\n",
+      "Epoch 447/500\n",
+      "339/339 [==============================] - 0s 201us/sample - loss: 6.6323 - mean_squared_error: 6.6323 - val_loss: 51.2426 - val_mean_squared_error: 51.2426\n",
+      "Epoch 448/500\n",
+      "339/339 [==============================] - 0s 240us/sample - loss: 6.6221 - mean_squared_error: 6.6221 - val_loss: 51.1670 - val_mean_squared_error: 51.1670\n",
+      "Epoch 449/500\n",
+      "339/339 [==============================] - 0s 195us/sample - loss: 6.6119 - mean_squared_error: 6.6119 - val_loss: 51.0934 - val_mean_squared_error: 51.0934\n",
+      "Epoch 450/500\n",
+      "339/339 [==============================] - 0s 161us/sample - loss: 6.6017 - mean_squared_error: 6.6017 - val_loss: 51.0218 - val_mean_squared_error: 51.0218\n",
+      "Epoch 451/500\n",
+      "339/339 [==============================] - 0s 165us/sample - loss: 6.5916 - mean_squared_error: 6.5916 - val_loss: 50.9514 - val_mean_squared_error: 50.9514\n",
+      "Epoch 452/500\n",
+      "339/339 [==============================] - 0s 261us/sample - loss: 6.5816 - mean_squared_error: 6.5816 - val_loss: 50.8823 - val_mean_squared_error: 50.8823\n",
+      "Epoch 453/500\n",
+      "339/339 [==============================] - 0s 156us/sample - loss: 6.5717 - mean_squared_error: 6.5717 - val_loss: 50.8130 - val_mean_squared_error: 50.8130\n",
+      "Epoch 454/500\n",
+      "339/339 [==============================] - 0s 160us/sample - loss: 6.5618 - mean_squared_error: 6.5618 - val_loss: 50.7436 - val_mean_squared_error: 50.7436\n",
+      "Epoch 455/500\n",
+      "339/339 [==============================] - 0s 162us/sample - loss: 6.5519 - mean_squared_error: 6.5519 - val_loss: 50.6741 - val_mean_squared_error: 50.6741\n",
+      "Epoch 456/500\n",
+      "339/339 [==============================] - 0s 165us/sample - loss: 6.5420 - mean_squared_error: 6.5420 - val_loss: 50.6048 - val_mean_squared_error: 50.6048\n",
+      "Epoch 457/500\n",
+      "339/339 [==============================] - 0s 158us/sample - loss: 6.5321 - mean_squared_error: 6.5321 - val_loss: 50.5364 - val_mean_squared_error: 50.5364\n",
+      "Epoch 458/500\n",
+      "339/339 [==============================] - 0s 196us/sample - loss: 6.5223 - mean_squared_error: 6.5223 - val_loss: 50.4696 - val_mean_squared_error: 50.4696\n",
+      "Epoch 459/500\n",
+      "339/339 [==============================] - 0s 161us/sample - loss: 6.5126 - mean_squared_error: 6.5126 - val_loss: 50.4043 - val_mean_squared_error: 50.4043\n",
+      "Epoch 460/500\n",
+      "339/339 [==============================] - 0s 161us/sample - loss: 6.5029 - mean_squared_error: 6.5029 - val_loss: 50.3399 - val_mean_squared_error: 50.3399\n",
+      "Epoch 461/500\n",
+      "339/339 [==============================] - 0s 163us/sample - loss: 6.4932 - mean_squared_error: 6.4932 - val_loss: 50.2763 - val_mean_squared_error: 50.2763\n",
+      "Epoch 462/500\n",
+      "339/339 [==============================] - 0s 167us/sample - loss: 6.4836 - mean_squared_error: 6.4836 - val_loss: 50.2132 - val_mean_squared_error: 50.2132\n",
+      "Epoch 463/500\n",
+      "339/339 [==============================] - 0s 165us/sample - loss: 6.4740 - mean_squared_error: 6.4740 - val_loss: 50.1506 - val_mean_squared_error: 50.1506\n",
+      "Epoch 464/500\n",
+      "339/339 [==============================] - 0s 542us/sample - loss: 6.4644 - mean_squared_error: 6.4644 - val_loss: 50.0882 - val_mean_squared_error: 50.0882\n",
+      "Epoch 465/500\n",
+      "339/339 [==============================] - 0s 162us/sample - loss: 6.4549 - mean_squared_error: 6.4549 - val_loss: 50.0259 - val_mean_squared_error: 50.0259\n",
+      "Epoch 466/500\n",
+      "339/339 [==============================] - 0s 199us/sample - loss: 6.4454 - mean_squared_error: 6.4454 - val_loss: 49.9640 - val_mean_squared_error: 49.9640\n",
+      "Epoch 467/500\n",
+      "339/339 [==============================] - 0s 152us/sample - loss: 6.4359 - mean_squared_error: 6.4359 - val_loss: 49.9022 - val_mean_squared_error: 49.9022\n",
+      "Epoch 468/500\n",
+      "339/339 [==============================] - 0s 176us/sample - loss: 6.4266 - mean_squared_error: 6.4266 - val_loss: 49.8418 - val_mean_squared_error: 49.8418\n",
+      "Epoch 469/500\n",
+      "339/339 [==============================] - 0s 168us/sample - loss: 6.4172 - mean_squared_error: 6.4172 - val_loss: 49.7825 - val_mean_squared_error: 49.7825\n",
+      "Epoch 470/500\n",
+      "339/339 [==============================] - 0s 165us/sample - loss: 6.4080 - mean_squared_error: 6.4080 - val_loss: 49.7248 - val_mean_squared_error: 49.7248\n",
+      "Epoch 471/500\n",
+      "339/339 [==============================] - 0s 167us/sample - loss: 6.3987 - mean_squared_error: 6.3987 - val_loss: 49.6688 - val_mean_squared_error: 49.6688\n",
+      "Epoch 472/500\n",
+      "339/339 [==============================] - 0s 168us/sample - loss: 6.3895 - mean_squared_error: 6.3895 - val_loss: 49.6132 - val_mean_squared_error: 49.6132\n",
+      "Epoch 473/500\n",
+      "339/339 [==============================] - 0s 160us/sample - loss: 6.3804 - mean_squared_error: 6.3804 - val_loss: 49.5577 - val_mean_squared_error: 49.5577\n",
+      "Epoch 474/500\n",
+      "339/339 [==============================] - 0s 160us/sample - loss: 6.3712 - mean_squared_error: 6.3712 - val_loss: 49.5022 - val_mean_squared_error: 49.5022\n",
+      "Epoch 475/500\n",
+      "339/339 [==============================] - 0s 170us/sample - loss: 6.3621 - mean_squared_error: 6.3621 - val_loss: 49.4469 - val_mean_squared_error: 49.4469\n",
+      "Epoch 476/500\n",
+      "339/339 [==============================] - 0s 166us/sample - loss: 6.3531 - mean_squared_error: 6.3531 - val_loss: 49.3931 - val_mean_squared_error: 49.3931\n",
+      "Epoch 477/500\n",
+      "339/339 [==============================] - 0s 169us/sample - loss: 6.3441 - mean_squared_error: 6.3441 - val_loss: 49.3407 - val_mean_squared_error: 49.3407\n",
+      "Epoch 478/500\n",
+      "339/339 [==============================] - 0s 162us/sample - loss: 6.3352 - mean_squared_error: 6.3352 - val_loss: 49.2901 - val_mean_squared_error: 49.2901\n",
+      "Epoch 479/500\n",
+      "339/339 [==============================] - 0s 162us/sample - loss: 6.3264 - mean_squared_error: 6.3264 - val_loss: 49.2394 - val_mean_squared_error: 49.2394\n",
+      "Epoch 480/500\n",
+      "339/339 [==============================] - 0s 167us/sample - loss: 6.3177 - mean_squared_error: 6.3177 - val_loss: 49.1889 - val_mean_squared_error: 49.1889\n",
+      "Epoch 481/500\n",
+      "339/339 [==============================] - 0s 168us/sample - loss: 6.3089 - mean_squared_error: 6.3089 - val_loss: 49.1389 - val_mean_squared_error: 49.1389\n",
+      "Epoch 482/500\n",
+      "339/339 [==============================] - 0s 170us/sample - loss: 6.3002 - mean_squared_error: 6.3002 - val_loss: 49.0900 - val_mean_squared_error: 49.0900\n",
+      "Epoch 483/500\n",
+      "339/339 [==============================] - 0s 148us/sample - loss: 6.2915 - mean_squared_error: 6.2915 - val_loss: 49.0418 - val_mean_squared_error: 49.0418\n",
+      "Epoch 484/500\n",
+      "339/339 [==============================] - 0s 174us/sample - loss: 6.2829 - mean_squared_error: 6.2829 - val_loss: 48.9942 - val_mean_squared_error: 48.9942\n",
+      "Epoch 485/500\n",
+      "339/339 [==============================] - 0s 162us/sample - loss: 6.2743 - mean_squared_error: 6.2743 - val_loss: 48.9473 - val_mean_squared_error: 48.9473\n",
+      "Epoch 486/500\n",
+      "339/339 [==============================] - 0s 163us/sample - loss: 6.2657 - mean_squared_error: 6.2657 - val_loss: 48.9013 - val_mean_squared_error: 48.9013\n",
+      "Epoch 487/500\n",
+      "339/339 [==============================] - 0s 171us/sample - loss: 6.2571 - mean_squared_error: 6.2571 - val_loss: 48.8561 - val_mean_squared_error: 48.8561\n",
+      "Epoch 488/500\n",
+      "339/339 [==============================] - 0s 180us/sample - loss: 6.2487 - mean_squared_error: 6.2487 - val_loss: 48.8120 - val_mean_squared_error: 48.8120\n",
+      "Epoch 489/500\n",
+      "339/339 [==============================] - 0s 159us/sample - loss: 6.2402 - mean_squared_error: 6.2402 - val_loss: 48.7685 - val_mean_squared_error: 48.7685\n",
+      "Epoch 490/500\n",
+      "339/339 [==============================] - 0s 177us/sample - loss: 6.2318 - mean_squared_error: 6.2318 - val_loss: 48.7258 - val_mean_squared_error: 48.7258\n",
+      "Epoch 491/500\n",
+      "339/339 [==============================] - 0s 161us/sample - loss: 6.2234 - mean_squared_error: 6.2234 - val_loss: 48.6840 - val_mean_squared_error: 48.6840\n",
+      "Epoch 492/500\n",
+      "339/339 [==============================] - 0s 164us/sample - loss: 6.2150 - mean_squared_error: 6.2150 - val_loss: 48.6429 - val_mean_squared_error: 48.6429\n",
+      "Epoch 493/500\n",
+      "339/339 [==============================] - 0s 163us/sample - loss: 6.2066 - mean_squared_error: 6.2066 - val_loss: 48.6019 - val_mean_squared_error: 48.6019\n",
+      "Epoch 494/500\n",
+      "339/339 [==============================] - 0s 163us/sample - loss: 6.1983 - mean_squared_error: 6.1983 - val_loss: 48.5609 - val_mean_squared_error: 48.5609\n",
+      "Epoch 495/500\n",
+      "339/339 [==============================] - 0s 175us/sample - loss: 6.1900 - mean_squared_error: 6.1900 - val_loss: 48.5198 - val_mean_squared_error: 48.5198\n",
+      "Epoch 496/500\n",
+      "339/339 [==============================] - 0s 595us/sample - loss: 6.1818 - mean_squared_error: 6.1818 - val_loss: 48.4785 - val_mean_squared_error: 48.4785\n",
+      "Epoch 497/500\n",
+      "339/339 [==============================] - 0s 214us/sample - loss: 6.1736 - mean_squared_error: 6.1736 - val_loss: 48.4373 - val_mean_squared_error: 48.4373\n",
+      "Epoch 498/500\n",
+      "339/339 [==============================] - 0s 164us/sample - loss: 6.1654 - mean_squared_error: 6.1654 - val_loss: 48.3960 - val_mean_squared_error: 48.3960\n",
+      "Epoch 499/500\n",
+      "339/339 [==============================] - 0s 158us/sample - loss: 6.1573 - mean_squared_error: 6.1573 - val_loss: 48.3547 - val_mean_squared_error: 48.3547\n",
+      "Epoch 500/500\n",
+      "339/339 [==============================] - 0s 167us/sample - loss: 6.1492 - mean_squared_error: 6.1492 - val_loss: 48.3145 - val_mean_squared_error: 48.3145\n"
      ]
     },
     {
      "data": {
       "text/plain": [
-       "<tensorflow.python.keras.callbacks.History at 0x1a2c41aa90>"
+       "<tensorflow.python.keras.callbacks.History at 0x1a3a42f6d8>"
       ]
      },
-     "execution_count": 9,
+     "execution_count": 17,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -442,8 +1376,13 @@
    "source": [
     "# Important Hyperparameters\n",
     "inputs = X.shape[1]\n",
-    "epochs = 50\n",
-    "batch_size = 10\n",
+    "# wandb.config.update(allow_val_change=True)\n",
+    "# wandb.config.epochs = 50\n",
+    "# wandb.config.batch_size = 20\n",
+    "wandb.config.update({\"epochs\": 500, \"batch_size\": 32}, allow_val_change=True)\n",
+    "\n",
+    "\n",
+    "\n",
     "\n",
     "# Create Model\n",
     "model = Sequential()\n",
@@ -452,8 +1391,15 @@
     "model.add(Dense(1))\n",
     "# Compile Model\n",
     "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
+    "\n",
     "# Fit Model\n",
-    "model.fit(X, y, validation_split=0.33, epochs=epochs, batch_size=batch_size)"
+    "\n",
+    "model.fit(X, y, \n",
+    "          validation_split=0.33, \n",
+    "          epochs=wandb.config.epochs, \n",
+    "          batch_size=wandb.config.epochs, \n",
+    "          callbacks=[WandbCallback()]\n",
+    "         )"
    ]
   },
   {
@@ -470,7 +1416,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 10,
+   "execution_count": 20,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -481,128 +1427,52 @@
     "outputId": "ccab02a8-f28f-4db9-9065-89b52b177266"
    },
    "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Train on 339 samples, validate on 167 samples\n",
-      "Epoch 1/50\n",
-      "339/339 [==============================] - 0s 383us/sample - loss: 496.6472 - mean_squared_error: 496.6472 - val_loss: 325.2020 - val_mean_squared_error: 325.2020\n",
-      "Epoch 2/50\n",
-      "339/339 [==============================] - 0s 103us/sample - loss: 243.5689 - mean_squared_error: 243.5689 - val_loss: 112.0480 - val_mean_squared_error: 112.0480\n",
-      "Epoch 3/50\n",
-      "339/339 [==============================] - 0s 92us/sample - loss: 82.1482 - mean_squared_error: 82.1482 - val_loss: 49.1192 - val_mean_squared_error: 49.1192\n",
-      "Epoch 4/50\n",
-      "339/339 [==============================] - 0s 83us/sample - loss: 44.6474 - mean_squared_error: 44.6474 - val_loss: 33.1579 - val_mean_squared_error: 33.1579\n",
-      "Epoch 5/50\n",
-      "339/339 [==============================] - 0s 86us/sample - loss: 33.9135 - mean_squared_error: 33.9135 - val_loss: 26.5114 - val_mean_squared_error: 26.5114\n",
-      "Epoch 6/50\n",
-      "339/339 [==============================] - 0s 90us/sample - loss: 29.1068 - mean_squared_error: 29.1068 - val_loss: 22.8563 - val_mean_squared_error: 22.8563\n",
-      "Epoch 7/50\n",
-      "339/339 [==============================] - 0s 86us/sample - loss: 26.0441 - mean_squared_error: 26.0441 - val_loss: 21.0577 - val_mean_squared_error: 21.0577\n",
-      "Epoch 8/50\n",
-      "339/339 [==============================] - 0s 83us/sample - loss: 23.8012 - mean_squared_error: 23.8012 - val_loss: 19.6500 - val_mean_squared_error: 19.6500\n",
-      "Epoch 9/50\n",
-      "339/339 [==============================] - 0s 87us/sample - loss: 22.0960 - mean_squared_error: 22.0960 - val_loss: 18.4615 - val_mean_squared_error: 18.4615\n",
-      "Epoch 10/50\n",
-      "339/339 [==============================] - 0s 87us/sample - loss: 20.4015 - mean_squared_error: 20.4015 - val_loss: 16.8798 - val_mean_squared_error: 16.8798\n",
-      "Epoch 11/50\n",
-      "339/339 [==============================] - 0s 87us/sample - loss: 18.8368 - mean_squared_error: 18.8368 - val_loss: 16.0103 - val_mean_squared_error: 16.0103\n",
-      "Epoch 12/50\n",
-      "339/339 [==============================] - 0s 84us/sample - loss: 17.7578 - mean_squared_error: 17.7578 - val_loss: 15.9651 - val_mean_squared_error: 15.9651\n",
-      "Epoch 13/50\n",
-      "339/339 [==============================] - 0s 82us/sample - loss: 16.6941 - mean_squared_error: 16.6941 - val_loss: 17.1631 - val_mean_squared_error: 17.1631\n",
-      "Epoch 14/50\n",
-      "339/339 [==============================] - 0s 85us/sample - loss: 16.4518 - mean_squared_error: 16.4518 - val_loss: 14.2232 - val_mean_squared_error: 14.2232\n",
-      "Epoch 15/50\n",
-      "339/339 [==============================] - 0s 83us/sample - loss: 15.2741 - mean_squared_error: 15.2741 - val_loss: 14.8824 - val_mean_squared_error: 14.8824\n",
-      "Epoch 16/50\n",
-      "339/339 [==============================] - 0s 88us/sample - loss: 15.0555 - mean_squared_error: 15.0555 - val_loss: 13.6472 - val_mean_squared_error: 13.6472\n",
-      "Epoch 17/50\n",
-      "339/339 [==============================] - 0s 87us/sample - loss: 14.4602 - mean_squared_error: 14.4602 - val_loss: 12.9781 - val_mean_squared_error: 12.9781\n",
-      "Epoch 18/50\n",
-      "339/339 [==============================] - 0s 89us/sample - loss: 13.8586 - mean_squared_error: 13.8586 - val_loss: 12.5347 - val_mean_squared_error: 12.5347\n",
-      "Epoch 19/50\n",
-      "339/339 [==============================] - 0s 87us/sample - loss: 13.6874 - mean_squared_error: 13.6874 - val_loss: 12.4062 - val_mean_squared_error: 12.4062\n",
-      "Epoch 20/50\n",
-      "339/339 [==============================] - 0s 89us/sample - loss: 13.1289 - mean_squared_error: 13.1289 - val_loss: 12.8965 - val_mean_squared_error: 12.8965\n",
-      "Epoch 21/50\n",
-      "339/339 [==============================] - 0s 90us/sample - loss: 12.9636 - mean_squared_error: 12.9636 - val_loss: 11.8393 - val_mean_squared_error: 11.8393\n",
-      "Epoch 22/50\n",
-      "339/339 [==============================] - 0s 84us/sample - loss: 12.6905 - mean_squared_error: 12.6905 - val_loss: 11.9046 - val_mean_squared_error: 11.9046\n",
-      "Epoch 23/50\n",
-      "339/339 [==============================] - 0s 84us/sample - loss: 12.3174 - mean_squared_error: 12.3174 - val_loss: 11.7349 - val_mean_squared_error: 11.7349\n",
-      "Epoch 24/50\n",
-      "339/339 [==============================] - 0s 88us/sample - loss: 11.9281 - mean_squared_error: 11.9281 - val_loss: 11.9656 - val_mean_squared_error: 11.9656\n",
-      "Epoch 25/50\n",
-      "339/339 [==============================] - 0s 84us/sample - loss: 11.8962 - mean_squared_error: 11.8962 - val_loss: 12.0452 - val_mean_squared_error: 12.0452\n",
-      "Epoch 26/50\n",
-      "339/339 [==============================] - 0s 83us/sample - loss: 11.5107 - mean_squared_error: 11.5107 - val_loss: 11.4149 - val_mean_squared_error: 11.4149\n",
-      "Epoch 27/50\n",
-      "339/339 [==============================] - 0s 84us/sample - loss: 11.2426 - mean_squared_error: 11.2426 - val_loss: 11.2239 - val_mean_squared_error: 11.2239\n",
-      "Epoch 28/50\n",
-      "339/339 [==============================] - 0s 84us/sample - loss: 11.1747 - mean_squared_error: 11.1747 - val_loss: 11.5999 - val_mean_squared_error: 11.5999\n",
-      "Epoch 29/50\n",
-      "339/339 [==============================] - 0s 89us/sample - loss: 10.8296 - mean_squared_error: 10.8296 - val_loss: 11.1040 - val_mean_squared_error: 11.1040\n",
-      "Epoch 30/50\n",
-      "339/339 [==============================] - 0s 83us/sample - loss: 10.5858 - mean_squared_error: 10.5858 - val_loss: 11.6639 - val_mean_squared_error: 11.6639\n",
-      "Epoch 31/50\n",
-      "339/339 [==============================] - 0s 82us/sample - loss: 10.3166 - mean_squared_error: 10.3166 - val_loss: 11.0816 - val_mean_squared_error: 11.0816\n",
-      "Epoch 32/50\n",
-      "339/339 [==============================] - 0s 83us/sample - loss: 10.3662 - mean_squared_error: 10.3662 - val_loss: 10.9698 - val_mean_squared_error: 10.9698\n",
-      "Epoch 33/50\n",
-      "339/339 [==============================] - 0s 84us/sample - loss: 10.3668 - mean_squared_error: 10.3668 - val_loss: 11.0636 - val_mean_squared_error: 11.0636\n",
-      "Epoch 34/50\n",
-      "339/339 [==============================] - 0s 85us/sample - loss: 10.2008 - mean_squared_error: 10.2008 - val_loss: 11.2904 - val_mean_squared_error: 11.2904\n",
-      "Epoch 35/50\n",
-      "339/339 [==============================] - 0s 85us/sample - loss: 9.7613 - mean_squared_error: 9.7613 - val_loss: 11.9080 - val_mean_squared_error: 11.9080\n",
-      "Epoch 36/50\n",
-      "339/339 [==============================] - 0s 80us/sample - loss: 9.6370 - mean_squared_error: 9.6370 - val_loss: 11.0098 - val_mean_squared_error: 11.0098\n",
-      "Epoch 37/50\n",
-      "339/339 [==============================] - 0s 82us/sample - loss: 9.4705 - mean_squared_error: 9.4705 - val_loss: 10.7119 - val_mean_squared_error: 10.7119\n",
-      "Epoch 38/50\n",
-      "339/339 [==============================] - 0s 85us/sample - loss: 9.6508 - mean_squared_error: 9.6508 - val_loss: 10.7547 - val_mean_squared_error: 10.7547\n",
-      "Epoch 39/50\n",
-      "339/339 [==============================] - 0s 85us/sample - loss: 9.1422 - mean_squared_error: 9.1422 - val_loss: 10.7156 - val_mean_squared_error: 10.7156\n",
-      "Epoch 40/50\n",
-      "339/339 [==============================] - 0s 83us/sample - loss: 9.0300 - mean_squared_error: 9.0300 - val_loss: 10.7564 - val_mean_squared_error: 10.7564\n",
-      "Epoch 41/50\n",
-      "339/339 [==============================] - 0s 87us/sample - loss: 8.8902 - mean_squared_error: 8.8902 - val_loss: 10.4726 - val_mean_squared_error: 10.4726\n",
-      "Epoch 42/50\n",
-      "339/339 [==============================] - 0s 87us/sample - loss: 8.7206 - mean_squared_error: 8.7206 - val_loss: 10.9554 - val_mean_squared_error: 10.9554\n",
-      "Epoch 43/50\n",
-      "339/339 [==============================] - 0s 82us/sample - loss: 8.6436 - mean_squared_error: 8.6436 - val_loss: 10.4429 - val_mean_squared_error: 10.4429\n",
-      "Epoch 44/50\n",
-      "339/339 [==============================] - 0s 83us/sample - loss: 8.7554 - mean_squared_error: 8.7554 - val_loss: 10.3802 - val_mean_squared_error: 10.3802\n",
-      "Epoch 45/50\n",
-      "339/339 [==============================] - 0s 81us/sample - loss: 8.2557 - mean_squared_error: 8.2557 - val_loss: 10.3291 - val_mean_squared_error: 10.3291\n",
-      "Epoch 46/50\n",
-      "339/339 [==============================] - 0s 85us/sample - loss: 8.3303 - mean_squared_error: 8.3303 - val_loss: 10.2898 - val_mean_squared_error: 10.2898\n",
-      "Epoch 47/50\n",
-      "339/339 [==============================] - 0s 90us/sample - loss: 8.1261 - mean_squared_error: 8.1261 - val_loss: 10.8009 - val_mean_squared_error: 10.8009\n",
-      "Epoch 48/50\n",
-      "339/339 [==============================] - 0s 85us/sample - loss: 7.9084 - mean_squared_error: 7.9084 - val_loss: 10.3298 - val_mean_squared_error: 10.3298\n",
-      "Epoch 49/50\n",
-      "339/339 [==============================] - 0s 84us/sample - loss: 7.6370 - mean_squared_error: 7.6370 - val_loss: 10.9119 - val_mean_squared_error: 10.9119\n",
-      "Epoch 50/50\n",
-      "339/339 [==============================] - 0s 93us/sample - loss: 7.9129 - mean_squared_error: 7.9129 - val_loss: 10.5253 - val_mean_squared_error: 10.5253\n"
-     ]
-    },
     {
      "data": {
+      "text/html": [
+       "\n",
+       "        Notebook configured with <a href=\"https://wandb.com\" target=\"_blank\">W&B</a>. You can <a href=\"https://app.wandb.ai/ds5/ds5-hyperparameter-tuning/runs/lmptsefv\" target=\"_blank\">open</a> the run page, or call <code>%%wandb</code>\n",
+       "        in a cell containing your training loop to display live results.  Learn more in our <a href=\"https://docs.wandb.com/docs/integrations/jupyter.html\" target=\"_blank\">docs</a>.\n",
+       "    "
+      ],
       "text/plain": [
-       "<tensorflow.python.keras.callbacks.History at 0x1a2ce7e3c8>"
+       "<IPython.core.display.HTML object>"
       ]
      },
-     "execution_count": 10,
      "metadata": {},
-     "output_type": "execute_result"
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "E0912 10:15:02.947648 4512605632 jupyter.py:96] Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n"
+     ]
+    },
+    {
+     "ename": "TypeError",
+     "evalue": "'NoneType' object is not iterable",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
+      "\u001b[0;32m<ipython-input-20-650e131b1d5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m           batch_size=wandb.config.batch_size)\n\u001b[0m",
+      "\u001b[0;32m~/anaconda3/envs/U4-S2-NNF/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
+      "\u001b[0;32m~/anaconda3/envs/U4-S2-NNF/lib/python3.7/site-packages/wandb/keras/__init__.py\u001b[0m in \u001b[0;36mnew_arrays\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# TODO: these could be generators, why index 0?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mval_inputs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mval_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mval_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_targets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
+     ]
     }
    ],
    "source": [
     "from sklearn.datasets import load_boston\n",
     "from sklearn.model_selection import train_test_split\n",
     "\n",
+    "# Create a Fresh Experiment\n",
+    "wandb.init(project=\"ds5-hyperparameter-tuning\", entity=\"ds5\")\n",
+    "wandb.config.epochs = 50\n",
+    "wandb.config.batch_size = 10\n",
+    "wandb.config.hidden_1_size=10\n",
+    "\n",
     "# Random Seed\n",
     "seed = 42\n",
     "numpy.random.seed(seed)\n",
@@ -633,10 +1503,16 @@
     "model.add(Dense(64, activation='relu', input_shape=(inputs,)))\n",
     "model.add(Dense(64, activation='relu'))\n",
     "model.add(Dense(1))\n",
+    "\n",
     "# Compile Model\n",
-    "model.compile(optimizer='rmsprop', loss='mse', metrics=['mse'])\n",
+    "model.compile(optimizer='rmsprop', loss='mse', metrics=['mse']])\n",
+    "\n",
     "# Fit Model\n",
-    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size)\n"
+    "model.fit(X_train, y_train, \n",
+    "          validation_data=(X_test, y_test), \n",
+    "          epochs=wandb.config.epochs, \n",
+    "          batch_size=wandb.config.batch_size,\n",
+    "          callbacks=[WandbCallback())\n"
    ]
   },
   {
@@ -658,7 +1534,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 14,
+   "execution_count": 6,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -673,7 +1549,7 @@
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "K-fold Cross-Val Results - Mean: 23.24 StDev: 14.49 MSE\n"
+      "K-fold Cross-Val Results - Mean: 22.75 StDev: 14.14 MSE\n"
      ]
     }
    ],
@@ -741,7 +1617,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 18,
+   "execution_count": 7,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -756,7 +1632,7 @@
      "name": "stderr",
      "output_type": "stream",
      "text": [
-      "W0815 10:16:17.225039 4430419392 deprecation.py:323] From /Users/jonathansokoll/anaconda3/envs/U4-S2-NNF/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
+      "W0815 11:30:39.552013 4802471360 deprecation.py:323] From /Users/jonathansokoll/anaconda3/envs/U4-S2-NNF/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
       "Instructions for updating:\n",
       "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
      ]
@@ -765,12 +1641,12 @@
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "acc: 75.32%\n",
       "acc: 76.62%\n",
-      "acc: 67.53%\n",
-      "acc: 68.63%\n",
-      "acc: 69.28%\n",
-      "71.48% +/- 3.74%\n"
+      "acc: 75.97%\n",
+      "acc: 70.78%\n",
+      "acc: 72.55%\n",
+      "acc: 71.90%\n",
+      "73.56% +/- 2.31%\n"
      ]
     }
    ],
@@ -796,7 +1672,9 @@
     "\n",
     "# define 5-fold cross validation test harness\n",
     "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
+    "\n",
     "cvscores = []\n",
+    "\n",
     "for train, test in kfold.split(X, Y):\n",
     "  # create model\n",
     "  model = Sequential()\n",
@@ -810,6 +1688,7 @@
     "  scores = model.evaluate(X[test], Y[test], verbose=0)\n",
     "  print(f'{model.metrics_names[1]}: {(scores[1]*100):.2f}%') \n",
     "  cvscores.append(scores[1]*100)\n",
+    "    \n",
     "print(f'{numpy.mean(cvscores):.2f}% +/- {numpy.std(cvscores):.2f}%')"
    ]
   },
@@ -906,7 +1785,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 20,
+   "execution_count": 11,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -917,803 +1796,17 @@
     "outputId": "ae996575-78e2-43fb-9dbe-5d44aaf0b430"
    },
    "outputs": [
-    {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "/Users/jonathansokoll/anaconda3/envs/U4-S2-NNF/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
-      "  warnings.warn(CV_WARNING, FutureWarning)\n"
-     ]
-    },
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "Epoch 1/20\n",
-      "512/512 [==============================] - 0s 419us/sample - loss: 15.3302 - acc: 0.6660\n",
-      "Epoch 2/20\n",
-      "512/512 [==============================] - 0s 95us/sample - loss: 3.3954 - acc: 0.6348\n",
-      "Epoch 3/20\n",
-      "512/512 [==============================] - 0s 88us/sample - loss: 1.5552 - acc: 0.5938\n",
-      "Epoch 4/20\n",
-      "512/512 [==============================] - 0s 87us/sample - loss: 1.3141 - acc: 0.5996\n",
-      "Epoch 5/20\n",
-      "512/512 [==============================] - 0s 87us/sample - loss: 1.1193 - acc: 0.6016\n",
-      "Epoch 6/20\n",
-      "512/512 [==============================] - 0s 89us/sample - loss: 1.0316 - acc: 0.6270\n",
-      "Epoch 7/20\n",
-      "512/512 [==============================] - 0s 91us/sample - loss: 0.9822 - acc: 0.6309\n",
-      "Epoch 8/20\n",
-      "512/512 [==============================] - 0s 89us/sample - loss: 0.8956 - acc: 0.6426\n",
-      "Epoch 9/20\n",
-      "512/512 [==============================] - 0s 87us/sample - loss: 0.8632 - acc: 0.6426\n",
-      "Epoch 10/20\n",
-      "512/512 [==============================] - 0s 86us/sample - loss: 0.8722 - acc: 0.6387\n",
-      "Epoch 11/20\n",
-      "512/512 [==============================] - 0s 87us/sample - loss: 0.8340 - acc: 0.6426\n",
-      "Epoch 12/20\n",
-      "512/512 [==============================] - 0s 88us/sample - loss: 0.7736 - acc: 0.6582\n",
-      "Epoch 13/20\n",
-      "512/512 [==============================] - 0s 87us/sample - loss: 0.8033 - acc: 0.6602\n",
-      "Epoch 14/20\n",
-      "512/512 [==============================] - 0s 87us/sample - loss: 0.7617 - acc: 0.6387\n",
-      "Epoch 15/20\n",
-      "512/512 [==============================] - 0s 83us/sample - loss: 0.7571 - acc: 0.6445\n",
-      "Epoch 16/20\n",
-      "512/512 [==============================] - 0s 74us/sample - loss: 0.7398 - acc: 0.6758\n",
-      "Epoch 17/20\n",
-      "512/512 [==============================] - 0s 78us/sample - loss: 0.7504 - acc: 0.6816\n",
-      "Epoch 18/20\n",
-      "512/512 [==============================] - 0s 76us/sample - loss: 0.7454 - acc: 0.6582\n",
-      "Epoch 19/20\n",
-      "512/512 [==============================] - 0s 79us/sample - loss: 0.7731 - acc: 0.6504\n",
-      "Epoch 20/20\n",
-      "512/512 [==============================] - 0s 75us/sample - loss: 0.6943 - acc: 0.6758\n",
-      "256/256 [==============================] - 0s 468us/sample - loss: 0.7612 - acc: 0.6992\n",
-      "Epoch 1/20\n",
-      "512/512 [==============================] - 0s 390us/sample - loss: 9.1114 - acc: 0.4766\n",
-      "Epoch 2/20\n",
-      "512/512 [==============================] - 0s 86us/sample - loss: 5.5282 - acc: 0.5723\n",
-      "Epoch 3/20\n",
-      "512/512 [==============================] - 0s 83us/sample - loss: 4.3788 - acc: 0.5723\n",
-      "Epoch 4/20\n",
-      "512/512 [==============================] - 0s 82us/sample - loss: 3.4485 - acc: 0.5840\n",
-      "Epoch 5/20\n",
-      "512/512 [==============================] - 0s 84us/sample - loss: 2.5568 - acc: 0.5918\n",
-      "Epoch 6/20\n",
-      "512/512 [==============================] - 0s 80us/sample - loss: 1.5828 - acc: 0.6230\n",
-      "Epoch 7/20\n",
-      "512/512 [==============================] - 0s 74us/sample - loss: 1.0696 - acc: 0.5938\n",
-      "Epoch 8/20\n",
-      "512/512 [==============================] - 0s 77us/sample - loss: 1.0402 - acc: 0.6133\n",
-      "Epoch 9/20\n",
-      "512/512 [==============================] - 0s 84us/sample - loss: 0.8922 - acc: 0.6289\n",
-      "Epoch 10/20\n",
-      "512/512 [==============================] - 0s 87us/sample - loss: 0.9074 - acc: 0.6309\n",
-      "Epoch 11/20\n",
-      "512/512 [==============================] - 0s 91us/sample - loss: 0.9143 - acc: 0.5957\n",
-      "Epoch 12/20\n",
-      "512/512 [==============================] - 0s 90us/sample - loss: 0.7977 - acc: 0.6309\n",
-      "Epoch 13/20\n",
-      "512/512 [==============================] - 0s 87us/sample - loss: 0.7761 - acc: 0.6484\n",
-      "Epoch 14/20\n",
-      "512/512 [==============================] - 0s 88us/sample - loss: 0.7725 - acc: 0.6289\n",
-      "Epoch 15/20\n",
-      "512/512 [==============================] - 0s 86us/sample - loss: 0.7280 - acc: 0.6406\n",
-      "Epoch 16/20\n",
-      "512/512 [==============================] - 0s 85us/sample - loss: 0.7468 - acc: 0.6543\n",
-      "Epoch 17/20\n",
-      "512/512 [==============================] - 0s 83us/sample - loss: 0.7334 - acc: 0.6523\n",
-      "Epoch 18/20\n",
-      "512/512 [==============================] - 0s 85us/sample - loss: 0.7951 - acc: 0.6367\n",
-      "Epoch 19/20\n",
-      "512/512 [==============================] - 0s 84us/sample - loss: 0.6843 - acc: 0.6621\n",
-      "Epoch 20/20\n",
-      "512/512 [==============================] - 0s 75us/sample - loss: 0.6816 - acc: 0.6758\n",
-      "256/256 [==============================] - 0s 445us/sample - loss: 0.7719 - acc: 0.6445\n",
-      "Epoch 1/20\n",
-      "512/512 [==============================] - 0s 392us/sample - loss: 4.7487 - acc: 0.4531\n",
-      "Epoch 2/20\n",
-      "512/512 [==============================] - 0s 79us/sample - loss: 3.4040 - acc: 0.4473\n",
-      "Epoch 3/20\n",
-      "512/512 [==============================] - 0s 77us/sample - loss: 2.3425 - acc: 0.5020\n",
-      "Epoch 4/20\n",
-      "512/512 [==============================] - 0s 81us/sample - loss: 1.6176 - acc: 0.5449\n",
-      "Epoch 5/20\n",
-      "512/512 [==============================] - 0s 77us/sample - loss: 1.2427 - acc: 0.6035\n",
-      "Epoch 6/20\n",
-      "512/512 [==============================] - 0s 81us/sample - loss: 1.0910 - acc: 0.6074\n",
-      "Epoch 7/20\n",
-      "512/512 [==============================] - 0s 77us/sample - loss: 0.9770 - acc: 0.6270\n",
-      "Epoch 8/20\n",
-      "512/512 [==============================] - 0s 82us/sample - loss: 0.9029 - acc: 0.6328\n",
-      "Epoch 9/20\n",
-      "512/512 [==============================] - 0s 79us/sample - loss: 0.8453 - acc: 0.6348\n",
-      "Epoch 10/20\n",
-      "512/512 [==============================] - 0s 81us/sample - loss: 0.8189 - acc: 0.6348\n",
-      "Epoch 11/20\n",
-      "512/512 [==============================] - 0s 78us/sample - loss: 0.7726 - acc: 0.6309\n",
-      "Epoch 12/20\n",
-      "512/512 [==============================] - 0s 77us/sample - loss: 0.7476 - acc: 0.6641\n",
-      "Epoch 13/20\n",
-      "512/512 [==============================] - 0s 78us/sample - loss: 0.7287 - acc: 0.6641\n",
-      "Epoch 14/20\n",
-      "512/512 [==============================] - 0s 78us/sample - loss: 0.7082 - acc: 0.6641\n",
-      "Epoch 15/20\n",
-      "512/512 [==============================] - 0s 78us/sample - loss: 0.6935 - acc: 0.6680\n",
-      "Epoch 16/20\n",
-      "512/512 [==============================] - 0s 81us/sample - loss: 0.6797 - acc: 0.6641\n",
-      "Epoch 17/20\n",
-      "512/512 [==============================] - 0s 77us/sample - loss: 0.6644 - acc: 0.6582\n",
-      "Epoch 18/20\n",
-      "512/512 [==============================] - 0s 80us/sample - loss: 0.6564 - acc: 0.6660\n",
-      "Epoch 19/20\n",
-      "512/512 [==============================] - 0s 91us/sample - loss: 0.6543 - acc: 0.6484\n",
-      "Epoch 20/20\n",
-      "512/512 [==============================] - 0s 84us/sample - loss: 0.6575 - acc: 0.6621\n",
-      "256/256 [==============================] - 0s 482us/sample - loss: 0.7347 - acc: 0.6562\n",
-      "Epoch 1/20\n",
-      "512/512 [==============================] - 0s 373us/sample - loss: 3.6687 - acc: 0.4844\n",
-      "Epoch 2/20\n",
-      "512/512 [==============================] - 0s 53us/sample - loss: 2.1635 - acc: 0.5645\n",
-      "Epoch 3/20\n",
-      "512/512 [==============================] - 0s 53us/sample - loss: 1.9848 - acc: 0.5625\n",
-      "Epoch 4/20\n",
-      "512/512 [==============================] - 0s 50us/sample - loss: 1.8195 - acc: 0.5586\n",
-      "Epoch 5/20\n",
-      "512/512 [==============================] - 0s 49us/sample - loss: 1.7241 - acc: 0.5625\n",
-      "Epoch 6/20\n",
-      "512/512 [==============================] - 0s 50us/sample - loss: 1.6064 - acc: 0.5801\n",
-      "Epoch 7/20\n",
-      "512/512 [==============================] - 0s 49us/sample - loss: 1.5171 - acc: 0.5801\n",
-      "Epoch 8/20\n",
-      "512/512 [==============================] - 0s 49us/sample - loss: 1.4284 - acc: 0.5918\n",
-      "Epoch 9/20\n",
-      "512/512 [==============================] - 0s 48us/sample - loss: 1.3014 - acc: 0.5859\n",
-      "Epoch 10/20\n",
-      "512/512 [==============================] - 0s 49us/sample - loss: 1.2618 - acc: 0.5918\n",
-      "Epoch 11/20\n",
-      "512/512 [==============================] - 0s 49us/sample - loss: 1.1691 - acc: 0.6133\n",
-      "Epoch 12/20\n",
-      "512/512 [==============================] - 0s 50us/sample - loss: 1.1232 - acc: 0.6172\n",
-      "Epoch 13/20\n",
-      "512/512 [==============================] - 0s 48us/sample - loss: 1.0428 - acc: 0.6309\n",
-      "Epoch 14/20\n",
-      "512/512 [==============================] - 0s 49us/sample - loss: 0.9959 - acc: 0.6270\n",
-      "Epoch 15/20\n",
-      "512/512 [==============================] - 0s 47us/sample - loss: 0.9482 - acc: 0.6328\n",
-      "Epoch 16/20\n",
-      "512/512 [==============================] - 0s 47us/sample - loss: 0.9201 - acc: 0.6426\n",
-      "Epoch 17/20\n",
-      "512/512 [==============================] - 0s 47us/sample - loss: 0.8558 - acc: 0.6602\n",
-      "Epoch 18/20\n",
-      "512/512 [==============================] - 0s 48us/sample - loss: 0.8373 - acc: 0.6543\n",
-      "Epoch 19/20\n",
-      "512/512 [==============================] - 0s 48us/sample - loss: 0.8123 - acc: 0.6562\n",
-      "Epoch 20/20\n",
-      "512/512 [==============================] - 0s 49us/sample - loss: 0.7867 - acc: 0.6758\n",
-      "256/256 [==============================] - 0s 486us/sample - loss: 0.8918 - acc: 0.6055\n",
-      "Epoch 1/20\n",
-      "512/512 [==============================] - 0s 368us/sample - loss: 30.3564 - acc: 0.6465\n",
-      "Epoch 2/20\n",
-      "512/512 [==============================] - 0s 45us/sample - loss: 20.7263 - acc: 0.6465\n",
-      "Epoch 3/20\n",
-      "512/512 [==============================] - 0s 51us/sample - loss: 11.9238 - acc: 0.6328\n",
-      "Epoch 4/20\n",
-      "512/512 [==============================] - 0s 52us/sample - loss: 6.7630 - acc: 0.6035\n",
-      "Epoch 5/20\n",
-      "512/512 [==============================] - 0s 50us/sample - loss: 3.9460 - acc: 0.5703\n",
-      "Epoch 6/20\n",
-      "512/512 [==============================] - 0s 50us/sample - loss: 2.4715 - acc: 0.5684\n",
-      "Epoch 7/20\n",
-      "512/512 [==============================] - 0s 50us/sample - loss: 1.9508 - acc: 0.6055\n",
-      "Epoch 8/20\n",
-      "512/512 [==============================] - 0s 50us/sample - loss: 1.6916 - acc: 0.5977\n",
-      "Epoch 9/20\n",
-      "512/512 [==============================] - 0s 48us/sample - loss: 1.5486 - acc: 0.6309\n",
-      "Epoch 10/20\n",
-      "512/512 [==============================] - 0s 50us/sample - loss: 1.4506 - acc: 0.6309\n",
-      "Epoch 11/20\n",
-      "512/512 [==============================] - 0s 52us/sample - loss: 1.3733 - acc: 0.6328\n",
-      "Epoch 12/20\n",
-      "512/512 [==============================] - 0s 47us/sample - loss: 1.3054 - acc: 0.6387\n",
-      "Epoch 13/20\n",
-      "512/512 [==============================] - 0s 47us/sample - loss: 1.2617 - acc: 0.6348\n",
-      "Epoch 14/20\n",
-      "512/512 [==============================] - 0s 49us/sample - loss: 1.2054 - acc: 0.6387\n",
-      "Epoch 15/20\n",
-      "512/512 [==============================] - 0s 50us/sample - loss: 1.1555 - acc: 0.6367\n",
-      "Epoch 16/20\n",
-      "512/512 [==============================] - 0s 47us/sample - loss: 1.1334 - acc: 0.6367\n",
-      "Epoch 17/20\n",
-      "512/512 [==============================] - 0s 49us/sample - loss: 1.0946 - acc: 0.6309\n",
-      "Epoch 18/20\n",
-      "512/512 [==============================] - 0s 46us/sample - loss: 1.0571 - acc: 0.6445\n",
-      "Epoch 19/20\n",
-      "512/512 [==============================] - 0s 50us/sample - loss: 1.0119 - acc: 0.6523\n",
-      "Epoch 20/20\n",
-      "512/512 [==============================] - 0s 49us/sample - loss: 0.9886 - acc: 0.6543\n",
-      "256/256 [==============================] - 0s 485us/sample - loss: 0.8301 - acc: 0.6641\n",
-      "Epoch 1/20\n",
-      "512/512 [==============================] - 0s 372us/sample - loss: 10.0618 - acc: 0.6328\n",
-      "Epoch 2/20\n",
-      "512/512 [==============================] - 0s 51us/sample - loss: 3.2820 - acc: 0.4902\n",
-      "Epoch 3/20\n",
-      "512/512 [==============================] - 0s 53us/sample - loss: 2.6071 - acc: 0.5117\n",
-      "Epoch 4/20\n",
-      "512/512 [==============================] - 0s 52us/sample - loss: 2.1527 - acc: 0.5176\n",
-      "Epoch 5/20\n",
-      "512/512 [==============================] - 0s 45us/sample - loss: 1.9088 - acc: 0.5742\n",
-      "Epoch 6/20\n",
-      "512/512 [==============================] - 0s 49us/sample - loss: 1.6627 - acc: 0.5742\n",
-      "Epoch 7/20\n",
-      "512/512 [==============================] - 0s 51us/sample - loss: 1.6007 - acc: 0.5938\n",
-      "Epoch 8/20\n",
-      "512/512 [==============================] - 0s 46us/sample - loss: 1.2936 - acc: 0.6289\n",
-      "Epoch 9/20\n",
-      "512/512 [==============================] - 0s 50us/sample - loss: 1.1793 - acc: 0.6348\n",
-      "Epoch 10/20\n",
-      "512/512 [==============================] - 0s 46us/sample - loss: 1.0948 - acc: 0.6543\n",
-      "Epoch 11/20\n",
-      "512/512 [==============================] - 0s 49us/sample - loss: 1.0196 - acc: 0.6074\n",
-      "Epoch 12/20\n",
-      "512/512 [==============================] - 0s 46us/sample - loss: 0.9237 - acc: 0.6562\n",
-      "Epoch 13/20\n",
-      "512/512 [==============================] - 0s 47us/sample - loss: 0.8814 - acc: 0.6406\n",
-      "Epoch 14/20\n",
-      "512/512 [==============================] - 0s 47us/sample - loss: 0.8317 - acc: 0.6680\n",
-      "Epoch 15/20\n",
-      "512/512 [==============================] - 0s 49us/sample - loss: 0.8194 - acc: 0.6660\n",
-      "Epoch 16/20\n",
-      "512/512 [==============================] - 0s 53us/sample - loss: 0.7761 - acc: 0.6719\n",
-      "Epoch 17/20\n",
-      "512/512 [==============================] - 0s 48us/sample - loss: 0.7750 - acc: 0.6699\n",
-      "Epoch 18/20\n",
-      "512/512 [==============================] - 0s 53us/sample - loss: 0.7388 - acc: 0.6660\n",
-      "Epoch 19/20\n",
-      "512/512 [==============================] - 0s 49us/sample - loss: 0.7342 - acc: 0.6719\n",
-      "Epoch 20/20\n",
-      "512/512 [==============================] - 0s 52us/sample - loss: 0.7102 - acc: 0.6719\n",
-      "256/256 [==============================] - 0s 482us/sample - loss: 0.7346 - acc: 0.6367\n",
-      "Epoch 1/20\n",
-      "512/512 [==============================] - 0s 350us/sample - loss: 18.3196 - acc: 0.4590\n",
-      "Epoch 2/20\n",
-      "512/512 [==============================] - 0s 30us/sample - loss: 15.5684 - acc: 0.5059\n",
-      "Epoch 3/20\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 13.1114 - acc: 0.5352\n",
-      "Epoch 4/20\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 10.9383 - acc: 0.5273\n",
-      "Epoch 5/20\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 9.0281 - acc: 0.5098\n",
-      "Epoch 6/20\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 7.3093 - acc: 0.4941\n",
-      "Epoch 7/20\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 5.8717 - acc: 0.4746\n",
-      "Epoch 8/20\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 4.7162 - acc: 0.4609\n",
-      "Epoch 9/20\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 3.9802 - acc: 0.4629\n",
-      "Epoch 10/20\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 3.5831 - acc: 0.4766\n",
-      "Epoch 11/20\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 3.3339 - acc: 0.5273\n",
-      "Epoch 12/20\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 3.1003 - acc: 0.5098\n",
-      "Epoch 13/20\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 2.9321 - acc: 0.4824\n",
-      "Epoch 14/20\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 2.7270 - acc: 0.5430\n",
-      "Epoch 15/20\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 2.5706 - acc: 0.5176\n",
-      "Epoch 16/20\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 2.3905 - acc: 0.5000\n",
-      "Epoch 17/20\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 2.3031 - acc: 0.5371\n",
-      "Epoch 18/20\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 2.0744 - acc: 0.5566\n",
-      "Epoch 19/20\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 1.9564 - acc: 0.5566\n",
-      "Epoch 20/20\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 1.8518 - acc: 0.5449\n",
-      "256/256 [==============================] - 0s 513us/sample - loss: 1.8776 - acc: 0.5820\n",
-      "Epoch 1/20\n",
-      "512/512 [==============================] - 0s 361us/sample - loss: 26.0958 - acc: 0.6465\n",
-      "Epoch 2/20\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 18.0421 - acc: 0.6348\n",
-      "Epoch 3/20\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 10.6976 - acc: 0.6172\n",
-      "Epoch 4/20\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 6.0661 - acc: 0.5762\n",
-      "Epoch 5/20\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 4.9137 - acc: 0.5801\n",
-      "Epoch 6/20\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 4.5337 - acc: 0.5820\n",
-      "Epoch 7/20\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 4.0811 - acc: 0.6094\n",
-      "Epoch 8/20\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 3.7691 - acc: 0.6094\n",
-      "Epoch 9/20\n",
-      "512/512 [==============================] - 0s 24us/sample - loss: 3.4583 - acc: 0.6113\n",
-      "Epoch 10/20\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 3.1552 - acc: 0.6230\n",
-      "Epoch 11/20\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 2.8719 - acc: 0.6113\n",
-      "Epoch 12/20\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 2.6205 - acc: 0.6309\n",
-      "Epoch 13/20\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 2.4389 - acc: 0.6387\n",
-      "Epoch 14/20\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 2.2830 - acc: 0.6445\n",
-      "Epoch 15/20\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 2.1618 - acc: 0.6543\n",
-      "Epoch 16/20\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 2.0703 - acc: 0.6504\n",
-      "Epoch 17/20\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 1.9604 - acc: 0.6465\n",
-      "Epoch 18/20\n",
-      "512/512 [==============================] - 0s 24us/sample - loss: 1.8771 - acc: 0.6445\n",
-      "Epoch 19/20\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 1.7951 - acc: 0.6562\n",
-      "Epoch 20/20\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 1.7303 - acc: 0.6602\n",
-      "256/256 [==============================] - 0s 523us/sample - loss: 2.4482 - acc: 0.5508\n",
-      "Epoch 1/20\n",
-      "512/512 [==============================] - 0s 380us/sample - loss: 12.3113 - acc: 0.3965\n",
-      "Epoch 2/20\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 7.0328 - acc: 0.5137\n",
-      "Epoch 3/20\n",
-      "512/512 [==============================] - 0s 33us/sample - loss: 4.1481 - acc: 0.5742\n",
-      "Epoch 4/20\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 3.0621 - acc: 0.6270\n",
-      "Epoch 5/20\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 2.6765 - acc: 0.6426\n",
-      "Epoch 6/20\n",
-      "512/512 [==============================] - 0s 33us/sample - loss: 2.3874 - acc: 0.6406\n",
-      "Epoch 7/20\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 2.2515 - acc: 0.6387\n",
-      "Epoch 8/20\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 2.1568 - acc: 0.6387\n",
-      "Epoch 9/20\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 2.0420 - acc: 0.6484\n",
-      "Epoch 10/20\n",
-      "512/512 [==============================] - 0s 30us/sample - loss: 1.9561 - acc: 0.6484\n",
-      "Epoch 11/20\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 1.8576 - acc: 0.6504\n",
-      "Epoch 12/20\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 1.7699 - acc: 0.6543\n",
-      "Epoch 13/20\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 1.6815 - acc: 0.6484\n",
-      "Epoch 14/20\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 1.5927 - acc: 0.6543\n",
-      "Epoch 15/20\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 1.4939 - acc: 0.6523\n",
-      "Epoch 16/20\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 1.4036 - acc: 0.6484\n",
-      "Epoch 17/20\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 1.3243 - acc: 0.6523\n",
-      "Epoch 18/20\n",
-      "512/512 [==============================] - 0s 45us/sample - loss: 1.2335 - acc: 0.6387\n",
-      "Epoch 19/20\n",
-      "512/512 [==============================] - 0s 33us/sample - loss: 1.1484 - acc: 0.6406\n",
-      "Epoch 20/20\n",
-      "512/512 [==============================] - 0s 33us/sample - loss: 1.0805 - acc: 0.6367\n",
-      "256/256 [==============================] - 0s 547us/sample - loss: 0.9339 - acc: 0.6719\n",
-      "Epoch 1/20\n",
-      "512/512 [==============================] - 0s 358us/sample - loss: 7.1647 - acc: 0.5996\n",
-      "Epoch 2/20\n",
-      "512/512 [==============================] - 0s 24us/sample - loss: 5.7492 - acc: 0.5371\n",
-      "Epoch 3/20\n",
-      "512/512 [==============================] - 0s 24us/sample - loss: 5.5667 - acc: 0.4863\n",
-      "Epoch 4/20\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 5.3361 - acc: 0.5039\n",
-      "Epoch 5/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 5.1068 - acc: 0.5156\n",
-      "Epoch 6/20\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 4.8999 - acc: 0.5234\n",
-      "Epoch 7/20\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 4.7160 - acc: 0.5176\n",
-      "Epoch 8/20\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 4.5534 - acc: 0.5156\n",
-      "Epoch 9/20\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 4.3618 - acc: 0.5176\n",
-      "Epoch 10/20\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 4.1905 - acc: 0.5195\n",
-      "Epoch 11/20\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 4.0183 - acc: 0.5273\n",
-      "Epoch 12/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 3.8485 - acc: 0.5215\n",
-      "Epoch 13/20\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 3.6705 - acc: 0.5234\n",
-      "Epoch 14/20\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 3.5121 - acc: 0.5273\n",
-      "Epoch 15/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 3.3385 - acc: 0.5352\n",
-      "Epoch 16/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 3.2022 - acc: 0.5352\n",
-      "Epoch 17/20\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 3.0172 - acc: 0.5391\n",
-      "Epoch 18/20\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 2.8784 - acc: 0.5469\n",
-      "Epoch 19/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 2.7449 - acc: 0.5391\n",
-      "Epoch 20/20\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 2.6390 - acc: 0.5605\n",
-      "256/256 [==============================] - 0s 531us/sample - loss: 2.5172 - acc: 0.5625\n",
-      "Epoch 1/20\n",
-      "512/512 [==============================] - 0s 370us/sample - loss: 17.5559 - acc: 0.6465\n",
-      "Epoch 2/20\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 14.3815 - acc: 0.6445\n",
-      "Epoch 3/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 11.0402 - acc: 0.6426\n",
-      "Epoch 4/20\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 7.8189 - acc: 0.6465\n",
-      "Epoch 5/20\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 5.4423 - acc: 0.5645\n",
-      "Epoch 6/20\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 4.6802 - acc: 0.4746\n",
-      "Epoch 7/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 4.0641 - acc: 0.4609\n",
-      "Epoch 8/20\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 3.3435 - acc: 0.4766\n",
-      "Epoch 9/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 2.9226 - acc: 0.5137\n",
-      "Epoch 10/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 2.5432 - acc: 0.5059\n",
-      "Epoch 11/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 2.3303 - acc: 0.5195\n",
-      "Epoch 12/20\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 2.1539 - acc: 0.5430\n",
-      "Epoch 13/20\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 2.0320 - acc: 0.5566\n",
-      "Epoch 14/20\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.9036 - acc: 0.5645\n",
-      "Epoch 15/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 1.8219 - acc: 0.5527\n",
-      "Epoch 16/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 1.7142 - acc: 0.5625\n",
-      "Epoch 17/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 1.6312 - acc: 0.5664\n",
-      "Epoch 18/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 1.5597 - acc: 0.5566\n",
-      "Epoch 19/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 1.4839 - acc: 0.5801\n",
-      "Epoch 20/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 1.4224 - acc: 0.5938\n",
-      "256/256 [==============================] - 0s 558us/sample - loss: 1.4621 - acc: 0.5430\n",
-      "Epoch 1/20\n",
-      "512/512 [==============================] - 0s 390us/sample - loss: 4.0413 - acc: 0.4648\n",
-      "Epoch 2/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 3.5925 - acc: 0.4883\n",
-      "Epoch 3/20\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 3.2800 - acc: 0.4941\n",
-      "Epoch 4/20\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 2.9631 - acc: 0.5020\n",
-      "Epoch 5/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 2.6506 - acc: 0.5137\n",
-      "Epoch 6/20\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 2.3760 - acc: 0.5293\n",
-      "Epoch 7/20\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 2.1348 - acc: 0.5469\n",
-      "Epoch 8/20\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 1.8958 - acc: 0.5586\n",
-      "Epoch 9/20\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 1.6638 - acc: 0.5684\n",
-      "Epoch 10/20\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 1.4815 - acc: 0.6133\n",
-      "Epoch 11/20\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 1.3476 - acc: 0.6289\n",
-      "Epoch 12/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 1.2700 - acc: 0.6367\n",
-      "Epoch 13/20\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.2265 - acc: 0.6641\n",
-      "Epoch 14/20\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 1.1924 - acc: 0.6504\n",
-      "Epoch 15/20\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 1.1571 - acc: 0.6602\n",
-      "Epoch 16/20\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 1.1302 - acc: 0.6543\n",
-      "Epoch 17/20\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 1.1063 - acc: 0.6660\n",
-      "Epoch 18/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 1.0881 - acc: 0.6699\n",
-      "Epoch 19/20\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.0675 - acc: 0.6738\n",
-      "Epoch 20/20\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.0410 - acc: 0.6660\n",
-      "256/256 [==============================] - 0s 584us/sample - loss: 0.8851 - acc: 0.6836\n",
-      "Epoch 1/20\n",
-      "512/512 [==============================] - 0s 379us/sample - loss: 62.1779 - acc: 0.3320\n",
-      "Epoch 2/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 56.5412 - acc: 0.3320\n",
-      "Epoch 3/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 50.9339 - acc: 0.3320\n",
-      "Epoch 4/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 45.5036 - acc: 0.3320\n",
-      "Epoch 5/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 40.1666 - acc: 0.3301\n",
-      "Epoch 6/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 34.7788 - acc: 0.3301\n",
-      "Epoch 7/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 29.4169 - acc: 0.3301\n",
-      "Epoch 8/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 24.1976 - acc: 0.3301\n",
-      "Epoch 9/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 18.8560 - acc: 0.3262\n",
-      "Epoch 10/20\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 13.9800 - acc: 0.3359\n",
-      "Epoch 11/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 9.9310 - acc: 0.3945\n",
-      "Epoch 12/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 7.3205 - acc: 0.4746\n",
-      "Epoch 13/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 5.7821 - acc: 0.5273\n",
-      "Epoch 14/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 4.8709 - acc: 0.5645\n",
-      "Epoch 15/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 4.4978 - acc: 0.6035\n",
-      "Epoch 16/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 4.2723 - acc: 0.6172\n",
-      "Epoch 17/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 4.0545 - acc: 0.6152\n",
-      "Epoch 18/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 3.8891 - acc: 0.6133\n",
-      "Epoch 19/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 3.7250 - acc: 0.6172\n",
-      "Epoch 20/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 3.6099 - acc: 0.6133\n",
-      "256/256 [==============================] - 0s 602us/sample - loss: 3.7517 - acc: 0.5859\n",
-      "Epoch 1/20\n",
-      "512/512 [==============================] - 0s 395us/sample - loss: 5.6183 - acc: 0.6211\n",
-      "Epoch 2/20\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 4.6766 - acc: 0.5918\n",
-      "Epoch 3/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 3.9083 - acc: 0.5840\n",
-      "Epoch 4/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 3.2536 - acc: 0.5684\n",
-      "Epoch 5/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 2.7091 - acc: 0.5566\n",
-      "Epoch 6/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 2.2743 - acc: 0.5352\n",
-      "Epoch 7/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 1.9757 - acc: 0.5254\n",
-      "Epoch 8/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 1.7653 - acc: 0.5352\n",
-      "Epoch 9/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 1.5965 - acc: 0.5527\n",
-      "Epoch 10/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 1.4451 - acc: 0.5547\n",
-      "Epoch 11/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 1.3361 - acc: 0.5684\n",
-      "Epoch 12/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 1.2514 - acc: 0.5840\n",
-      "Epoch 13/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 1.1764 - acc: 0.5840\n",
-      "Epoch 14/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 1.1201 - acc: 0.5742\n",
-      "Epoch 15/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 1.0645 - acc: 0.5801\n",
-      "Epoch 16/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 1.0178 - acc: 0.5938\n",
-      "Epoch 17/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 0.9777 - acc: 0.5938\n",
-      "Epoch 18/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 0.9445 - acc: 0.5938\n",
-      "Epoch 19/20\n",
-      "512/512 [==============================] - 0s 14us/sample - loss: 0.9170 - acc: 0.6016\n",
-      "Epoch 20/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 0.8863 - acc: 0.5996\n",
-      "256/256 [==============================] - 0s 674us/sample - loss: 0.9265 - acc: 0.5781\n",
-      "Epoch 1/20\n",
-      "512/512 [==============================] - 0s 407us/sample - loss: 12.6917 - acc: 0.6152\n",
-      "Epoch 2/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 10.2044 - acc: 0.5703\n",
-      "Epoch 3/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 8.4844 - acc: 0.4941\n",
-      "Epoch 4/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 7.8500 - acc: 0.4512\n",
-      "Epoch 5/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 7.5967 - acc: 0.4180\n",
-      "Epoch 6/20\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 7.2985 - acc: 0.4141\n",
-      "Epoch 7/20\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 6.9659 - acc: 0.4199\n",
-      "Epoch 8/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 6.6614 - acc: 0.4082\n",
-      "Epoch 9/20\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 6.4061 - acc: 0.4062\n",
-      "Epoch 10/20\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 6.1629 - acc: 0.4043\n",
-      "Epoch 11/20\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 5.9091 - acc: 0.3906\n",
-      "Epoch 12/20\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 5.6538 - acc: 0.3828\n",
-      "Epoch 13/20\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 5.4092 - acc: 0.3809\n",
-      "Epoch 14/20\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 5.1579 - acc: 0.3809\n",
-      "Epoch 15/20\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 4.9105 - acc: 0.3945\n",
-      "Epoch 16/20\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 4.6876 - acc: 0.3867\n",
-      "Epoch 17/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 4.4418 - acc: 0.3984\n",
-      "Epoch 18/20\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 4.2109 - acc: 0.4082\n",
-      "Epoch 19/20\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 3.9823 - acc: 0.4023\n",
-      "Epoch 20/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 3.7777 - acc: 0.4082\n",
-      "256/256 [==============================] - 0s 605us/sample - loss: 3.2759 - acc: 0.4492\n",
-      "Epoch 1/20\n",
-      "512/512 [==============================] - 0s 393us/sample - loss: 6.2852 - acc: 0.5938\n",
-      "Epoch 2/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 5.6469 - acc: 0.5938\n",
-      "Epoch 3/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 5.0258 - acc: 0.5977\n",
-      "Epoch 4/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 4.4447 - acc: 0.5918\n",
-      "Epoch 5/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 3.8873 - acc: 0.5801\n",
-      "Epoch 6/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 3.3620 - acc: 0.5801\n",
-      "Epoch 7/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 2.8443 - acc: 0.5879\n",
-      "Epoch 8/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 2.3259 - acc: 0.5840\n",
-      "Epoch 9/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 1.8459 - acc: 0.5723\n",
-      "Epoch 10/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 1.4317 - acc: 0.5645\n",
-      "Epoch 11/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 1.1985 - acc: 0.5508\n",
-      "Epoch 12/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 1.1470 - acc: 0.5586\n",
-      "Epoch 13/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 1.1277 - acc: 0.5664\n",
-      "Epoch 14/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 1.0886 - acc: 0.5566\n",
-      "Epoch 15/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 1.0419 - acc: 0.5664\n",
-      "Epoch 16/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 1.0212 - acc: 0.5898\n",
-      "Epoch 17/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 0.9974 - acc: 0.6016\n",
-      "Epoch 18/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 0.9661 - acc: 0.5918\n",
-      "Epoch 19/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 0.9653 - acc: 0.5664\n",
-      "Epoch 20/20\n",
-      "512/512 [==============================] - 0s 14us/sample - loss: 0.9326 - acc: 0.6016\n",
-      "256/256 [==============================] - 0s 631us/sample - loss: 1.1096 - acc: 0.5820\n",
-      "Epoch 1/20\n",
-      "512/512 [==============================] - 0s 420us/sample - loss: 89.1233 - acc: 0.6465\n",
-      "Epoch 2/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 84.2759 - acc: 0.6465\n",
-      "Epoch 3/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 79.6149 - acc: 0.6465\n",
-      "Epoch 4/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 74.9714 - acc: 0.6465\n",
-      "Epoch 5/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 70.6292 - acc: 0.6465\n",
-      "Epoch 6/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 66.3456 - acc: 0.6465\n",
-      "Epoch 7/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 62.2570 - acc: 0.6465\n",
-      "Epoch 8/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 58.2121 - acc: 0.6465\n",
-      "Epoch 9/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 54.3886 - acc: 0.6465\n",
-      "Epoch 10/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 50.4206 - acc: 0.6465\n",
-      "Epoch 11/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 46.8197 - acc: 0.6465\n",
-      "Epoch 12/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 43.0544 - acc: 0.6465\n",
-      "Epoch 13/20\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 39.4615 - acc: 0.6465\n",
-      "Epoch 14/20\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 35.8201 - acc: 0.6465\n",
-      "Epoch 15/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 32.2193 - acc: 0.6465\n",
-      "Epoch 16/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 28.8101 - acc: 0.6445\n",
-      "Epoch 17/20\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 25.4112 - acc: 0.6445\n",
-      "Epoch 18/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 21.9444 - acc: 0.6445\n",
-      "Epoch 19/20\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 18.4608 - acc: 0.6445\n",
-      "Epoch 20/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 15.4329 - acc: 0.6406\n",
-      "256/256 [==============================] - 0s 674us/sample - loss: 13.4161 - acc: 0.6328\n",
-      "Epoch 1/20\n",
-      "512/512 [==============================] - 0s 420us/sample - loss: 12.7121 - acc: 0.5723\n",
-      "Epoch 2/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 11.7563 - acc: 0.5762\n",
-      "Epoch 3/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 10.7671 - acc: 0.5820\n",
-      "Epoch 4/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 9.8497 - acc: 0.5840\n",
-      "Epoch 5/20\n",
-      "512/512 [==============================] - 0s 14us/sample - loss: 8.9775 - acc: 0.5801\n",
-      "Epoch 6/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 8.0967 - acc: 0.5801\n",
-      "Epoch 7/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 7.1615 - acc: 0.5723\n",
-      "Epoch 8/20\n",
-      "512/512 [==============================] - 0s 14us/sample - loss: 6.2291 - acc: 0.5684\n",
-      "Epoch 9/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 5.3146 - acc: 0.5703\n",
-      "Epoch 10/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 4.4273 - acc: 0.5410\n",
-      "Epoch 11/20\n",
-      "512/512 [==============================] - 0s 14us/sample - loss: 3.6744 - acc: 0.5156\n",
-      "Epoch 12/20\n",
-      "512/512 [==============================] - 0s 14us/sample - loss: 3.0898 - acc: 0.4473\n",
-      "Epoch 13/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 2.6743 - acc: 0.4180\n",
-      "Epoch 14/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 2.3861 - acc: 0.4297\n",
-      "Epoch 15/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 2.1355 - acc: 0.4551\n",
-      "Epoch 16/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 1.9303 - acc: 0.4570\n",
-      "Epoch 17/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 1.7551 - acc: 0.4980\n",
-      "Epoch 18/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 1.6265 - acc: 0.5293\n",
-      "Epoch 19/20\n",
-      "512/512 [==============================] - 0s 14us/sample - loss: 1.5158 - acc: 0.5352\n",
-      "Epoch 20/20\n",
-      "512/512 [==============================] - 0s 14us/sample - loss: 1.4232 - acc: 0.5586\n",
-      "256/256 [==============================] - 0s 652us/sample - loss: 1.3057 - acc: 0.5586\n",
-      "Epoch 1/20\n",
-      "768/768 [==============================] - 0s 365us/sample - loss: 4.6330 - acc: 0.6055\n",
-      "Epoch 2/20\n",
-      "768/768 [==============================] - 0s 85us/sample - loss: 2.2350 - acc: 0.5846\n",
-      "Epoch 3/20\n",
-      "768/768 [==============================] - 0s 81us/sample - loss: 1.4406 - acc: 0.6055\n",
-      "Epoch 4/20\n",
-      "768/768 [==============================] - 0s 81us/sample - loss: 1.0927 - acc: 0.5938\n",
-      "Epoch 5/20\n",
-      "768/768 [==============================] - 0s 80us/sample - loss: 0.8973 - acc: 0.6120\n",
-      "Epoch 6/20\n",
-      "768/768 [==============================] - 0s 79us/sample - loss: 0.8137 - acc: 0.6276\n",
-      "Epoch 7/20\n",
-      "768/768 [==============================] - 0s 82us/sample - loss: 0.7395 - acc: 0.6445\n",
-      "Epoch 8/20\n",
-      "768/768 [==============================] - 0s 85us/sample - loss: 0.7110 - acc: 0.6719\n",
-      "Epoch 9/20\n",
-      "768/768 [==============================] - 0s 88us/sample - loss: 0.6842 - acc: 0.6667\n",
-      "Epoch 10/20\n",
-      "768/768 [==============================] - 0s 86us/sample - loss: 0.6571 - acc: 0.6680\n",
-      "Epoch 11/20\n",
-      "768/768 [==============================] - 0s 97us/sample - loss: 0.6400 - acc: 0.6641\n",
-      "Epoch 12/20\n",
-      "768/768 [==============================] - 0s 84us/sample - loss: 0.6599 - acc: 0.6823\n",
-      "Epoch 13/20\n",
-      "768/768 [==============================] - 0s 83us/sample - loss: 0.6441 - acc: 0.6693\n",
-      "Epoch 14/20\n",
-      "768/768 [==============================] - 0s 81us/sample - loss: 0.6294 - acc: 0.6901\n",
-      "Epoch 15/20\n",
-      "768/768 [==============================] - 0s 83us/sample - loss: 0.6212 - acc: 0.6914\n",
-      "Epoch 16/20\n",
-      "768/768 [==============================] - 0s 83us/sample - loss: 0.6359 - acc: 0.6719\n",
-      "Epoch 17/20\n",
-      "768/768 [==============================] - 0s 87us/sample - loss: 0.6217 - acc: 0.6784\n",
-      "Epoch 18/20\n",
-      "768/768 [==============================] - 0s 88us/sample - loss: 0.6410 - acc: 0.6797\n",
-      "Epoch 19/20\n",
-      "768/768 [==============================] - 0s 103us/sample - loss: 0.6087 - acc: 0.6888\n",
-      "Epoch 20/20\n",
-      "768/768 [==============================] - 0s 85us/sample - loss: 0.6171 - acc: 0.6654\n",
-      "Best: 0.6666666666666666 using {'batch_size': 10, 'epochs': 20}\n",
-      "Means: 0.6666666666666666, Stdev: 0.023509726673525765 with: {'batch_size': 10, 'epochs': 20}\n",
-      "Means: 0.6354166666666666, Stdev: 0.023938510821419574 with: {'batch_size': 20, 'epochs': 20}\n",
-      "Means: 0.6015625, Stdev: 0.05132917537611204 with: {'batch_size': 40, 'epochs': 20}\n",
-      "Means: 0.5963541666666666, Stdev: 0.062200890169250976 with: {'batch_size': 60, 'epochs': 20}\n",
-      "Means: 0.5377604166666666, Stdev: 0.06268959956880585 with: {'batch_size': 80, 'epochs': 20}\n",
-      "Means: 0.5911458333333334, Stdev: 0.03097754493065187 with: {'batch_size': 100, 'epochs': 20}\n"
+      "Best: 0.6692708333333334 using {'batch_size': 10, 'epochs': 20}\n",
+      "Means: 0.6692708333333334, Stdev: 0.01813592223591682 with: {'batch_size': 10, 'epochs': 20}\n",
+      "Means: 0.6145833333333334, Stdev: 0.036966326996297885 with: {'batch_size': 20, 'epochs': 20}\n",
+      "Means: 0.5638020833333334, Stdev: 0.03097754493065187 with: {'batch_size': 40, 'epochs': 20}\n",
+      "Means: 0.5885416666666666, Stdev: 0.031304206458779446 with: {'batch_size': 60, 'epochs': 20}\n",
+      "Means: 0.5143229166666666, Stdev: 0.045591514062530196 with: {'batch_size': 80, 'epochs': 20}\n",
+      "Means: 0.5651041666666666, Stdev: 0.0411342942680503 with: {'batch_size': 100, 'epochs': 20}\n"
      ]
     }
    ],
@@ -1748,7 +1841,7 @@
     "    return model\n",
     "\n",
     "# create model\n",
-    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
+    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
     "\n",
     "# define the grid search parameters\n",
     "# batch_size = [10, 20, 40, 60, 80, 100]\n",
@@ -1785,7 +1878,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 21,
+   "execution_count": 13,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -1800,1598 +1893,22 @@
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "Epoch 1/20\n",
-      "512/512 [==============================] - 0s 470us/sample - loss: 49.4870 - acc: 0.3320\n",
-      "Epoch 2/20\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 44.9599 - acc: 0.3320\n",
-      "Epoch 3/20\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 40.4447 - acc: 0.3320\n",
-      "Epoch 4/20\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 36.0018 - acc: 0.3320\n",
-      "Epoch 5/20\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 31.4488 - acc: 0.3320\n",
-      "Epoch 6/20\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 26.8802 - acc: 0.3320\n",
-      "Epoch 7/20\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 22.3500 - acc: 0.3320\n",
-      "Epoch 8/20\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 17.7660 - acc: 0.3320\n",
-      "Epoch 9/20\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 12.9074 - acc: 0.3418\n",
-      "Epoch 10/20\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 9.2358 - acc: 0.4121\n",
-      "Epoch 11/20\n",
-      "512/512 [==============================] - 0s 30us/sample - loss: 7.0487 - acc: 0.4883\n",
-      "Epoch 12/20\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 5.7628 - acc: 0.5469\n",
-      "Epoch 13/20\n",
-      "512/512 [==============================] - 0s 34us/sample - loss: 5.2076 - acc: 0.5801\n",
-      "Epoch 14/20\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 4.6549 - acc: 0.5898\n",
-      "Epoch 15/20\n",
-      "512/512 [==============================] - 0s 33us/sample - loss: 4.0558 - acc: 0.5801\n",
-      "Epoch 16/20\n",
-      "512/512 [==============================] - 0s 35us/sample - loss: 3.4826 - acc: 0.5566\n",
-      "Epoch 17/20\n",
-      "512/512 [==============================] - 0s 33us/sample - loss: 2.8904 - acc: 0.5586\n",
-      "Epoch 18/20\n",
-      "512/512 [==============================] - 0s 34us/sample - loss: 2.3778 - acc: 0.5527\n",
-      "Epoch 19/20\n",
-      "512/512 [==============================] - 0s 33us/sample - loss: 1.9409 - acc: 0.5156\n",
-      "Epoch 20/20\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 1.6772 - acc: 0.5195\n",
-      "256/256 [==============================] - 0s 772us/sample - loss: 1.5568 - acc: 0.5352\n",
-      "Epoch 1/20\n",
-      "512/512 [==============================] - 0s 481us/sample - loss: 32.9027 - acc: 0.3613\n",
-      "Epoch 2/20\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 26.9860 - acc: 0.3594\n",
-      "Epoch 3/20\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 21.2236 - acc: 0.3555\n",
-      "Epoch 4/20\n",
-      "512/512 [==============================] - 0s 30us/sample - loss: 15.1370 - acc: 0.3574\n",
-      "Epoch 5/20\n",
-      "512/512 [==============================] - 0s 36us/sample - loss: 9.5931 - acc: 0.3750\n",
-      "Epoch 6/20\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 5.7732 - acc: 0.4219\n",
-      "Epoch 7/20\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 5.0780 - acc: 0.4902\n",
-      "Epoch 8/20\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 5.0070 - acc: 0.4941\n",
-      "Epoch 9/20\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 4.6202 - acc: 0.4922\n",
-      "Epoch 10/20\n",
-      "512/512 [==============================] - 0s 30us/sample - loss: 4.4111 - acc: 0.4551\n",
-      "Epoch 11/20\n",
-      "512/512 [==============================] - 0s 33us/sample - loss: 4.2825 - acc: 0.4395\n",
-      "Epoch 12/20\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 4.0982 - acc: 0.4531\n",
-      "Epoch 13/20\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 3.9432 - acc: 0.4590\n",
-      "Epoch 14/20\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 3.7890 - acc: 0.4668\n",
-      "Epoch 15/20\n",
-      "512/512 [==============================] - 0s 30us/sample - loss: 3.6390 - acc: 0.4512\n",
-      "Epoch 16/20\n",
-      "512/512 [==============================] - 0s 30us/sample - loss: 3.4619 - acc: 0.4531\n",
-      "Epoch 17/20\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 3.3139 - acc: 0.4551\n",
-      "Epoch 18/20\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 3.1698 - acc: 0.4531\n",
-      "Epoch 19/20\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 3.0151 - acc: 0.4531\n",
-      "Epoch 20/20\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 2.8778 - acc: 0.4453\n",
-      "256/256 [==============================] - 0s 684us/sample - loss: 2.6442 - acc: 0.4727\n",
-      "Epoch 1/20\n",
-      "512/512 [==============================] - 0s 475us/sample - loss: 34.1614 - acc: 0.6387\n",
-      "Epoch 2/20\n",
-      "512/512 [==============================] - 0s 30us/sample - loss: 30.7436 - acc: 0.6387\n",
-      "Epoch 3/20\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 27.5491 - acc: 0.6387\n",
-      "Epoch 4/20\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 24.2572 - acc: 0.6387\n",
-      "Epoch 5/20\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 20.9913 - acc: 0.6387\n",
-      "Epoch 6/20\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 17.6476 - acc: 0.6387\n",
-      "Epoch 7/20\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 14.3089 - acc: 0.6387\n",
-      "Epoch 8/20\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 10.9542 - acc: 0.6387\n",
-      "Epoch 9/20\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 7.3657 - acc: 0.6387\n",
-      "Epoch 10/20\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 3.9738 - acc: 0.6211\n",
-      "Epoch 11/20\n",
-      "512/512 [==============================] - 0s 30us/sample - loss: 2.3999 - acc: 0.5801\n",
-      "Epoch 12/20\n",
-      "512/512 [==============================] - 0s 33us/sample - loss: 2.4351 - acc: 0.5000\n",
-      "Epoch 13/20\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 1.9864 - acc: 0.5664\n",
-      "Epoch 14/20\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 1.8951 - acc: 0.5859\n",
-      "Epoch 15/20\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 1.7966 - acc: 0.5762\n",
-      "Epoch 16/20\n",
-      "512/512 [==============================] - 0s 35us/sample - loss: 1.7357 - acc: 0.5684\n",
-      "Epoch 17/20\n",
-      "512/512 [==============================] - 0s 35us/sample - loss: 1.6827 - acc: 0.5762\n",
-      "Epoch 18/20\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 1.6322 - acc: 0.5840\n",
-      "Epoch 19/20\n",
-      "512/512 [==============================] - 0s 34us/sample - loss: 1.5858 - acc: 0.6055\n",
-      "Epoch 20/20\n",
-      "512/512 [==============================] - 0s 33us/sample - loss: 1.5428 - acc: 0.6074\n",
-      "256/256 [==============================] - 0s 708us/sample - loss: 1.7096 - acc: 0.5391\n",
-      "Epoch 1/40\n",
-      "512/512 [==============================] - 0s 500us/sample - loss: 15.3882 - acc: 0.6699\n",
-      "Epoch 2/40\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 12.0160 - acc: 0.6602\n",
-      "Epoch 3/40\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 9.6975 - acc: 0.6465\n",
-      "Epoch 4/40\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 8.3426 - acc: 0.6133\n",
-      "Epoch 5/40\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 7.1740 - acc: 0.5977\n",
-      "Epoch 6/40\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 5.8879 - acc: 0.6074\n",
-      "Epoch 7/40\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 4.6352 - acc: 0.6191\n",
-      "Epoch 8/40\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 3.3032 - acc: 0.6113\n",
-      "Epoch 9/40\n",
-      "512/512 [==============================] - 0s 34us/sample - loss: 2.6030 - acc: 0.5684\n",
-      "Epoch 10/40\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 2.3890 - acc: 0.5820\n",
-      "Epoch 11/40\n",
-      "512/512 [==============================] - 0s 33us/sample - loss: 2.3115 - acc: 0.6094\n",
-      "Epoch 12/40\n",
-      "512/512 [==============================] - 0s 30us/sample - loss: 2.2427 - acc: 0.6074\n",
-      "Epoch 13/40\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 2.1503 - acc: 0.6055\n",
-      "Epoch 14/40\n",
-      "512/512 [==============================] - 0s 35us/sample - loss: 2.0802 - acc: 0.6035\n",
-      "Epoch 15/40\n",
-      "512/512 [==============================] - 0s 34us/sample - loss: 2.0302 - acc: 0.6035\n",
-      "Epoch 16/40\n",
-      "512/512 [==============================] - 0s 33us/sample - loss: 1.9652 - acc: 0.6094\n",
-      "Epoch 17/40\n",
-      "512/512 [==============================] - 0s 33us/sample - loss: 1.9160 - acc: 0.6133\n",
-      "Epoch 18/40\n",
-      "512/512 [==============================] - 0s 33us/sample - loss: 1.8549 - acc: 0.6133\n",
-      "Epoch 19/40\n",
-      "512/512 [==============================] - 0s 30us/sample - loss: 1.8036 - acc: 0.6152\n",
-      "Epoch 20/40\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 1.7731 - acc: 0.6035\n",
-      "Epoch 21/40\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 1.7098 - acc: 0.6074\n",
-      "Epoch 22/40\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 1.6720 - acc: 0.6348\n",
-      "Epoch 23/40\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 1.6341 - acc: 0.6113\n",
-      "Epoch 24/40\n",
-      "512/512 [==============================] - 0s 34us/sample - loss: 1.5817 - acc: 0.6289\n",
-      "Epoch 25/40\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 1.5456 - acc: 0.6152\n",
-      "Epoch 26/40\n",
-      "512/512 [==============================] - 0s 30us/sample - loss: 1.5035 - acc: 0.6172\n",
-      "Epoch 27/40\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 1.4701 - acc: 0.6055\n",
-      "Epoch 28/40\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 1.4212 - acc: 0.6289\n",
-      "Epoch 29/40\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 1.3982 - acc: 0.6250\n",
-      "Epoch 30/40\n",
-      "512/512 [==============================] - 0s 30us/sample - loss: 1.3630 - acc: 0.6172\n",
-      "Epoch 31/40\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 1.3428 - acc: 0.6152\n",
-      "Epoch 32/40\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 1.3045 - acc: 0.6211\n",
-      "Epoch 33/40\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 1.2860 - acc: 0.6289\n",
-      "Epoch 34/40\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 1.2580 - acc: 0.6152\n",
-      "Epoch 35/40\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 1.2312 - acc: 0.6113\n",
-      "Epoch 36/40\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 1.2208 - acc: 0.6172\n",
-      "Epoch 37/40\n",
-      "512/512 [==============================] - 0s 34us/sample - loss: 1.1898 - acc: 0.6172\n",
-      "Epoch 38/40\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 1.1658 - acc: 0.6270\n",
-      "Epoch 39/40\n",
-      "512/512 [==============================] - 0s 34us/sample - loss: 1.1727 - acc: 0.6406\n",
-      "Epoch 40/40\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 1.1753 - acc: 0.5859\n",
-      "256/256 [==============================] - 0s 769us/sample - loss: 1.0779 - acc: 0.6445\n",
-      "Epoch 1/40\n",
-      "512/512 [==============================] - 0s 515us/sample - loss: 18.4529 - acc: 0.6445\n",
-      "Epoch 2/40\n",
-      "512/512 [==============================] - 0s 30us/sample - loss: 13.3743 - acc: 0.6211\n",
-      "Epoch 3/40\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 9.0551 - acc: 0.5508\n",
-      "Epoch 4/40\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 6.4166 - acc: 0.4570\n",
-      "Epoch 5/40\n",
-      "512/512 [==============================] - 0s 34us/sample - loss: 5.1081 - acc: 0.4219\n",
-      "Epoch 6/40\n",
-      "512/512 [==============================] - 0s 37us/sample - loss: 4.3346 - acc: 0.4180\n",
-      "Epoch 7/40\n",
-      "512/512 [==============================] - 0s 36us/sample - loss: 3.4714 - acc: 0.4395\n",
-      "Epoch 8/40\n",
-      "512/512 [==============================] - 0s 37us/sample - loss: 2.7766 - acc: 0.4590\n",
-      "Epoch 9/40\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 2.4618 - acc: 0.5156\n",
-      "Epoch 10/40\n",
-      "512/512 [==============================] - 0s 36us/sample - loss: 2.2390 - acc: 0.5273\n",
-      "Epoch 11/40\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 2.1271 - acc: 0.5605\n",
-      "Epoch 12/40\n",
-      "512/512 [==============================] - 0s 38us/sample - loss: 2.0347 - acc: 0.5723\n",
-      "Epoch 13/40\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 1.9465 - acc: 0.5625\n",
-      "Epoch 14/40\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 1.8298 - acc: 0.5840\n",
-      "Epoch 15/40\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 1.7451 - acc: 0.5879\n",
-      "Epoch 16/40\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 1.6609 - acc: 0.5742\n",
-      "Epoch 17/40\n",
-      "512/512 [==============================] - 0s 34us/sample - loss: 1.5671 - acc: 0.5879\n",
-      "Epoch 18/40\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 1.5126 - acc: 0.5820\n",
-      "Epoch 19/40\n",
-      "512/512 [==============================] - 0s 30us/sample - loss: 1.4298 - acc: 0.6152\n",
-      "Epoch 20/40\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 1.3280 - acc: 0.6094\n",
-      "Epoch 21/40\n",
-      "512/512 [==============================] - 0s 34us/sample - loss: 1.2582 - acc: 0.6035\n",
-      "Epoch 22/40\n",
-      "512/512 [==============================] - 0s 33us/sample - loss: 1.2052 - acc: 0.6250\n",
-      "Epoch 23/40\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 1.1548 - acc: 0.6230\n",
-      "Epoch 24/40\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 1.0857 - acc: 0.6367\n",
-      "Epoch 25/40\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 1.0797 - acc: 0.6543\n",
-      "Epoch 26/40\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 1.0411 - acc: 0.6406\n",
-      "Epoch 27/40\n",
-      "512/512 [==============================] - 0s 34us/sample - loss: 0.9998 - acc: 0.6523\n",
-      "Epoch 28/40\n",
-      "512/512 [==============================] - 0s 34us/sample - loss: 0.9868 - acc: 0.6621\n",
-      "Epoch 29/40\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 0.9657 - acc: 0.6582\n",
-      "Epoch 30/40\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 0.9308 - acc: 0.6543\n",
-      "Epoch 31/40\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 0.9089 - acc: 0.6660\n",
-      "Epoch 32/40\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 0.8898 - acc: 0.6582\n",
-      "Epoch 33/40\n",
-      "512/512 [==============================] - 0s 33us/sample - loss: 0.8777 - acc: 0.6562\n",
-      "Epoch 34/40\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 0.8514 - acc: 0.6660\n",
-      "Epoch 35/40\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 0.8369 - acc: 0.6680\n",
-      "Epoch 36/40\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 0.8368 - acc: 0.6504\n",
-      "Epoch 37/40\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 0.8050 - acc: 0.6699\n",
-      "Epoch 38/40\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 0.7848 - acc: 0.6621\n",
-      "Epoch 39/40\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 0.7754 - acc: 0.6641\n",
-      "Epoch 40/40\n",
-      "512/512 [==============================] - 0s 24us/sample - loss: 0.7665 - acc: 0.6738\n",
-      "256/256 [==============================] - 0s 784us/sample - loss: 0.8207 - acc: 0.6328\n",
-      "Epoch 1/40\n",
-      "512/512 [==============================] - 0s 494us/sample - loss: 8.2352 - acc: 0.5977\n",
-      "Epoch 2/40\n",
-      "512/512 [==============================] - 0s 24us/sample - loss: 6.3014 - acc: 0.6074\n",
-      "Epoch 3/40\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 5.0591 - acc: 0.5898\n",
-      "Epoch 4/40\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 4.4479 - acc: 0.5801\n",
-      "Epoch 5/40\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 4.0863 - acc: 0.5742\n",
-      "Epoch 6/40\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 3.7824 - acc: 0.5879\n",
-      "Epoch 7/40\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 3.5751 - acc: 0.6055\n",
-      "Epoch 8/40\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 3.4354 - acc: 0.6016\n",
-      "Epoch 9/40\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 3.3031 - acc: 0.5898\n",
-      "Epoch 10/40\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 3.1745 - acc: 0.5898\n",
-      "Epoch 11/40\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 3.0600 - acc: 0.5918\n",
-      "Epoch 12/40\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 2.9378 - acc: 0.5879\n",
-      "Epoch 13/40\n",
-      "512/512 [==============================] - 0s 37us/sample - loss: 2.8123 - acc: 0.5898\n",
-      "Epoch 14/40\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 2.7064 - acc: 0.5859\n",
-      "Epoch 15/40\n",
-      "512/512 [==============================] - 0s 39us/sample - loss: 2.5897 - acc: 0.5879\n",
-      "Epoch 16/40\n",
-      "512/512 [==============================] - 0s 38us/sample - loss: 2.4802 - acc: 0.5918\n",
-      "Epoch 17/40\n",
-      "512/512 [==============================] - 0s 41us/sample - loss: 2.3956 - acc: 0.5996\n",
-      "Epoch 18/40\n",
-      "512/512 [==============================] - 0s 36us/sample - loss: 2.3312 - acc: 0.5996\n",
-      "Epoch 19/40\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 2.2180 - acc: 0.6074\n",
-      "Epoch 20/40\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 2.1470 - acc: 0.6016\n",
-      "Epoch 21/40\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 2.0695 - acc: 0.6074\n",
-      "Epoch 22/40\n",
-      "512/512 [==============================] - 0s 30us/sample - loss: 1.9920 - acc: 0.6133\n",
-      "Epoch 23/40\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 1.9398 - acc: 0.6035\n",
-      "Epoch 24/40\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 1.8772 - acc: 0.6094\n",
-      "Epoch 25/40\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 1.7994 - acc: 0.6094\n",
-      "Epoch 26/40\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 1.7436 - acc: 0.6035\n",
-      "Epoch 27/40\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 1.6840 - acc: 0.6191\n",
-      "Epoch 28/40\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 1.6787 - acc: 0.6016\n",
-      "Epoch 29/40\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 1.6214 - acc: 0.6270\n",
-      "Epoch 30/40\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 1.5763 - acc: 0.6074\n",
-      "Epoch 31/40\n",
-      "512/512 [==============================] - 0s 30us/sample - loss: 1.5017 - acc: 0.6289\n",
-      "Epoch 32/40\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 1.4577 - acc: 0.6289\n",
-      "Epoch 33/40\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 1.4190 - acc: 0.6152\n",
-      "Epoch 34/40\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 1.3724 - acc: 0.6191\n",
-      "Epoch 35/40\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 1.3531 - acc: 0.6211\n",
-      "Epoch 36/40\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 1.3074 - acc: 0.6211\n",
-      "Epoch 37/40\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 1.2627 - acc: 0.6172\n",
-      "Epoch 38/40\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 1.2219 - acc: 0.6172\n",
-      "Epoch 39/40\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 1.1772 - acc: 0.6113\n",
-      "Epoch 40/40\n",
-      "512/512 [==============================] - 0s 30us/sample - loss: 1.1443 - acc: 0.6172\n",
-      "256/256 [==============================] - 0s 838us/sample - loss: 0.9296 - acc: 0.7031\n",
-      "Epoch 1/60\n",
-      "512/512 [==============================] - 0s 522us/sample - loss: 31.5950 - acc: 0.6680\n",
-      "Epoch 2/60\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 27.1322 - acc: 0.6680\n",
-      "Epoch 3/60\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 22.5662 - acc: 0.6680\n",
-      "Epoch 4/60\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 18.3071 - acc: 0.6680\n",
-      "Epoch 5/60\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 13.7967 - acc: 0.6660\n",
-      "Epoch 6/60\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 9.5506 - acc: 0.6602\n",
-      "Epoch 7/60\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 6.9980 - acc: 0.5801\n",
-      "Epoch 8/60\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 5.8610 - acc: 0.5000\n",
-      "Epoch 9/60\n",
-      "512/512 [==============================] - 0s 30us/sample - loss: 5.4913 - acc: 0.4902\n",
-      "Epoch 10/60\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 5.0268 - acc: 0.4980\n",
-      "Epoch 11/60\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 4.5995 - acc: 0.5176\n",
-      "Epoch 12/60\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 4.2674 - acc: 0.5391\n",
-      "Epoch 13/60\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 3.9252 - acc: 0.5527\n",
-      "Epoch 14/60\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 3.6152 - acc: 0.5586\n",
-      "Epoch 15/60\n",
-      "512/512 [==============================] - 0s 24us/sample - loss: 3.3266 - acc: 0.5645\n",
-      "Epoch 16/60\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 3.0503 - acc: 0.5762\n",
-      "Epoch 17/60\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 2.8313 - acc: 0.5781\n",
-      "Epoch 18/60\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 2.6093 - acc: 0.5898\n",
-      "Epoch 19/60\n",
-      "512/512 [==============================] - 0s 24us/sample - loss: 2.4514 - acc: 0.5938\n",
-      "Epoch 20/60\n",
-      "512/512 [==============================] - 0s 24us/sample - loss: 2.3014 - acc: 0.6016\n",
-      "Epoch 21/60\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 2.1672 - acc: 0.6113\n",
-      "Epoch 22/60\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 2.0614 - acc: 0.6211\n",
-      "Epoch 23/60\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 1.9495 - acc: 0.6133\n",
-      "Epoch 24/60\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 1.8756 - acc: 0.6113\n",
-      "Epoch 25/60\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 1.7977 - acc: 0.6172\n",
-      "Epoch 26/60\n",
-      "512/512 [==============================] - 0s 24us/sample - loss: 1.7129 - acc: 0.6211\n",
-      "Epoch 27/60\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 1.6478 - acc: 0.6250\n",
-      "Epoch 28/60\n",
-      "512/512 [==============================] - 0s 24us/sample - loss: 1.5811 - acc: 0.6289\n",
-      "Epoch 29/60\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 1.5200 - acc: 0.6328\n",
-      "Epoch 30/60\n",
-      "512/512 [==============================] - 0s 24us/sample - loss: 1.4710 - acc: 0.6387\n",
-      "Epoch 31/60\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 1.4190 - acc: 0.6426\n",
-      "Epoch 32/60\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 1.3821 - acc: 0.6445\n",
-      "Epoch 33/60\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 1.3408 - acc: 0.6387\n",
-      "Epoch 34/60\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 1.3098 - acc: 0.6426\n",
-      "Epoch 35/60\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 1.2872 - acc: 0.6484\n",
-      "Epoch 36/60\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 1.2567 - acc: 0.6582\n",
-      "Epoch 37/60\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 1.2568 - acc: 0.6270\n",
-      "Epoch 38/60\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 1.2176 - acc: 0.6504\n",
-      "Epoch 39/60\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 1.1976 - acc: 0.6504\n",
-      "Epoch 40/60\n",
-      "512/512 [==============================] - 0s 30us/sample - loss: 1.1799 - acc: 0.6543\n",
-      "Epoch 41/60\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 1.1676 - acc: 0.6543\n",
-      "Epoch 42/60\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 1.1598 - acc: 0.6504\n",
-      "Epoch 43/60\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 1.1308 - acc: 0.6484\n",
-      "Epoch 44/60\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 1.1205 - acc: 0.6445\n",
-      "Epoch 45/60\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 1.1055 - acc: 0.6602\n",
-      "Epoch 46/60\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 1.0918 - acc: 0.6562\n",
-      "Epoch 47/60\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 1.0892 - acc: 0.6484\n",
-      "Epoch 48/60\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 1.0683 - acc: 0.6621\n",
-      "Epoch 49/60\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 1.0544 - acc: 0.6523\n",
-      "Epoch 50/60\n",
-      "512/512 [==============================] - 0s 31us/sample - loss: 1.0443 - acc: 0.6504\n",
-      "Epoch 51/60\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 1.0349 - acc: 0.6504\n",
-      "Epoch 52/60\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 1.0275 - acc: 0.6504\n",
-      "Epoch 53/60\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 1.0162 - acc: 0.6465\n",
-      "Epoch 54/60\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 1.0031 - acc: 0.6543\n",
-      "Epoch 55/60\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 1.0070 - acc: 0.6543\n",
-      "Epoch 56/60\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 1.0016 - acc: 0.6504\n",
-      "Epoch 57/60\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 0.9766 - acc: 0.6523\n",
-      "Epoch 58/60\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 0.9704 - acc: 0.6465\n",
-      "Epoch 59/60\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 0.9765 - acc: 0.6504\n",
-      "Epoch 60/60\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 0.9528 - acc: 0.6484\n",
-      "256/256 [==============================] - 0s 786us/sample - loss: 1.1179 - acc: 0.6172\n",
-      "Epoch 1/60\n",
-      "512/512 [==============================] - 0s 515us/sample - loss: 13.2762 - acc: 0.4785\n",
-      "Epoch 2/60\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 11.0430 - acc: 0.4922\n",
-      "Epoch 3/60\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 9.2324 - acc: 0.5273\n",
-      "Epoch 4/60\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 7.6667 - acc: 0.5410\n",
-      "Epoch 5/60\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 6.2774 - acc: 0.5586\n",
-      "Epoch 6/60\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 5.1285 - acc: 0.5781\n",
-      "Epoch 7/60\n",
-      "512/512 [==============================] - 0s 30us/sample - loss: 4.2771 - acc: 0.5820\n",
-      "Epoch 8/60\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 3.7149 - acc: 0.6133\n",
-      "Epoch 9/60\n",
-      "512/512 [==============================] - 0s 32us/sample - loss: 3.3620 - acc: 0.6211\n",
-      "Epoch 10/60\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 3.0786 - acc: 0.6211\n",
-      "Epoch 11/60\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 2.8617 - acc: 0.6211\n",
-      "Epoch 12/60\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 2.6957 - acc: 0.6113\n",
-      "Epoch 13/60\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 2.5708 - acc: 0.6133\n",
-      "Epoch 14/60\n",
-      "512/512 [==============================] - 0s 30us/sample - loss: 2.4584 - acc: 0.6172\n",
-      "Epoch 15/60\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 2.3579 - acc: 0.6172\n",
-      "Epoch 16/60\n",
-      "512/512 [==============================] - 0s 35us/sample - loss: 2.2631 - acc: 0.6133\n",
-      "Epoch 17/60\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 2.1868 - acc: 0.6191\n",
-      "Epoch 18/60\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 2.0927 - acc: 0.6191\n",
-      "Epoch 19/60\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 2.0190 - acc: 0.6172\n",
-      "Epoch 20/60\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 1.9510 - acc: 0.6230\n",
-      "Epoch 21/60\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 1.8772 - acc: 0.6250\n",
-      "Epoch 22/60\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 1.8204 - acc: 0.6191\n",
-      "Epoch 23/60\n",
-      "512/512 [==============================] - 0s 24us/sample - loss: 1.7448 - acc: 0.6211\n",
-      "Epoch 24/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 1.6830 - acc: 0.6172\n",
-      "Epoch 25/60\n",
-      "512/512 [==============================] - 0s 24us/sample - loss: 1.6327 - acc: 0.6230\n",
-      "Epoch 26/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 1.5709 - acc: 0.6230\n",
-      "Epoch 27/60\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 1.5127 - acc: 0.6270\n",
-      "Epoch 28/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 1.4715 - acc: 0.6250\n",
-      "Epoch 29/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 1.4326 - acc: 0.6172\n",
-      "Epoch 30/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 1.4044 - acc: 0.6211\n",
-      "Epoch 31/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 1.3486 - acc: 0.6230\n",
-      "Epoch 32/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 1.3174 - acc: 0.6172\n",
-      "Epoch 33/60\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 1.2799 - acc: 0.6152\n",
-      "Epoch 34/60\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 1.2548 - acc: 0.6230\n",
-      "Epoch 35/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 1.2278 - acc: 0.6133\n",
-      "Epoch 36/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 1.1972 - acc: 0.6172\n",
-      "Epoch 37/60\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 1.1725 - acc: 0.6250\n",
-      "Epoch 38/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 1.1483 - acc: 0.6211\n",
-      "Epoch 39/60\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 1.1247 - acc: 0.6328\n",
-      "Epoch 40/60\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 1.1043 - acc: 0.6367\n",
-      "Epoch 41/60\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 1.0824 - acc: 0.6270\n",
-      "Epoch 42/60\n",
-      "512/512 [==============================] - 0s 24us/sample - loss: 1.0634 - acc: 0.6270\n",
-      "Epoch 43/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 1.0416 - acc: 0.6309\n",
-      "Epoch 44/60\n",
-      "512/512 [==============================] - 0s 24us/sample - loss: 1.0243 - acc: 0.6250\n",
-      "Epoch 45/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 1.0030 - acc: 0.6367\n",
-      "Epoch 46/60\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 0.9938 - acc: 0.6387\n",
-      "Epoch 47/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 0.9696 - acc: 0.6348\n",
-      "Epoch 48/60\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 0.9746 - acc: 0.6543\n",
-      "Epoch 49/60\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 0.9399 - acc: 0.6367\n",
-      "Epoch 50/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 0.9275 - acc: 0.6328\n",
-      "Epoch 51/60\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 0.9222 - acc: 0.6445\n",
-      "Epoch 52/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 0.9022 - acc: 0.6348\n",
-      "Epoch 53/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 0.8931 - acc: 0.6523\n",
-      "Epoch 54/60\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 0.8823 - acc: 0.6484\n",
-      "Epoch 55/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 0.8666 - acc: 0.6406\n",
-      "Epoch 56/60\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 0.8650 - acc: 0.6543\n",
-      "Epoch 57/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 0.8470 - acc: 0.6562\n",
-      "Epoch 58/60\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 0.8412 - acc: 0.6543\n",
-      "Epoch 59/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 0.8302 - acc: 0.6543\n",
-      "Epoch 60/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 0.8404 - acc: 0.6504\n",
-      "256/256 [==============================] - 0s 767us/sample - loss: 0.9732 - acc: 0.6875\n",
-      "Epoch 1/60\n",
-      "512/512 [==============================] - 0s 514us/sample - loss: 39.9020 - acc: 0.6387\n",
-      "Epoch 2/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 34.9020 - acc: 0.6387\n",
-      "Epoch 3/60\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 30.1216 - acc: 0.6445\n",
-      "Epoch 4/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 25.6583 - acc: 0.6484\n",
-      "Epoch 5/60\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 22.0640 - acc: 0.6484\n",
-      "Epoch 6/60\n",
-      "512/512 [==============================] - 0s 24us/sample - loss: 19.4301 - acc: 0.6328\n",
-      "Epoch 7/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 17.5566 - acc: 0.5879\n",
-      "Epoch 8/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 15.6477 - acc: 0.5742\n",
-      "Epoch 9/60\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 13.7930 - acc: 0.5703\n",
-      "Epoch 10/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 12.0367 - acc: 0.5527\n",
-      "Epoch 11/60\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 10.2758 - acc: 0.5547\n",
-      "Epoch 12/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 8.6269 - acc: 0.5605\n",
-      "Epoch 13/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 6.9318 - acc: 0.5547\n",
-      "Epoch 14/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 5.4002 - acc: 0.5527\n",
-      "Epoch 15/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 3.9499 - acc: 0.5547\n",
-      "Epoch 16/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 2.6908 - acc: 0.5781\n",
-      "Epoch 17/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 1.7729 - acc: 0.6055\n",
-      "Epoch 18/60\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 1.4019 - acc: 0.6328\n",
-      "Epoch 19/60\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.3484 - acc: 0.6621\n",
-      "Epoch 20/60\n",
-      "512/512 [==============================] - 0s 24us/sample - loss: 1.3038 - acc: 0.6621\n",
-      "Epoch 21/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 1.2546 - acc: 0.6523\n",
-      "Epoch 22/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 1.1963 - acc: 0.6582\n",
-      "Epoch 23/60\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 1.1635 - acc: 0.6523\n",
-      "Epoch 24/60\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 1.1202 - acc: 0.6582\n",
-      "Epoch 25/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 1.0805 - acc: 0.6641\n",
-      "Epoch 26/60\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 1.0519 - acc: 0.6543\n",
-      "Epoch 27/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 1.0176 - acc: 0.6523\n",
-      "Epoch 28/60\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 0.9855 - acc: 0.6602\n",
-      "Epoch 29/60\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 0.9690 - acc: 0.6582\n",
-      "Epoch 30/60\n",
-      "512/512 [==============================] - 0s 30us/sample - loss: 0.9391 - acc: 0.6582\n",
-      "Epoch 31/60\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 0.9233 - acc: 0.6797\n",
-      "Epoch 32/60\n",
-      "512/512 [==============================] - 0s 30us/sample - loss: 0.9093 - acc: 0.6582\n",
-      "Epoch 33/60\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 0.8894 - acc: 0.6680\n",
-      "Epoch 34/60\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 0.8646 - acc: 0.6758\n",
-      "Epoch 35/60\n",
-      "512/512 [==============================] - 0s 27us/sample - loss: 0.8737 - acc: 0.6680\n",
-      "Epoch 36/60\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 0.8878 - acc: 0.6836\n",
-      "Epoch 37/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 0.8451 - acc: 0.6660\n",
-      "Epoch 38/60\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 0.8368 - acc: 0.6719\n",
-      "Epoch 39/60\n",
-      "512/512 [==============================] - 0s 24us/sample - loss: 0.8112 - acc: 0.6816\n",
-      "Epoch 40/60\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 0.8073 - acc: 0.6777\n",
-      "Epoch 41/60\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 0.7996 - acc: 0.6758\n",
-      "Epoch 42/60\n",
-      "512/512 [==============================] - 0s 24us/sample - loss: 0.7918 - acc: 0.6680\n",
-      "Epoch 43/60\n",
-      "512/512 [==============================] - 0s 24us/sample - loss: 0.7733 - acc: 0.6816\n",
-      "Epoch 44/60\n",
-      "512/512 [==============================] - 0s 29us/sample - loss: 0.7809 - acc: 0.6797\n",
-      "Epoch 45/60\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 0.7638 - acc: 0.6855\n",
-      "Epoch 46/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 0.7567 - acc: 0.6719\n",
-      "Epoch 47/60\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 0.7511 - acc: 0.6816\n",
-      "Epoch 48/60\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 0.7562 - acc: 0.6855\n",
-      "Epoch 49/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 0.7413 - acc: 0.6777\n",
-      "Epoch 50/60\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 0.7358 - acc: 0.6855\n",
-      "Epoch 51/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 0.7286 - acc: 0.6836\n",
-      "Epoch 52/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 0.7230 - acc: 0.6895\n",
-      "Epoch 53/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 0.7225 - acc: 0.6875\n",
-      "Epoch 54/60\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 0.7097 - acc: 0.6855\n",
-      "Epoch 55/60\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 0.7037 - acc: 0.6895\n",
-      "Epoch 56/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 0.7097 - acc: 0.6855\n",
-      "Epoch 57/60\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 0.6907 - acc: 0.6836\n",
-      "Epoch 58/60\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 0.6875 - acc: 0.6777\n",
-      "Epoch 59/60\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 0.6855 - acc: 0.6875\n",
-      "Epoch 60/60\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 0.6947 - acc: 0.6836\n",
-      "256/256 [==============================] - 0s 804us/sample - loss: 0.6976 - acc: 0.6992\n",
-      "Epoch 1/20\n",
-      "512/512 [==============================] - 0s 502us/sample - loss: 28.7079 - acc: 0.3320\n",
-      "Epoch 2/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 26.1592 - acc: 0.3320\n",
-      "Epoch 3/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 23.6050 - acc: 0.3320\n",
-      "Epoch 4/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 21.0595 - acc: 0.3320\n",
-      "Epoch 5/20\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 18.3299 - acc: 0.3340\n",
-      "Epoch 6/20\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 15.4965 - acc: 0.3340\n",
-      "Epoch 7/20\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 12.3212 - acc: 0.3340\n",
-      "Epoch 8/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 8.9116 - acc: 0.3379\n",
-      "Epoch 9/20\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 5.7163 - acc: 0.3477\n",
-      "Epoch 10/20\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 3.9419 - acc: 0.4062\n",
-      "Epoch 11/20\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 3.5115 - acc: 0.4785\n",
-      "Epoch 12/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 3.3837 - acc: 0.5059\n",
-      "Epoch 13/20\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 3.1619 - acc: 0.5195\n",
-      "Epoch 14/20\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 2.8666 - acc: 0.5137\n",
-      "Epoch 15/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 2.5894 - acc: 0.4941\n",
-      "Epoch 16/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 2.3568 - acc: 0.4941\n",
-      "Epoch 17/20\n",
-      "512/512 [==============================] - 0s 15us/sample - loss: 2.2386 - acc: 0.4961\n",
-      "Epoch 18/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 2.0862 - acc: 0.5020\n",
-      "Epoch 19/20\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 1.9964 - acc: 0.5059\n",
-      "Epoch 20/20\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.9428 - acc: 0.5312\n",
-      "256/256 [==============================] - 0s 845us/sample - loss: 2.3181 - acc: 0.4961\n",
-      "Epoch 1/20\n",
-      "512/512 [==============================] - 0s 503us/sample - loss: 16.0385 - acc: 0.4961\n",
-      "Epoch 2/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 14.0327 - acc: 0.5078\n",
-      "Epoch 3/20\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 12.1456 - acc: 0.5234\n",
-      "Epoch 4/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 10.4195 - acc: 0.5586\n",
-      "Epoch 5/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 8.9802 - acc: 0.5957\n",
-      "Epoch 6/20\n",
-      "512/512 [==============================] - 0s 24us/sample - loss: 7.6475 - acc: 0.6055\n",
-      "Epoch 7/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 6.4785 - acc: 0.6113\n",
-      "Epoch 8/20\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 5.2042 - acc: 0.6113\n",
-      "Epoch 9/20\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 3.9308 - acc: 0.6074\n",
-      "Epoch 10/20\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 2.8226 - acc: 0.5957\n",
-      "Epoch 11/20\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 2.1677 - acc: 0.6113\n",
-      "Epoch 12/20\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 2.1074 - acc: 0.5840\n",
-      "Epoch 13/20\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 2.0073 - acc: 0.6172\n",
-      "Epoch 14/20\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.8985 - acc: 0.6484\n",
-      "Epoch 15/20\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 1.8596 - acc: 0.6504\n",
-      "Epoch 16/20\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 1.8089 - acc: 0.6504\n",
-      "Epoch 17/20\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.7639 - acc: 0.6348\n",
-      "Epoch 18/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 1.7285 - acc: 0.6406\n",
-      "Epoch 19/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 1.6687 - acc: 0.6445\n",
-      "Epoch 20/20\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 1.5919 - acc: 0.6504\n",
-      "256/256 [==============================] - 0s 834us/sample - loss: 1.8234 - acc: 0.6875\n",
-      "Epoch 1/20\n",
-      "512/512 [==============================] - 0s 518us/sample - loss: 6.8758 - acc: 0.4082\n",
-      "Epoch 2/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 4.9696 - acc: 0.4512\n",
-      "Epoch 3/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 4.1253 - acc: 0.5176\n",
-      "Epoch 4/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 4.0199 - acc: 0.5449\n",
-      "Epoch 5/20\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 3.9440 - acc: 0.5469\n",
-      "Epoch 6/20\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 3.7426 - acc: 0.5469\n",
-      "Epoch 7/20\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 3.5838 - acc: 0.5352\n",
-      "Epoch 8/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 3.4395 - acc: 0.5254\n",
-      "Epoch 9/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 3.3078 - acc: 0.5293\n",
-      "Epoch 10/20\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 3.1765 - acc: 0.5332\n",
-      "Epoch 11/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 3.0597 - acc: 0.5430\n",
-      "Epoch 12/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 2.9442 - acc: 0.5410\n",
-      "Epoch 13/20\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 2.8222 - acc: 0.5449\n",
-      "Epoch 14/20\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 2.7238 - acc: 0.5527\n",
-      "Epoch 15/20\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 2.6094 - acc: 0.5547\n",
-      "Epoch 16/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 2.4919 - acc: 0.5625\n",
-      "Epoch 17/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 2.3891 - acc: 0.5625\n",
-      "Epoch 18/20\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 2.2993 - acc: 0.5762\n",
-      "Epoch 19/20\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 2.2100 - acc: 0.5820\n",
-      "Epoch 20/20\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 2.1390 - acc: 0.5801\n",
-      "256/256 [==============================] - 0s 854us/sample - loss: 2.3454 - acc: 0.5234\n",
-      "Epoch 1/40\n",
-      "512/512 [==============================] - 0s 517us/sample - loss: 9.2426 - acc: 0.3320\n",
-      "Epoch 2/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 6.6331 - acc: 0.3281\n",
-      "Epoch 3/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 4.2222 - acc: 0.3496\n",
-      "Epoch 4/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 2.3282 - acc: 0.4414\n",
-      "Epoch 5/40\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 1.7429 - acc: 0.5156\n",
-      "Epoch 6/40\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 1.7138 - acc: 0.5742\n",
-      "Epoch 7/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.5644 - acc: 0.5645\n",
-      "Epoch 8/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.3880 - acc: 0.5566\n",
-      "Epoch 9/40\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 1.3267 - acc: 0.5566\n",
-      "Epoch 10/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.3062 - acc: 0.5645\n",
-      "Epoch 11/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 1.2772 - acc: 0.5605\n",
-      "Epoch 12/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.2518 - acc: 0.5684\n",
-      "Epoch 13/40\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 1.2254 - acc: 0.5664\n",
-      "Epoch 14/40\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 1.1990 - acc: 0.5703\n",
-      "Epoch 15/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.1712 - acc: 0.5684\n",
-      "Epoch 16/40\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 1.1442 - acc: 0.5684\n",
-      "Epoch 17/40\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 1.1163 - acc: 0.5723\n",
-      "Epoch 18/40\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 1.0980 - acc: 0.5742\n",
-      "Epoch 19/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.0794 - acc: 0.5820\n",
-      "Epoch 20/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 1.0494 - acc: 0.5898\n",
-      "Epoch 21/40\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 1.0367 - acc: 0.5938\n",
-      "Epoch 22/40\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 1.0149 - acc: 0.6055\n",
-      "Epoch 23/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 0.9993 - acc: 0.5977\n",
-      "Epoch 24/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.9874 - acc: 0.6035\n",
-      "Epoch 25/40\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 0.9651 - acc: 0.6074\n",
-      "Epoch 26/40\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 0.9509 - acc: 0.6074\n",
-      "Epoch 27/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.9384 - acc: 0.6094\n",
-      "Epoch 28/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.9279 - acc: 0.6133\n",
-      "Epoch 29/40\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 0.9134 - acc: 0.6113\n",
-      "Epoch 30/40\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 0.9031 - acc: 0.6133\n",
-      "Epoch 31/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 0.8933 - acc: 0.6211\n",
-      "Epoch 32/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.8834 - acc: 0.6211\n",
-      "Epoch 33/40\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 0.8756 - acc: 0.6172\n",
-      "Epoch 34/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.8661 - acc: 0.6152\n",
-      "Epoch 35/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.8583 - acc: 0.6211\n",
-      "Epoch 36/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.8501 - acc: 0.6250\n",
-      "Epoch 37/40\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 0.8423 - acc: 0.6270\n",
-      "Epoch 38/40\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 0.8365 - acc: 0.6367\n",
-      "Epoch 39/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 0.8262 - acc: 0.6348\n",
-      "Epoch 40/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 0.8219 - acc: 0.6309\n",
-      "256/256 [==============================] - 0s 846us/sample - loss: 1.1425 - acc: 0.5547\n",
-      "Epoch 1/40\n",
-      "512/512 [==============================] - 0s 523us/sample - loss: 18.3446 - acc: 0.3535\n",
-      "Epoch 2/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 14.1918 - acc: 0.3633\n",
-      "Epoch 3/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 10.8471 - acc: 0.3867\n",
-      "Epoch 4/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 8.5810 - acc: 0.4219\n",
-      "Epoch 5/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 7.0979 - acc: 0.4492\n",
-      "Epoch 6/40\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 5.7754 - acc: 0.4551\n",
-      "Epoch 7/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 4.5010 - acc: 0.4727\n",
-      "Epoch 8/40\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 3.3476 - acc: 0.4863\n",
-      "Epoch 9/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 2.5703 - acc: 0.5371\n",
-      "Epoch 10/40\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 2.1195 - acc: 0.5723\n",
-      "Epoch 11/40\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 1.9772 - acc: 0.5703\n",
-      "Epoch 12/40\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 1.9409 - acc: 0.5742\n",
-      "Epoch 13/40\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 1.9088 - acc: 0.5859\n",
-      "Epoch 14/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.8711 - acc: 0.5996\n",
-      "Epoch 15/40\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 1.8354 - acc: 0.5996\n",
-      "Epoch 16/40\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 1.8006 - acc: 0.6074\n",
-      "Epoch 17/40\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 1.7671 - acc: 0.6035\n",
-      "Epoch 18/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.7353 - acc: 0.5996\n",
-      "Epoch 19/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 1.7051 - acc: 0.5879\n",
-      "Epoch 20/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 1.6717 - acc: 0.5918\n",
-      "Epoch 21/40\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 1.6437 - acc: 0.5918\n",
-      "Epoch 22/40\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 1.6236 - acc: 0.5879\n",
-      "Epoch 23/40\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 1.5875 - acc: 0.5918\n",
-      "Epoch 24/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.5678 - acc: 0.5957\n",
-      "Epoch 25/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 1.5333 - acc: 0.5898\n",
-      "Epoch 26/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 1.5223 - acc: 0.5938\n",
-      "Epoch 27/40\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 1.4956 - acc: 0.5977\n",
-      "Epoch 28/40\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 1.4671 - acc: 0.5977\n",
-      "Epoch 29/40\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 1.4468 - acc: 0.5996\n",
-      "Epoch 30/40\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 1.4299 - acc: 0.5996\n",
-      "Epoch 31/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 1.4086 - acc: 0.6016\n",
-      "Epoch 32/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.3857 - acc: 0.6016\n",
-      "Epoch 33/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.3603 - acc: 0.6094\n",
-      "Epoch 34/40\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 1.3405 - acc: 0.6055\n",
-      "Epoch 35/40\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 1.3203 - acc: 0.6055\n",
-      "Epoch 36/40\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 1.2886 - acc: 0.6270\n",
-      "Epoch 37/40\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 1.2767 - acc: 0.6289\n",
-      "Epoch 38/40\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 1.2514 - acc: 0.6250\n",
-      "Epoch 39/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 1.2282 - acc: 0.6211\n",
-      "Epoch 40/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.2071 - acc: 0.6309\n",
-      "256/256 [==============================] - 0s 855us/sample - loss: 1.4880 - acc: 0.6172\n",
-      "Epoch 1/40\n",
-      "512/512 [==============================] - 0s 552us/sample - loss: 6.1297 - acc: 0.6387\n",
-      "Epoch 2/40\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 4.6791 - acc: 0.6289\n",
-      "Epoch 3/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 3.4004 - acc: 0.6250\n",
-      "Epoch 4/40\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 2.4991 - acc: 0.6348\n",
-      "Epoch 5/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.8149 - acc: 0.6074\n",
-      "Epoch 6/40\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 1.5251 - acc: 0.5957\n",
-      "Epoch 7/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.3509 - acc: 0.5977\n",
-      "Epoch 8/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.2062 - acc: 0.6250\n",
-      "Epoch 9/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 1.1471 - acc: 0.6328\n",
-      "Epoch 10/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.1240 - acc: 0.6309\n",
-      "Epoch 11/40\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 1.0854 - acc: 0.6387\n",
-      "Epoch 12/40\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 1.0465 - acc: 0.6523\n",
-      "Epoch 13/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 1.0140 - acc: 0.6406\n",
-      "Epoch 14/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.9868 - acc: 0.6465\n",
-      "Epoch 15/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.9647 - acc: 0.6426\n",
-      "Epoch 16/40\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 0.9390 - acc: 0.6504\n",
-      "Epoch 17/40\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 0.9222 - acc: 0.6465\n",
-      "Epoch 18/40\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 0.8987 - acc: 0.6582\n",
-      "Epoch 19/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 0.8846 - acc: 0.6641\n",
-      "Epoch 20/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 0.8690 - acc: 0.6543\n",
-      "Epoch 21/40\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 0.8524 - acc: 0.6602\n",
-      "Epoch 22/40\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 0.8377 - acc: 0.6484\n",
-      "Epoch 23/40\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 0.8257 - acc: 0.6562\n",
-      "Epoch 24/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.8136 - acc: 0.6582\n",
-      "Epoch 25/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.7996 - acc: 0.6621\n",
-      "Epoch 26/40\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 0.7917 - acc: 0.6582\n",
-      "Epoch 27/40\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 0.7833 - acc: 0.6660\n",
-      "Epoch 28/40\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 0.7795 - acc: 0.6621\n",
-      "Epoch 29/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 0.7670 - acc: 0.6680\n",
-      "Epoch 30/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.7638 - acc: 0.6621\n",
-      "Epoch 31/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 0.7602 - acc: 0.6641\n",
-      "Epoch 32/40\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 0.7494 - acc: 0.6680\n",
-      "Epoch 33/40\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 0.7505 - acc: 0.6719\n",
-      "Epoch 34/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.7531 - acc: 0.6543\n",
-      "Epoch 35/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.7360 - acc: 0.6660\n",
-      "Epoch 36/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 0.7442 - acc: 0.6855\n",
-      "Epoch 37/40\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 0.7304 - acc: 0.6738\n",
-      "Epoch 38/40\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 0.7330 - acc: 0.6523\n",
-      "Epoch 39/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.7223 - acc: 0.6738\n",
-      "Epoch 40/40\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.7197 - acc: 0.6855\n",
-      "256/256 [==============================] - 0s 853us/sample - loss: 0.7378 - acc: 0.6484\n",
-      "Epoch 1/60\n",
-      "512/512 [==============================] - 0s 542us/sample - loss: 13.1027 - acc: 0.5273\n",
-      "Epoch 2/60\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 11.9761 - acc: 0.5352\n",
-      "Epoch 3/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 10.9983 - acc: 0.5352\n",
-      "Epoch 4/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 9.9223 - acc: 0.5449\n",
-      "Epoch 5/60\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 8.9456 - acc: 0.5469\n",
-      "Epoch 6/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 8.0193 - acc: 0.5762\n",
-      "Epoch 7/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 7.1871 - acc: 0.5859\n",
-      "Epoch 8/60\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 6.3827 - acc: 0.5820\n",
-      "Epoch 9/60\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 5.5435 - acc: 0.5859\n",
-      "Epoch 10/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 4.7948 - acc: 0.5742\n",
-      "Epoch 11/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 4.0109 - acc: 0.5762\n",
-      "Epoch 12/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 3.3006 - acc: 0.5781\n",
-      "Epoch 13/60\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 2.5979 - acc: 0.5742\n",
-      "Epoch 14/60\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.9929 - acc: 0.5547\n",
-      "Epoch 15/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 1.4830 - acc: 0.5645\n",
-      "Epoch 16/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 1.1838 - acc: 0.5957\n",
-      "Epoch 17/60\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 1.0711 - acc: 0.6133\n",
-      "Epoch 18/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 1.0415 - acc: 0.6387\n",
-      "Epoch 19/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 0.9832 - acc: 0.6328\n",
-      "Epoch 20/60\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 0.9246 - acc: 0.6387\n",
-      "Epoch 21/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.8728 - acc: 0.6602\n",
-      "Epoch 22/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.8355 - acc: 0.6543\n",
-      "Epoch 23/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.8056 - acc: 0.6895\n",
-      "Epoch 24/60\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 0.7829 - acc: 0.6641\n",
-      "Epoch 25/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 0.7460 - acc: 0.6738\n",
-      "Epoch 26/60\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 0.7318 - acc: 0.6699\n",
-      "Epoch 27/60\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 0.7074 - acc: 0.6836\n",
-      "Epoch 28/60\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 0.6961 - acc: 0.6973\n",
-      "Epoch 29/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.6903 - acc: 0.6953\n",
-      "Epoch 30/60\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 0.6786 - acc: 0.7031\n",
-      "Epoch 31/60\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 0.6548 - acc: 0.7051\n",
-      "Epoch 32/60\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 0.6425 - acc: 0.7129\n",
-      "Epoch 33/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 0.6355 - acc: 0.7012\n",
-      "Epoch 34/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.6287 - acc: 0.6953\n",
-      "Epoch 35/60\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 0.6284 - acc: 0.7051\n",
-      "Epoch 36/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 0.6214 - acc: 0.6973\n",
-      "Epoch 37/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.6125 - acc: 0.7051\n",
-      "Epoch 38/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.6077 - acc: 0.7090\n",
-      "Epoch 39/60\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 0.6065 - acc: 0.7129\n",
-      "Epoch 40/60\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 0.6126 - acc: 0.7031\n",
-      "Epoch 41/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 0.6029 - acc: 0.7246\n",
-      "Epoch 42/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 0.6001 - acc: 0.7188\n",
-      "Epoch 43/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.5971 - acc: 0.7090\n",
-      "Epoch 44/60\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 0.6114 - acc: 0.7129\n",
-      "Epoch 45/60\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 0.6078 - acc: 0.7090\n",
-      "Epoch 46/60\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 0.6079 - acc: 0.7168\n",
-      "Epoch 47/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 0.6018 - acc: 0.7090\n",
-      "Epoch 48/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 0.5969 - acc: 0.7168\n",
-      "Epoch 49/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 0.5870 - acc: 0.7168\n",
-      "Epoch 50/60\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 0.5781 - acc: 0.7344\n",
-      "Epoch 51/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 0.5863 - acc: 0.7207\n",
-      "Epoch 52/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 0.5911 - acc: 0.7090\n",
-      "Epoch 53/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 0.5814 - acc: 0.7109\n",
-      "Epoch 54/60\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 0.5877 - acc: 0.7070\n",
-      "Epoch 55/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 0.5928 - acc: 0.7109\n",
-      "Epoch 56/60\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 0.5740 - acc: 0.7188\n",
-      "Epoch 57/60\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 0.5703 - acc: 0.7266\n",
-      "Epoch 58/60\n",
-      "512/512 [==============================] - 0s 24us/sample - loss: 0.5916 - acc: 0.7129\n",
-      "Epoch 59/60\n",
-      "512/512 [==============================] - 0s 24us/sample - loss: 0.5762 - acc: 0.7344\n",
-      "Epoch 60/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 0.5675 - acc: 0.7246\n",
-      "256/256 [==============================] - 0s 855us/sample - loss: 0.6951 - acc: 0.6719\n",
-      "Epoch 1/60\n",
-      "512/512 [==============================] - 0s 554us/sample - loss: 8.5126 - acc: 0.5898\n",
-      "Epoch 2/60\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 6.8310 - acc: 0.5566\n",
-      "Epoch 3/60\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 5.7500 - acc: 0.5195\n",
-      "Epoch 4/60\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 4.8866 - acc: 0.5078\n",
-      "Epoch 5/60\n",
-      "512/512 [==============================] - 0s 28us/sample - loss: 4.3596 - acc: 0.5176\n",
-      "Epoch 6/60\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 4.0117 - acc: 0.5215\n",
-      "Epoch 7/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 3.8097 - acc: 0.5332\n",
-      "Epoch 8/60\n",
-      "512/512 [==============================] - 0s 24us/sample - loss: 3.6082 - acc: 0.5410\n",
-      "Epoch 9/60\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 3.4139 - acc: 0.5469\n",
-      "Epoch 10/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 3.2336 - acc: 0.5547\n",
-      "Epoch 11/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 3.0835 - acc: 0.5625\n",
-      "Epoch 12/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 2.9445 - acc: 0.5684\n",
-      "Epoch 13/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 2.8303 - acc: 0.5547\n",
-      "Epoch 14/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 2.6901 - acc: 0.5625\n",
-      "Epoch 15/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 2.5765 - acc: 0.5801\n",
-      "Epoch 16/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 2.4541 - acc: 0.5820\n",
-      "Epoch 17/60\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 2.3573 - acc: 0.5742\n",
-      "Epoch 18/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 2.2347 - acc: 0.5820\n",
-      "Epoch 19/60\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 2.1278 - acc: 0.5762\n",
-      "Epoch 20/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 2.0275 - acc: 0.5762\n",
-      "Epoch 21/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 1.9027 - acc: 0.5781\n",
-      "Epoch 22/60\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 1.8073 - acc: 0.5781\n",
-      "Epoch 23/60\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 1.7083 - acc: 0.5957\n",
-      "Epoch 24/60\n",
-      "512/512 [==============================] - 0s 26us/sample - loss: 1.6193 - acc: 0.5957\n",
-      "Epoch 25/60\n",
-      "512/512 [==============================] - 0s 25us/sample - loss: 1.5588 - acc: 0.5801\n",
-      "Epoch 26/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 1.4799 - acc: 0.5938\n",
-      "Epoch 27/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 1.4179 - acc: 0.5938\n",
-      "Epoch 28/60\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.3642 - acc: 0.5898\n",
-      "Epoch 29/60\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.3068 - acc: 0.5898\n",
-      "Epoch 30/60\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.2699 - acc: 0.6016\n",
-      "Epoch 31/60\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 1.2181 - acc: 0.5879\n",
-      "Epoch 32/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 1.1823 - acc: 0.5957\n",
-      "Epoch 33/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 1.1374 - acc: 0.6055\n",
-      "Epoch 34/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 1.1096 - acc: 0.6133\n",
-      "Epoch 35/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 1.0646 - acc: 0.6016\n",
-      "Epoch 36/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 1.0375 - acc: 0.6055\n",
-      "Epoch 37/60\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 1.0232 - acc: 0.6035\n",
-      "Epoch 38/60\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 1.0063 - acc: 0.6230\n",
-      "Epoch 39/60\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 1.0003 - acc: 0.5938\n",
-      "Epoch 40/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 1.0107 - acc: 0.6465\n",
-      "Epoch 41/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 0.9760 - acc: 0.5996\n",
-      "Epoch 42/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 0.9652 - acc: 0.6387\n",
-      "Epoch 43/60\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 0.9488 - acc: 0.6094\n",
-      "Epoch 44/60\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 0.9310 - acc: 0.6289\n",
-      "Epoch 45/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.9399 - acc: 0.6309\n",
-      "Epoch 46/60\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 0.9211 - acc: 0.6211\n",
-      "Epoch 47/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.9016 - acc: 0.6289\n",
-      "Epoch 48/60\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 0.9199 - acc: 0.5938\n",
-      "Epoch 49/60\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 0.9767 - acc: 0.6445\n",
-      "Epoch 50/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 0.9136 - acc: 0.6133\n",
-      "Epoch 51/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 0.8759 - acc: 0.6641\n",
-      "Epoch 52/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 0.8894 - acc: 0.6426\n",
-      "Epoch 53/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 0.8488 - acc: 0.6426\n",
-      "Epoch 54/60\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 0.8534 - acc: 0.6621\n",
-      "Epoch 55/60\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 0.8471 - acc: 0.6699\n",
-      "Epoch 56/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.8334 - acc: 0.6465\n",
-      "Epoch 57/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.8338 - acc: 0.6484\n",
-      "Epoch 58/60\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 0.8264 - acc: 0.6504\n",
-      "Epoch 59/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 0.8219 - acc: 0.6680\n",
-      "Epoch 60/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 0.8161 - acc: 0.6641\n",
-      "256/256 [==============================] - 0s 926us/sample - loss: 0.8679 - acc: 0.6562\n",
-      "Epoch 1/60\n",
-      "512/512 [==============================] - 0s 556us/sample - loss: 8.7710 - acc: 0.5098\n",
-      "Epoch 2/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 7.8195 - acc: 0.5391\n",
-      "Epoch 3/60\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 7.0712 - acc: 0.5762\n",
-      "Epoch 4/60\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 6.3780 - acc: 0.5781\n",
-      "Epoch 5/60\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 5.7297 - acc: 0.5684\n",
-      "Epoch 6/60\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 5.1831 - acc: 0.5605\n",
-      "Epoch 7/60\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 4.7841 - acc: 0.5527\n",
-      "Epoch 8/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 4.4465 - acc: 0.5430\n",
-      "Epoch 9/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 4.1612 - acc: 0.5488\n",
-      "Epoch 10/60\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 3.9449 - acc: 0.5410\n",
-      "Epoch 11/60\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 3.7557 - acc: 0.5332\n",
-      "Epoch 12/60\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 3.5849 - acc: 0.5273\n",
-      "Epoch 13/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 3.4264 - acc: 0.5215\n",
-      "Epoch 14/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 3.2695 - acc: 0.5195\n",
-      "Epoch 15/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 3.1482 - acc: 0.5273\n",
-      "Epoch 16/60\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 2.9818 - acc: 0.5371\n",
-      "Epoch 17/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 2.8375 - acc: 0.5312\n",
-      "Epoch 18/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 2.7011 - acc: 0.5332\n",
-      "Epoch 19/60\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 2.5673 - acc: 0.5430\n",
-      "Epoch 20/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 2.4370 - acc: 0.5527\n",
-      "Epoch 21/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 2.3202 - acc: 0.5645\n",
-      "Epoch 22/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 2.1882 - acc: 0.5586\n",
-      "Epoch 23/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 2.0833 - acc: 0.5527\n",
-      "Epoch 24/60\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.9743 - acc: 0.5684\n",
-      "Epoch 25/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 1.8649 - acc: 0.5723\n",
-      "Epoch 26/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 1.7700 - acc: 0.5664\n",
-      "Epoch 27/60\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.6674 - acc: 0.5703\n",
-      "Epoch 28/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 1.5731 - acc: 0.5762\n",
-      "Epoch 29/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 1.4816 - acc: 0.5684\n",
-      "Epoch 30/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 1.3931 - acc: 0.5723\n",
-      "Epoch 31/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 1.3143 - acc: 0.5840\n",
-      "Epoch 32/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 1.2465 - acc: 0.5840\n",
-      "Epoch 33/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 1.1864 - acc: 0.5938\n",
-      "Epoch 34/60\n",
-      "512/512 [==============================] - 0s 20us/sample - loss: 1.1122 - acc: 0.6035\n",
-      "Epoch 35/60\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.0906 - acc: 0.5996\n",
-      "Epoch 36/60\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 1.0391 - acc: 0.6035\n",
-      "Epoch 37/60\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 0.9784 - acc: 0.6094\n",
-      "Epoch 38/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.9334 - acc: 0.6289\n",
-      "Epoch 39/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.9262 - acc: 0.6074\n",
-      "Epoch 40/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 0.8853 - acc: 0.6172\n",
-      "Epoch 41/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 0.8633 - acc: 0.6133\n",
-      "Epoch 42/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 0.8811 - acc: 0.6055\n",
-      "Epoch 43/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 0.8384 - acc: 0.6289\n",
-      "Epoch 44/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 0.8170 - acc: 0.6152\n",
-      "Epoch 45/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 0.8090 - acc: 0.6191\n",
-      "Epoch 46/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.7945 - acc: 0.6289\n",
-      "Epoch 47/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.7884 - acc: 0.6250\n",
-      "Epoch 48/60\n",
-      "512/512 [==============================] - 0s 16us/sample - loss: 0.7712 - acc: 0.6348\n",
-      "Epoch 49/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.7794 - acc: 0.6387\n",
-      "Epoch 50/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.7660 - acc: 0.6387\n",
-      "Epoch 51/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 0.7571 - acc: 0.6367\n",
-      "Epoch 52/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 0.7568 - acc: 0.6387\n",
-      "Epoch 53/60\n",
-      "512/512 [==============================] - 0s 23us/sample - loss: 0.7525 - acc: 0.6445\n",
-      "Epoch 54/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 0.7491 - acc: 0.6484\n",
-      "Epoch 55/60\n",
-      "512/512 [==============================] - 0s 21us/sample - loss: 0.7631 - acc: 0.6270\n",
-      "Epoch 56/60\n",
-      "512/512 [==============================] - 0s 22us/sample - loss: 0.7543 - acc: 0.6465\n",
-      "Epoch 57/60\n",
-      "512/512 [==============================] - 0s 19us/sample - loss: 0.7747 - acc: 0.6406\n",
-      "Epoch 58/60\n",
-      "512/512 [==============================] - 0s 18us/sample - loss: 0.7377 - acc: 0.6543\n",
-      "Epoch 59/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.7323 - acc: 0.6523\n",
-      "Epoch 60/60\n",
-      "512/512 [==============================] - 0s 17us/sample - loss: 0.7225 - acc: 0.6582\n",
-      "256/256 [==============================] - 0s 926us/sample - loss: 0.7979 - acc: 0.6016\n",
-      "Epoch 1/60\n",
-      "768/768 [==============================] - 0s 377us/sample - loss: 3.3434 - acc: 0.6497\n",
-      "Epoch 2/60\n",
-      "768/768 [==============================] - 0s 20us/sample - loss: 2.3115 - acc: 0.6315\n",
-      "Epoch 3/60\n",
-      "768/768 [==============================] - 0s 19us/sample - loss: 1.9140 - acc: 0.5859\n",
-      "Epoch 4/60\n",
-      "768/768 [==============================] - 0s 19us/sample - loss: 1.7964 - acc: 0.5990\n",
-      "Epoch 5/60\n",
-      "768/768 [==============================] - 0s 24us/sample - loss: 1.6957 - acc: 0.5938\n",
-      "Epoch 6/60\n",
-      "768/768 [==============================] - 0s 27us/sample - loss: 1.5914 - acc: 0.6042\n",
-      "Epoch 7/60\n",
-      "768/768 [==============================] - 0s 34us/sample - loss: 1.5388 - acc: 0.6029\n",
-      "Epoch 8/60\n",
-      "768/768 [==============================] - 0s 32us/sample - loss: 1.4779 - acc: 0.6055\n",
-      "Epoch 9/60\n",
-      "768/768 [==============================] - 0s 24us/sample - loss: 1.4357 - acc: 0.6029\n",
-      "Epoch 10/60\n",
-      "768/768 [==============================] - 0s 20us/sample - loss: 1.3636 - acc: 0.6315\n",
-      "Epoch 11/60\n",
-      "768/768 [==============================] - 0s 19us/sample - loss: 1.3244 - acc: 0.6094\n",
-      "Epoch 12/60\n",
-      "768/768 [==============================] - 0s 19us/sample - loss: 1.2745 - acc: 0.6120\n",
-      "Epoch 13/60\n",
-      "768/768 [==============================] - 0s 21us/sample - loss: 1.2291 - acc: 0.6146\n",
-      "Epoch 14/60\n",
-      "768/768 [==============================] - 0s 20us/sample - loss: 1.1992 - acc: 0.6276\n",
-      "Epoch 15/60\n",
-      "768/768 [==============================] - 0s 19us/sample - loss: 1.1589 - acc: 0.6224\n",
-      "Epoch 16/60\n",
-      "768/768 [==============================] - 0s 23us/sample - loss: 1.1296 - acc: 0.6198\n",
-      "Epoch 17/60\n",
-      "768/768 [==============================] - 0s 21us/sample - loss: 1.0947 - acc: 0.6263\n",
-      "Epoch 18/60\n",
-      "768/768 [==============================] - 0s 25us/sample - loss: 1.0578 - acc: 0.6393\n",
-      "Epoch 19/60\n",
-      "768/768 [==============================] - 0s 26us/sample - loss: 1.0350 - acc: 0.6458\n",
-      "Epoch 20/60\n",
-      "768/768 [==============================] - 0s 22us/sample - loss: 1.0213 - acc: 0.6497\n",
-      "Epoch 21/60\n",
-      "768/768 [==============================] - 0s 23us/sample - loss: 0.9856 - acc: 0.6549\n",
-      "Epoch 22/60\n",
-      "768/768 [==============================] - 0s 21us/sample - loss: 0.9763 - acc: 0.6497\n",
-      "Epoch 23/60\n",
-      "768/768 [==============================] - 0s 29us/sample - loss: 0.9448 - acc: 0.6549\n",
-      "Epoch 24/60\n",
-      "768/768 [==============================] - 0s 29us/sample - loss: 0.9064 - acc: 0.6497\n",
-      "Epoch 25/60\n",
-      "768/768 [==============================] - 0s 26us/sample - loss: 0.8815 - acc: 0.6536\n",
-      "Epoch 26/60\n",
-      "768/768 [==============================] - 0s 26us/sample - loss: 0.8659 - acc: 0.6562\n",
-      "Epoch 27/60\n",
-      "768/768 [==============================] - 0s 25us/sample - loss: 0.8470 - acc: 0.6562\n",
-      "Epoch 28/60\n",
-      "768/768 [==============================] - 0s 23us/sample - loss: 0.8305 - acc: 0.6576\n",
-      "Epoch 29/60\n",
-      "768/768 [==============================] - 0s 22us/sample - loss: 0.8235 - acc: 0.6562\n",
-      "Epoch 30/60\n",
-      "768/768 [==============================] - 0s 23us/sample - loss: 0.7963 - acc: 0.6641\n",
-      "Epoch 31/60\n",
-      "768/768 [==============================] - 0s 29us/sample - loss: 0.7901 - acc: 0.6680\n",
-      "Epoch 32/60\n",
-      "768/768 [==============================] - 0s 28us/sample - loss: 0.7679 - acc: 0.6654\n",
-      "Epoch 33/60\n",
-      "768/768 [==============================] - 0s 26us/sample - loss: 0.7620 - acc: 0.6615\n",
-      "Epoch 34/60\n",
-      "768/768 [==============================] - 0s 23us/sample - loss: 0.7447 - acc: 0.6680\n",
-      "Epoch 35/60\n",
-      "768/768 [==============================] - 0s 21us/sample - loss: 0.7340 - acc: 0.6758\n",
-      "Epoch 36/60\n",
-      "768/768 [==============================] - 0s 21us/sample - loss: 0.7204 - acc: 0.6810\n",
-      "Epoch 37/60\n",
-      "768/768 [==============================] - 0s 23us/sample - loss: 0.7134 - acc: 0.6745\n",
-      "Epoch 38/60\n",
-      "768/768 [==============================] - 0s 27us/sample - loss: 0.7110 - acc: 0.6719\n",
-      "Epoch 39/60\n",
-      "768/768 [==============================] - 0s 28us/sample - loss: 0.6943 - acc: 0.6888\n",
-      "Epoch 40/60\n",
-      "768/768 [==============================] - 0s 24us/sample - loss: 0.6887 - acc: 0.6888\n",
-      "Epoch 41/60\n",
-      "768/768 [==============================] - 0s 20us/sample - loss: 0.6812 - acc: 0.6732\n",
-      "Epoch 42/60\n",
-      "768/768 [==============================] - 0s 21us/sample - loss: 0.6733 - acc: 0.6836\n",
-      "Epoch 43/60\n",
-      "768/768 [==============================] - 0s 21us/sample - loss: 0.6607 - acc: 0.6810\n",
-      "Epoch 44/60\n",
-      "768/768 [==============================] - 0s 23us/sample - loss: 0.6552 - acc: 0.6849\n",
-      "Epoch 45/60\n",
-      "768/768 [==============================] - 0s 27us/sample - loss: 0.6517 - acc: 0.6953\n",
-      "Epoch 46/60\n",
-      "768/768 [==============================] - 0s 26us/sample - loss: 0.6539 - acc: 0.6901\n",
-      "Epoch 47/60\n",
-      "768/768 [==============================] - 0s 22us/sample - loss: 0.6523 - acc: 0.6810\n",
-      "Epoch 48/60\n",
-      "768/768 [==============================] - 0s 20us/sample - loss: 0.6389 - acc: 0.7005\n",
-      "Epoch 49/60\n",
-      "768/768 [==============================] - 0s 20us/sample - loss: 0.6402 - acc: 0.6979\n",
-      "Epoch 50/60\n",
-      "768/768 [==============================] - 0s 21us/sample - loss: 0.6476 - acc: 0.6901\n",
-      "Epoch 51/60\n",
-      "768/768 [==============================] - 0s 25us/sample - loss: 0.6492 - acc: 0.6810\n",
-      "Epoch 52/60\n",
-      "768/768 [==============================] - 0s 23us/sample - loss: 0.6496 - acc: 0.6888\n",
-      "Epoch 53/60\n",
-      "768/768 [==============================] - 0s 25us/sample - loss: 0.6277 - acc: 0.6966\n",
-      "Epoch 54/60\n",
-      "768/768 [==============================] - 0s 20us/sample - loss: 0.6266 - acc: 0.6940\n",
-      "Epoch 55/60\n",
-      "768/768 [==============================] - 0s 20us/sample - loss: 0.6237 - acc: 0.7031\n",
-      "Epoch 56/60\n",
-      "768/768 [==============================] - 0s 20us/sample - loss: 0.6342 - acc: 0.6849\n",
-      "Epoch 57/60\n",
-      "768/768 [==============================] - 0s 20us/sample - loss: 0.6240 - acc: 0.6966\n",
-      "Epoch 58/60\n",
-      "768/768 [==============================] - 0s 26us/sample - loss: 0.6251 - acc: 0.6810\n",
-      "Epoch 59/60\n",
-      "768/768 [==============================] - 0s 29us/sample - loss: 0.6202 - acc: 0.6953\n",
-      "Epoch 60/60\n",
-      "768/768 [==============================] - 0s 27us/sample - loss: 0.6382 - acc: 0.6810\n",
-      "Best: 0.66796875 using {'batch_size': 60, 'epochs': 60}\n",
-      "Means: 0.515625, Stdev: 0.030425316264447715 with: {'batch_size': 60, 'epochs': 20}\n",
-      "Means: 0.66015625, Stdev: 0.030757843257858637 with: {'batch_size': 60, 'epochs': 40}\n",
-      "Means: 0.66796875, Stdev: 0.036225072248030094 with: {'batch_size': 60, 'epochs': 60}\n",
-      "Means: 0.5690104166666666, Stdev: 0.08452516857873688 with: {'batch_size': 80, 'epochs': 20}\n",
-      "Means: 0.6067708333333334, Stdev: 0.03897559777889522 with: {'batch_size': 80, 'epochs': 40}\n",
-      "Means: 0.6432291666666666, Stdev: 0.03014540860101621 with: {'batch_size': 80, 'epochs': 60}\n"
+      "Best: 0.6966145833333334 using {'batch_size': 10, 'epochs': 200}\n",
+      "Means: 0.6393229166666666, Stdev: 0.038051143740110566 with: {'batch_size': 10, 'epochs': 20}\n",
+      "Means: 0.65234375, Stdev: 0.024079742199097563 with: {'batch_size': 10, 'epochs': 40}\n",
+      "Means: 0.6588541666666666, Stdev: 0.0048719497223619025 with: {'batch_size': 10, 'epochs': 60}\n",
+      "Means: 0.6966145833333334, Stdev: 0.01573313277811793 with: {'batch_size': 10, 'epochs': 200}\n",
+      "Means: 0.5690104166666666, Stdev: 0.07509307737133446 with: {'batch_size': 80, 'epochs': 20}\n",
+      "Means: 0.578125, Stdev: 0.0229993772560621 with: {'batch_size': 80, 'epochs': 40}\n",
+      "Means: 0.6354166666666666, Stdev: 0.06087848816438 with: {'batch_size': 80, 'epochs': 60}\n",
+      "Means: 0.6861979166666666, Stdev: 0.009743899444723805 with: {'batch_size': 80, 'epochs': 200}\n"
      ]
     }
    ],
    "source": [
     "# define the grid search parameters\n",
-    "param_grid = {'batch_size': [60, 80],\n",
-    "              'epochs': [20, 40, 60]}\n",
+    "param_grid = {'batch_size': [10,80],\n",
+    "              'epochs': [20, 40, 60,200]}\n",
     "\n",
     "# Create Grid Search\n",
     "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
@@ -3543,6 +2060,27 @@
     "8) Being a perfectionist here is going to be a long and painful journey, so please do a cost-benefit analysis of how you're using your time here. Learn the concept, have the experience, but don't spend 3 days hyperparameter tuning your assignment."
    ]
   },
+  {
+   "cell_type": "code",
+   "execution_count": 18,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "300.0\n"
+     ]
+    }
+   ],
+   "source": [
+    "cost = (120 * 40) + (5 * 80) # + opportunity cost\n",
+    "\n",
+    "# 95% Accuracy\n",
+    "value_additional_accuracy = (.10*200*15)\n",
+    "print(value_additional_accuracy)"
+   ]
+  },
   {
    "cell_type": "markdown",
    "metadata": {
@@ -3570,9 +2108,9 @@
    "version": "0.3.2"
   },
   "kernelspec": {
-   "display_name": "U4-S2-NNF (Python 3.7)",
+   "display_name": "U4-S1-NLP (Python3)",
    "language": "python",
-   "name": "u4-s2-nnf"
+   "name": "u4-s1-nlp"
   },
   "language_info": {
    "codemirror_mode": {
@@ -3588,5 +2126,5 @@
   }
  },
  "nbformat": 4,
- "nbformat_minor": 2
+ "nbformat_minor": 4
 }
